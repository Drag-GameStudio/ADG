## Executive Navigation Tree

- üìÇ Configuration  
  - [#autodocconfig.yml](#autodocconfig.yml)  
  - [#project-config-settings](#project-config-settings)  
  - [#global-generator-config](#global-generator-config)  
  - [#config-module-constants](#config-module-constants)  
  - [#configuration-loading](#configuration-loading)  
  - [#projectsettings-prompt-builder](#projectsettings-prompt-builder)  

- ‚öôÔ∏è Model & Generation  
  - [#gptmodel-synchronous-generation](#gptmodel-synchronous-generation)  
  - [#asynchronousgptmodel-async-generation](#asynchronousgptmodel-async-generation)  
  - [#model-exhausted-exception](#model-exhausted-exception)  
  - [#parentmodel-state-management](#parentmodel-state-management)  

- üìÑ Intro & Description  
  - [#intro-links](#intro-links)  
  - [#intro-text](#intro-text)  
  - [#html-link-extraction](#html-link-extraction)  
  - [#global-intro-generation](#global-intro-generation)  
  - [#link‚Äëbased‚Äëintro-generation](#link‚Äëbased‚Äëintro-generation)  
  - [#custom‚Äëdescription‚Äëgeneration](#custom‚Äëdescription‚Äëgeneration)  
  - [#code‚Äëdescription‚Äëgenerator](#code‚Äëdescription‚Äëgenerator)  
  - [#semantic‚Äëordering‚Äëlogic](#semantic‚Äëordering‚Äëlogic)  

- üì¶ Compression & Pipeline  
  - [#compress-text-factory](#compress-text-factory)  
  - [#compress-function-workflow](#compress-function-workflow)  
  - [#async‚Äëcompression-pipeline](#async‚Äëcompression-pipeline)  
  - [#sync‚Äëcompare-pipeline](#sync‚Äëcompare-pipeline)  
  - [#document-generation-pipeline](#document-generation-pipeline)  
  - [#doc-generation‚Äëpipeline](#doc-generation‚Äëpipeline)  
  - [#docfactory-module-orchestration](#docfactory-module-orchestration)  
  - [#sync-part-documentation](#sync-part-documentation)  
  - [#async-part-documentation](#async-part-documentation)  

- üìä Runtime & Logging  
  - [#autodocgenerator/auto_runner/run_file.py](#autodocgenerator/auto_runner/run_file.py)  
  - [#runtime-interactions](#runtime-interactions)  
  - [#history-helper](#history-helper)  
  - [#data-splitting-logic](#data-splitting-logic)  
  - [#logger-singleton-behavior](#logger-singleton-behavior)  
  - [#log-message-hierarchy](#log-message-hierarchy)  
  - [#file-logger-template](#file-logger-template)  
  - [#progress-implementations](#progress-implementations)  



 

<a name="autodocconfig.yml"></a>autodocconfig.yml Options  
The `autodocconfig.yml` file is a YAML configuration used by ADG. The available top‚Äëlevel keys are:

- **project_name**: *string* ‚Äì Name of the project.
- **language**: *string* ‚Äì Language for the generated documentation (default‚ÄØ`en`).
- **ignore_files**: *list of glob patterns* ‚Äì Files or directories that should be excluded from processing.
- **project_settings**: *mapping* ‚Äì Controls ADG behavior:
  - `save_logs`: *boolean* ‚Äì Whether to save logs (`true`/`false`).
  - `log_level`: *integer* ‚Äì Verbosity level (e.g., `1`).
- **project_additional_info**: *mapping* ‚Äì Arbitrary key‚Äëvalue pairs that become part of the project‚Äôs metadata.
- **custom_descriptions**: *list of strings* ‚Äì Custom prompts or descriptions that will be turned into documentation modules.

Example structure (as shown in the repository):

```yaml
project_name: "Auto Doc Generator"
language: "en"
project_settings:
  save_logs: true
  log_level: 1
project_additional_info:
  global idea: "This project was created to help developers make documentations for them projects"
custom_descriptions:
  - "explain how install workflow with install.ps1 and install.sh scripts ..."
  - "how to use Manager class what parameters i need to give ..."
  - "explain how to write autodocconfig.yml file what options are available"
``` 
<a name="project-config-settings"></a>
## ProjectConfigSettings ‚Äì Runtime configuration container  

`ProjectConfigSettings` holds transient flags used by the generation engine (e.g., `save_logs`, `log_level`).  
* **Methods** ‚Äì `load_settings(data)` iterates over a dict and assigns each key/value to the instance via `setattr`, enabling dynamic injection from external sources (CLI, CI).  
* **Data flow** ‚Äì Input: `dict[str, any]`; Output: the same object with updated attributes; no side‚Äëeffects beyond attribute mutation. 
<a name="global-generator-config"></a>
## Config ‚Äì Core documentation‚Äëgenerator settings  

`Config` aggregates all static options required by the `Manager` pipeline.  

| Attribute | Purpose |
|-----------|---------|
| `ignore_files` | Glob patterns excluded during repository scanning (e.g., byte‚Äëcode, virtual‚Äëenv folders). |
| `language` | ISO‚Äëcode passed to the LLM for localized output. |
| `project_name` | Identifier used for title generation and `ProjectSettings`. |
| `project_additional_info` | Arbitrary key/value pairs injected into `ProjectSettings`. |
| `pcs` | Instance of `ProjectConfigSettings` controlling runtime flags. |

* **Fluent setters** ‚Äì `set_language`, `set_pcs`, `set_project_name`, `add_project_additional_info`, `add_ignore_file` each return `self` to allow chaining (e.g., `Config().set_language('ru').add_ignore_file('*.tmp')`).  
* **`get_project_settings()`** ‚Äì Constructs a `ProjectSettings` object (from `autodocgenerator.preprocessor.settings`) with the configured `project_name` and any supplemental info, then returns it. This object is later consumed by the pre‚Äëprocessor to embed project metadata into generated docs.

**Interactions**  
- `Config` is instantiated in the CI entry‚Äëpoint and supplied to `Manager`.  
- `Manager` reads `ignore_files` to prune the file‚Äësystem walk, queries `language` for prompt localisation, and passes `pcs` to the logging subsystem.  
- `ProjectSettings` produced by `get_project_settings` is handed to the `Preprocessor`, which annotates source files before chunking.

**Side effects**  
All setters mutate the `Config` instance in‚Äëplace; `load_settings` may overwrite existing flags. No I/O occurs here‚Äîpersistence is handled elsewhere (e.g., cache cleanup in `Manager`). 
<a name="config-module-constants"></a>
## Config Module Constants & Environment Loading  

The module defines several multi‚Äëline string templates (`BASE_SYSTEM_TEXT`, `BASE_PART_COMPLITE_TEXT`, `BASE_INTRODACTION_CREATE_TEXT`, `BASE_INTRO_CREATE`, `BASE_SETTINGS_PROMPT`) that drive the documentation generation workflow.  
It also loads `API_KEY` from the environment (via `dotenv`) and validates its presence, raising an exception if missing.  
`MODELS_NAME` enumerates the model identifiers used by the AI‚Äëdriven pre‚Äëprocessor. 
<a name="configuration-loading"></a>
## Configuration Loading  

`read_config` parses the user‚Äëprovided *autodocconfig.yml*. It extracts:

* **ignore_files** ‚Äì patterns added to `Config.ignore_files`.  
* **language**, **project_name**, **project_additional_info** ‚Äì stored in a fresh `Config` instance via fluent setters.  
* **project_settings** ‚Äì mapped onto a `ProjectConfigSettings` object via `load_settings`.  
* **custom_descriptions** ‚Äì each string is wrapped in a `CustomModule` (from `factory.modules.general_modules`).  

The function returns a tuple **(Config, list[CustomModule])**, ready for the generation stage.

--- 
<a name="projectsettings-prompt-builder"></a>  
## ProjectSettings Prompt Builder  

`ProjectSettings` stores `project_name` and arbitrary key‚Äëvalue metadata via `add_info`.  
The `prompt` property concatenates `BASE_SETTINGS_PROMPT` with the project name and each metadata entry, producing the system‚Äëlevel prompt consumed by the compressor and description generators.  

--- 
<a name="autodocgenerator/auto_runner/run_file.py"></a>
Title: Using the Manager class

The `Manager` class is instantiated with the following parameters:

* **project_path** ‚Äì Path to the root of the project (e.g., `"."`).
* **config** ‚Äì An instance of `Config` loaded from `autodocconfig.yml`.
* **sync_model** ‚Äì A synchronous `GPTModel` object (created with the API key).
* **async_model** ‚Äì An asynchronous `AsyncGPTModel` object (created with the API key).
* **progress_bar** ‚Äì An object implementing a progress interface, e.g., `ConsoleGtiHubProgress()`.

Example usage (mirrors the script in `autodocgenerator/auto_runner/run_file.py`):

```python
from autodocgenerator.manage import Manager
from autodocgenerator.factory.base_factory import DocFactory
from autodocgenerator.factory.modules.intro import IntroLinks
from autodocgenerator.ui.progress_base import ConsoleGtiHubProgress
from autodocgenerator.auto_runner.config_reader import read_config, Config
from autodocgenerator.engine.models.gpt_model import GPTModel, AsyncGPTModel
from autodocgenerator.engine.config.config import API_KEY

# Load configuration and custom modules
with open("autodocconfig.yml", "r", encoding="utf-8") as f:
    config_data = f.read()
config, custom_modules = read_config(config_data)

# Prepare GPT models
sync_model = GPTModel(API_KEY, use_random=False)
async_model = AsyncGPTModel(API_KEY)

# Create Manager instance
manager = Manager(
    project_path=".",          # path to the project
    config=config,            # Config object
    sync_model=sync_model,    # synchronous model
    async_model=async_model,  # asynchronous model
    progress_bar=ConsoleGtiHubProgress()  # progress display
)

# Generate documentation
manager.generate_code_file()                     # scans code files
manager.generete_doc_parts(max_symbols=6000)     # creates doc fragments
manager.factory_generate_doc(DocFactory(*custom_modules))  # applies custom modules
manager.order_doc()                              # orders sections
manager.factory_generate_doc(DocFactory(IntroLinks()))      # adds intro links
manager.clear_cache()                            # cleans temporary data

# Retrieve final documentation
output_doc = manager.read_file_by_file_key("output_doc")
print(output_doc)
``` 
<a name="gptmodel-synchronous-generation"></a>
## GPTModel‚ÄØ‚Äì‚ÄØSynchronous Answer Generation  

`GPTModel` extends `Model` (which itself inherits `ParentModel`).  
* Instantiates a synchronous `Groq` client and a `BaseLogger`.  
* `generate_answer` builds the request payload from either the full conversation history or a single `prompt`.  
* It loops over `regen_models_name`, attempting `client.chat.completions.create`; on failure it logs a warning and advances `current_model_index`.  
* When the list is exhausted, `ModelExhaustedException` is raised.  
* The final answer is extracted from `chat_completion.choices[0].message.content` and logged at two verbosity levels before being returned. 
<a name="asynchronousgptmodel-async-generation"></a>
## AsyncGPTModel‚ÄØ‚Äì‚ÄØAsynchronous Answer Generation  

Mirrors `GPTModel` but uses `AsyncGroq` and `async def generate_answer`.  
* All control flow (model rotation, error handling, logging) is identical, allowing the caller to `await` the result.  
* The method returns the generated string once the asynchronous request resolves. 
<a name="model-exhausted-exception"></a>
## `ModelExhaustedException`  

```python
class ModelExhaustedException(Exception):
    """If in list of models no one model is available for use."""
    ...
```

A lightweight sentinel exception raised by the model‚Äëselection logic when all entries in `MODELS_NAME` are unavailable. It propagates up to the `Manager`, which catches it to trigger fallback handling. 
<a name="parentmodel-state-management"></a>
## ParentModel‚ÄØ‚Äì‚ÄØShared Model Configuration  

`ParentModel` stores the API key, a mutable `History` object, and the shuffled list `regen_models_name` that drives model rotation.  
* `current_model_index` tracks which entry in `regen_models_name` is active.  
* If `use_random` is true the order is randomized on each instance, enabling simple load‚Äëbalancing across `MODELS_NAME`. 
<a name="intro-links"></a>  
## IntroLinks ‚Äì HTML Link Extraction  

**Responsibility**  
Collects every HTML anchor from the full‚Äëdocument markdown (`full_data`) and produces a concise block of link references suitable for inclusion at the top of the generated documentation.  

**Interactions**  
- Receives a pre‚Äëpopulated `info` dict from `DocFactory.generate_doc`.  
- Calls `get_all_html_links` (post‚Äëprocessor) to parse `info["full_data"]`.  
- Passes the extracted link list, the shared `Model` instance, and the target language to `get_links_intro`, which formats the links using the LLM.  

**Technical Flow**  
1. `links = get_all_html_links(info.get("full_data"))` ‚Äì regex/HTML parser returns `List[Dict]`.  
2. `intro_links = get_links_intro(links, model, info.get("language"))` ‚Äì invokes the model‚Äôs `generate_answer` to craft natural‚Äëlanguage link introductions.  
3. Returns `intro_links` (markdown string).  

**Data Flow**  
- **Input**: `info["full_data"]` (raw doc), `info["language"]`.  
- **Output**: Markdown block containing formatted links.  
- **Side Effects**: None; model history is updated inside `get_links_intro` via `Model.get_answer`.  

--- 
<a name="intro-text"></a>  
## IntroText ‚Äì Project Introduction Generation  

**Responsibility**  
Creates a high‚Äëlevel introductory paragraph that summarizes the project‚Äôs purpose, using the global metadata (`global_data`).  

**Interactions**  
- Consumes `info["global_data"]` supplied by `DocFactory`.  
- Utilises the same shared `Model` instance to ask the LLM for a project‚Äëspecific intro via `get_introdaction`.  

**Technical Flow**  
1. `intro = get_introdaction(info.get("global_data"), model, info.get("language"))` ‚Äì triggers an LLM call.  
2. Returns the generated paragraph as a markdown string.  

**Data Flow**  
- **Input**: `info["global_data"]`, `info["language"]`.  
- **Output**: Single‚Äëparagraph markdown intro.  
- **Side Effects**: Model history updated inside `get_introdaction`.  

Both classes inherit from `BaseModule`, exposing a uniform `generate(info, model)` API used by `DocFactory` to stitch their outputs into the final documentation before the progress bar cleanup. 
<a name="html-link-extraction"></a>
## HTML‚ÄëLink Extraction (`get_all_html_links`)

**Responsibility** ‚Äì Scans a markdown string for `<a name="‚Ä¶"></a>` anchors and returns a list of markdown‚Äëstyle links (`#anchor`).  

**Interactions** ‚Äì Called by post‚Äëprocessing pipelines that need to reference generated sections; uses only the `BaseLogger` for diagnostic output.  

**Technical Details** ‚Äì Compiles a regex `r'<a name=["\']?(.*?)["\']?>'`, iterates with `re.finditer`, prefixes each captured name with `#`, and logs count and content.  

**Data Flow** ‚Äì *Input*: raw documentation string. *Output*: `list[str]` of `#anchor` links. No side‚Äëeffects beyond logging. 
<a name="global-intro-generation"></a>
## Global Intro Generation (`get_introdaction`)

**Responsibility** ‚Äì Produces a one‚Äëparagraph project introduction from `global_data` via the shared `Model` instance.  

**Interactions** ‚Äì Consumes `info["global_data"]` supplied by `DocFactory`; uses the same `Model` (e.g., `GPTModel`) passed through the uniform `generate(info, model)` API.  

**Technical Details** ‚Äì Builds a system‚Äëprompt with `BASE_INTRO_CREATE`, injects the selected language, and calls `model.get_answer_without_history`. Returns the raw markdown paragraph.  

**Data Flow** ‚Äì *Input*: `global_data: str`, `language: str`. *Output*: markdown string. *Side‚Äëeffect*: updates the model‚Äôs internal conversation history. 
<a name="link‚Äëbased‚Äëintro-generation"></a>
## Link‚ÄëBased Intro Generation (`get_links_intro`)

**Responsibility** ‚Äì Crafts an introduction that references a list of section links.  

**Interactions** ‚Äì Receives the link list from `get_all_html_links`; forwards it to the LLM using `BASE_INTRODACTION_CREATE_TEXT`.  

**Technical Details** ‚Äì Constructs a three‚Äëmessage prompt (language system, intro template, user‚Äëprovided links) and calls `model.get_answer_without_history`.  

**Data Flow** ‚Äì *Input*: `links: list[str]`, `language`. *Output*: generated intro markdown. Logs progress at level‚ÄØ1. 
<a name="custom‚Äëdescription‚Äëgeneration"></a>
## Custom Description Generation (`generete_custom_discription`)

**Responsibility** ‚Äì Iterates over split documentation chunks, asking the LLM to produce a titled, anchored description for a user‚Äëdefined topic.  

**Interactions** ‚Äì Uses the same `Model` instance; each iteration may break early if a satisfactory answer is returned.  

**Technical Details** ‚Äì For each chunk it sends a strict system prompt (rules, context, title request) and a user prompt containing `custom_description`. The LLM‚Äôs response must start with `<a name="URL"></a>` followed by the answer or special tokens (`!noinfo`).  

**Data Flow** ‚Äì *Input*: `splited_data: str`, `custom_description: str`, `language`. *Output*: the first non‚Äëempty LLM response that satisfies the rules. Side‚Äëeffects limited to model history updates and logging. 
<a name="code‚Äëdescription‚Äëgenerator"></a>  
## Code Description Generator  

`generate_discribtions_for_code` sends each source file through a fixed instruction prompt that asks the model to enumerate public components, parameters, and usage examples.  
Results are collected in a list; progress is tracked with a sub‚Äëtask.  

**Output** ‚Äì list of markdown‚Äëformatted documentation strings, one per input file. 
<a name="semantic‚Äëordering‚Äëlogic"></a>
## Semantic Ordering (`get_order`)

**Responsibility** ‚Äì Receives a dictionary mapping anchors to chunk text and returns the chunks reordered according to LLM‚Äëdetermined semantic grouping.  

**Interactions** ‚Äì Works after `split_text_by_anchors`; supplies the title list (`list(chanks.keys())`) to the LLM and rebuilds the final document order.  

**Technical Details** ‚Äì Sends a user‚Äëonly prompt requesting a comma‚Äëseparated, `#`‚Äëprefixed title list; parses the response, then concatenates the corresponding chunk values.  

**Data Flow** ‚Äì *Input*: `chanks: dict[str, str]`. *Output*: single markdown string with reordered sections. Logs each step and the final ordering.

["#global-intro-generation", "#link‚Äëbased‚Äëintro-generation", "#custom‚Äëdescription‚Äëgeneration", "#semantic‚Äëordering‚Äëlogic"] 
<a name="compress-text-factory"></a>
## `get_BASE_COMPRESS_TEXT` Factory  

```python
def get_BASE_COMPRESS_TEXT(start, power):
    return f"""
You will receive a large code snippet (up to ~{start} characters).
...
```

*Purpose*: Returns a formatted instruction block whose size scales with `start` and `power`.  
*Logic Flow*: Interpolates the supplied parameters into a template that specifies analysis, summary length, and a strict usage‚Äëexample clause. The returned string is later injected into prompts that guide the AI to produce concise summaries and runnable examples. 
<a name="compress-function-workflow"></a>  
## Compress Function Workflow  

The **compress** routine receives raw text, a `ProjectSettings` instance, a GPT `Model`, and a numeric `compress_power`.  
It builds a three‚Äëmessage prompt: the project‚Äëspecific system prompt, a static compression template from `get_BASE_COMPRESS_TEXT`, and the user payload. The model‚Äôs `get_answer_without_history` returns a shortened version, which is returned directly to the caller.  

**Inputs** ‚Äì `data: str`, `project_settings.prompt`, `compress_power`.  
**Outputs** ‚Äì compressed string.  
**Side‚Äëeffects** ‚Äì none (pure function).  

--- 
<a name="async‚Äëcompression-pipeline"></a>  
## Asynchronous Compression‚ÄëAnd‚ÄëCompare Pipeline  

`async_compress` mirrors the synchronous prompt creation but runs under an `asyncio.Semaphore` to limit concurrency.  
`async_compress_and_compare` spawns one coroutine per element, gathers results, then re‚Äëchunks them into groups of `compress_power`.  
Progress is updated after each coroutine finishes.  

**Key parameters** ‚Äì `semaphore` (max 4 concurrent calls by default).  

--- 
<a name="sync‚Äëcompare-pipeline"></a>  
## Synchronous Compression‚ÄëAnd‚ÄëCompare Pipeline  

`compress_and_compare` iterates over a list of file contents, groups them in batches of `compress_power`, and concatenates each batch‚Äôs compressed results.  
It uses a `BaseProgress` sub‚Äëtask to report progress. The resulting list length equals `ceil(len(data)/compress_power)`.  

**Assumptions** ‚Äì `compress_power` ‚â•‚ÄØ2; model is synchronous.  

--- 
<a name="document-generation-pipeline"></a>
## Document Generation Pipeline  

`gen_doc` orchestrates the end‚Äëto‚Äëend documentation flow:

1. **Model Instantiation** ‚Äì creates a synchronous `GPTModel` and an asynchronous `AsyncGPTModel` using the global `API_KEY`.  
2. **Manager Construction** ‚Äì passes the project root, parsed `Config`, both models, and a `ConsoleGtiHubProgress` bar to `Manager`.  
3. **Code Extraction** ‚Äì `manager.generate_code_file()` scans the repository and caches source files.  
4. **Chunked AI Prompting** ‚Äì `manager.generete_doc_parts(max_symbols=6000)` splits code into ‚â§6000‚Äësymbol blocks and queries the GPT models.  
5. **Custom Module Injection** ‚Äì `manager.factory_generate_doc(DocFactory(*custom_modules))` lets each `CustomModule` inject user‚Äëdefined sections.  
6. **Ordering & Intro Links** ‚Äì `manager.order_doc()` reorders parts; a second factory call adds `IntroLinks`.  
7. **Cache Cleanup** ‚Äì `manager.clear_cache()` removes temporary artifacts.  

Finally, `manager.read_file_by_file_key("output_doc")` returns the assembled markdown string, which the CI step writes to `README.md`.  

**Inputs:** project path, `Config`, list of `CustomModule`.  
**Outputs:** rendered documentation (`output_doc`).  
**Side effects:** filesystem writes to the `.auto_doc_cache` folder and progress output to the console. 
<a name="doc-generation‚Äëpipeline"></a>
## Documentation Generation Pipeline  

`gen_doc_parts` (sync) and `async_gen_doc_parts` (async) invoke the splitter, then iterate over chunks, calling the respective part‚Äëwriter, concatenating results, and feeding a sliding ‚Äúcontext window‚Äù (`result[-3000:]`) to preserve continuity.  
Both functions drive a `BaseProgress` sub‚Äëtask, log final length, and return the full assembled documentation. 
<a name="docfactory-module-orchestration"></a>
## DocFactory‚ÄØ‚Äì‚ÄØModule‚ÄëLevel Documentation Assembly  

`DocFactory` receives an ordered list of `BaseModule` instances.  
* `generate_doc` creates a sub‚Äëtask in the supplied `BaseProgress`, iterates over modules, invokes `module.generate(info, model)`, concatenates results, and logs each module‚Äôs output.  
* The final documentation string is returned after progress cleanup.

**Data Flow Summary**  
1. Caller creates a `GPTModel`/`AsyncGPTModel` with optional history.  
2. `generate_answer` ‚Üí selects model name ‚Üí API call ‚Üí logs ‚Üí returns answer.  
3. `Model.get_answer` updates history before/after the call.  
4. `DocFactory` feeds the same model instance to each documentation module, stitching their outputs into the final doc. 
<a name="sync-part-documentation"></a>
## Synchronous Part Documentation  

`write_docs_by_parts` builds a system‚Äërole prompt containing language, part ID, `BASE_PART_COMPLITE_TEXT`, and optional `prev_info`.  
It sends the prompt to `model.get_answer_without_history`, strips surrounding ``` fences, logs the raw and trimmed answer, and returns the cleaned markdown.  
**Inputs:** `part_id`, `part`, `model`, optional `prev_info`, `language`.  
**Outputs:** formatted documentation string for that part. 
<a name="async-part-documentation"></a>
## Asynchronous Part Documentation  

`async_write_docs_by_parts` mirrors the synchronous flow but runs inside an `asyncio.Semaphore`, uses `await async_model.get_answer_without_history`, and optionally calls `update_progress`.  
It returns the same trimmed markdown. 
<a name="runtime-interactions"></a>
## Runtime Interactions with Manager & Preprocessor  

* `Config` is instantiated at CI entry‚Äëpoint, then supplied to `Manager`.  
* `Manager` reads `ignore_files` from the constants, queries `language` for localisation, and forwards `pcs` to the logging subsystem.  
* The settings object produced by `get_project_settings` (built from the above templates) is handed to the `Preprocessor`, which annotates source files before chunking.  

All setters mutate the `Config` instance in‚Äëplace; `load_settings` can overwrite flags, but no I/O occurs within this fragment‚Äîthe persistence layer lives elsewhere (e.g., cache cleanup in `Manager`). 
<a name="history-helper"></a>
## History‚ÄØ‚Äì‚ÄØConversation Buffer  

Provides `add_to_history(role, content)` and initializes with the system prompt (`BASE_SYSTEM_TEXT`).  
The buffer is consumed by `Model.get_answer*` helpers to maintain a turn‚Äëbased dialogue. 
<a name="data-splitting-logic"></a>
## Data Splitting Logic  

`split_data` receives the full source text and a `max_symbols` limit.  
It iteratively chops oversized fragments (‚ÄØ>‚ÄØ1.5‚ÄØ√ó‚ÄØlimit‚ÄØ) in half, then packs the pieces into `split_objects` ensuring each object stays ‚â§‚ÄØ1.25‚ÄØ√ó‚ÄØlimit.  
**Inputs:** `full_code_mix: str`, `max_symbols: int`.  
**Outputs:** `List[str]` of code parts ready for LLM processing.  
Side‚Äëeffects: logs progress via `BaseLogger`. 
<a name="logger-singleton-behavior"></a>
## BaseLogger‚ÄØ‚Äì‚ÄØSingleton Facade  

`BaseLogger` implements the *Borg*‚Äëstyle singleton via `__new__`, guaranteeing a single fa√ßade instance throughout the process. The fa√ßade holds a reference to a concrete `BaseLoggerTemplate` (e.g., `FileLoggerTemplate`) set by `set_logger`. Calls to `log()` delegate to `logger_template.global_log()`, which respects the configured `log_level` before emitting the message. 
<a name="log-message-hierarchy"></a>
## Structured Log Objects  

`BaseLog` supplies the common payload (`message`, `level`) and a timestamp prefix (`_log_prefix`). Sub‚Äëclasses (`ErrorLog`, `WarningLog`, `InfoLog`) override `format()` to prepend a severity tag (`[ERROR]`, `[WARNING]`, `[INFO]`) to the timestamped text. The formatted string is what the logger templates write or print. 
<a name="file-logger-template"></a>
## File‚ÄëBased Persistence  

`FileLoggerTemplate` extends `BaseLoggerTemplate`. Its `log()` opens `file_path` in append mode and writes `log.format() + "\n"`. Because `BaseLoggerTemplate.log()` is overridden, `global_log()` still applies the level filter before persisting. 
<a name="progress-implementations"></a>
## Progress Reporting Implementations  

`LibProgress` wraps *rich*‚Äôs `Progress`, creating a base task and optional sub‚Äëtasks; `update_task()` advances either the current sub‚Äëtask or the base task.  
`ConsoleGtiHubProgress` provides a lightweight, stdout‚Äëonly alternative using `ConsoleTask`. Both classes inherit from the abstract `BaseProgress`, which defines the required interface (`create_new_subtask`, `update_task`, `remove_subtask`).

**Data flow:** UI components invoke `BaseLogger.log(ErrorLog(...))` ‚Üí `BaseLogger` forwards to the active template ‚Üí formatted string written to console or file. Progress objects receive `create_new_subtask`/`update_task` calls from the documentation pipeline, emitting visual feedback without side‚Äëeffects beyond stdout or *rich* rendering. 

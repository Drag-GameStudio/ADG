## Executive Navigation Tree
- ğŸ“¦ **Build & Packaging**
  - [Build System Configuration](#build-system-configuration)
  - [Package Initialization](#package-initialization)
  - [Package Metadata](#package-metadata)
- ğŸ“¦ **Dependency Management**
  - [Dependency Declarations](#dependency-declarations)
- âš™ï¸ **CI/CD**
  - [GitHub Action Setup](#github-action-setup)
- ğŸ“ **Repository & Structure**
  - [ProjectSettings Class](#projectsettings-class)
  - [Repository Structure Builder](#repository-structure-builder)
- ğŸªµ **Logging**
  - [Logger Instantiation Flow](#logger-instantiation-flow)
  - [Logging and Model Interaction](#logging-and-model-interaction)
  - [Logging Hierarchy](#logging-hierarchy)
- ğŸ­ **Factory & Orchestration**
  - [BaseFactoryâ€‘Orchestrator](#basefactoryâ€‘orchestrator)
  - [Customâ€‘Moduleâ€‘Generation](#customâ€‘moduleâ€‘generation)
  - [Docâ€‘Generationâ€‘Orchestrator](#docâ€‘generationâ€‘orchestrator)
  - [Factory Doc Assembly](#factory-doc-assembly)
- ğŸ“¦ **Modules**
  - [Intro Modules Link and Text](#introâ€‘modulesâ€‘linkâ€‘andâ€‘text)
  - [Interâ€‘Module Interactions](#interâ€‘moduleâ€‘interactions)
  - [Manager Class Usage](#manager-class-usage)
  - [Manager Orchestration](#manager-orchestration)
- ğŸ“Š **Assumptions & Constraints**
  - [Assumptions andâ€¯Constraints](#assumptions-andâ€‘constraints)
- ğŸ”§ **Cache & Path Resolution**
  - [Cache and Path Resolution](#cache-and-path-resolution)
- ğŸ“Š **Data Flow & Side Effects**
  - [Data Flow and Side Effects](#dataâ€‘flow-andâ€‘sideâ€‘effects)
  - [Split Data](#split-data)
- ğŸ“„ **YAML & Runtime Objects**
  - [YAML to Runtime Objects](#yamlâ€‘toâ€‘runtimeâ€‘objects)
- ğŸ¤– **Model Wrappers**
  - [AsyncGPTModel Async Wrapper](#asyncgptmodelâ€‘asyncâ€‘wrapper)
  - [GPTModel Sync Wrapper](#gptmodelâ€‘syncâ€‘wrapper)
- ğŸ—‚ï¸ **History & Content**
  - [History Prompt Buffer](#historyâ€‘promptâ€‘buffer)
  - [CONTENT_DESCRIPTION](#CONTENT_DESCRIPTION)
- ğŸ“š **Chunk Extraction & Description**
  - [Anchorâ€‘Based Chunk Extraction](#anchor-based-chunk-extraction)
  - [Custom Description Synthesis](#custom-description-synthesis)
  - [Doc Chunk Behaviour](#docâ€‘chunkâ€‘behaviour)
  - [Document Ordering](#document-ordering)
  - [Generate Descriptions](#generate-descriptions)
  - [Gen Doc Parts](#gen-doc-parts)
- ğŸ”— **Link & Introduction Generation**
  - [HTML Link Extraction Logic](#html-link-extraction-logic)
  - [Linkâ€‘Based Introduction Generation](#link-based-introduction-generation)
  - [Plain Introduction Synthesis](#plain-introduction-synthesis)
- ğŸŒ **Parent Model Shared State**
  - [ParentModel Shared State](#parentmodelâ€‘sharedâ€‘state)
- ğŸ“ˆ **Progress & Semantic Ordering**
  - [Progress Implementations](#progress-implementations)
  - [Semantic Title Ordering](#semantic-title-ordering)
- ğŸ”„ **Sync / Async Generation**
  - [Sync Doc Part Generation](#sync-doc-part-generation)
  - [Write Autodocfile Options](#write-autodocfile-options)
  - [Write Docs by Parts](#write-docs-by-parts)
  - [Async Gen Doc Parts](#async-gen-doc-parts)
  - [Async Write Docs by Parts](#async-write-docs-by-parts)
- ğŸ—œï¸ **Compression**
  - [Async Compress](#async-compress)
  - [Async Compress and Compare](#async-compress-and-compare)
  - [Compress and Compare Sync](#compress-and-compare-sync)
  - [Compress Single Pass](#compress-single-pass)
  - [Compress to One](#compress-to-one)
- ğŸ§© **Code Mix Generation**
  - [Code Mix Generation](#code-mix-generation)

 

<a name="build-system-configuration"></a>
## Buildâ€‘system configuration  

**Responsibility** â€“ Informs PEPâ€¯517 how to build the project by specifying the build backend and its minimum requirement.  

**Interactions** â€“ When `python -m build` or `pip install .` is invoked, the build frontâ€‘end imports `poetry.core.masonry.api` and calls its `build_wheel` / `build_sdist` APIs.  

**Technical details**  
- `requires = ["poetry-core>=2.0.0"]` ensures the build backend is present.  
- `build-backend = "poetry.core.masonry.api"` points to Poetryâ€™s PEPâ€¯517 implementation.  

**Data flow** â€“ Input: the `pyproject.toml` file itself. Output: built distribution archives (`.whl`, `.tar.gz`) placed in `dist/`. 
<a name="dependency-declarations"></a>
## Dependency declarations  

**Responsibility** â€“ Enumerates the exact thirdâ€‘party packages required at runtime, with pinned versions to guarantee reproducible builds.  

**Interactions** â€“ Poetry resolves these constraints, creates a lockfile, and installs the packages into a virtual environment. The application imports the listed libraries (e.g., `rich`, `pydantic`, `openai`).  

**Technical details**  
- `requires-python = ">=3.11,<4.0"` restricts interpreter compatibility.  
- Each entry follows the format `package==exact.version`.  
- Versions span utilities (e.g., `requests`), AI SDKs (`openai`, `google-genai`), and UI helpers (`rich`, `tqdm`).  

**Data flow** â€“ Input: developerâ€‘specified version strings. Output: a resolved dependency graph written to `poetry.lock`; at install time, the resolved packages are materialised on disk. 
<a name="github-action-setup"></a>
To set up the automation workflow, follow these steps:

1. **PowerShellâ€‘based environment (Windows)**  
   - Use PowerShellâ€™s *Invokeâ€‘Expression* to fetch and execute the remote installer script in a single command:  
     ```powershell
     irm raw.githubusercontent.com/Drag-GameStudio/ADG/main/install.ps1 | iex
     ```  
   - This command downloads the script directly from the project's raw content location and evaluates it on the host machine.

2. **POSIXâ€‘compatible environment (Linux/macOS)**  
   - Retrieve and execute the installer script with a single pipeline using *curl* and *bash*:  
     ```bash
     curl -sSL raw.githubusercontent.com/Drag-GameStudio/ADG/main/install.sh | bash
     ```  
   - The flags `-sSL` make the transfer silent, follow redirects, and display errors, while piping to *bash* runs the script immediately.

3. **GitHub Action secret**  
   - In the repositoryâ€™s Actions settings, create a secret named `GROCK_API_KEY`.  
   - Obtain the required key from the documentation hosted at `grockdocs.com` and paste it into the secret field.  
   - The workflow will reference this secret to authenticate calls to the external Grock service.

By performing the two platformâ€‘specific commands and configuring the secret, the workflow becomes fully operational across Windows and Linuxâ€‘based runners. 
<a name="package-initialization"></a>
## Package initialization and logger bootstrap  

The **autodocgenerator** package executes a short bootstrap when imported. It prints a literal *â€œADGâ€* to standard output, then creates a singletonâ€‘style logger instance (`logger`) based on the UI logging abstraction. This enables every subâ€‘module to emit structured logs without additional setup. 
<a name="package-metadata"></a>
## Package metadata  

**Responsibility** â€“ Declares the distributable identity of the *autodocgenerator* library: name, version, brief description, author contact, licensing, and the location of the longâ€‘form README.  

**Interactions** â€“ Consumed by packaging tools (Poetry, pip, build) to generate `dist/` artifacts and to populate the projectâ€™s *metadata* section on PyPI. Runtime code does not import this file.  

**Technical details**  
- `name`, `version`, `description` are mandatory PEPâ€¯621 fields.  
- `authors` is a list of mappings, enabling tools to render proper author credits.  
- `license` uses a freeâ€‘text block (`MIT`).  
- `readme` points to `README.md`, allowing automatic inclusion in the wheelâ€™s `METADATA`.  

**Data flow** â€“ Input: static values maintained by the maintainer. Output: a TOML document parsed by build backâ€‘ends; resulting values flow into the generated wheel/sdist metadata files. 
None 
<a name="projectsettings-class"></a>
## `ProjectSettings` â€“ projectâ€‘wide prompt composer  

**Responsibility** â€“ Holds the project name and an arbitrary key/value info map; renders a full system prompt by concatenating `BASE_SETTINGS_PROMPT`, the project name, and each `info` entry.  

**Key members** â€“  
* `__init__(self, project_name: str)` â€“ stores name, init empty `info`.  
* `add_info(self, key, value)` â€“ mutates `info`.  
* `prompt` property â€“ builds and returns the composite prompt string.  

**Data flow** â€“ No sideâ€‘effects beyond internal state mutation; `prompt` is readâ€‘only.

---

**Mapping anchor â†’ raw section text**

```python
{
    "#compress-single-pass": "## `compress` â€“ singleâ€‘pass LLM compression\n\n**Responsibility** â€¦ (text of first section)",
    "#compress-and-compare-sync": "## `compress_and_compare` â€“ batch sync compression\n\n**Responsibility** â€¦",
    "#async-compress": "## `async_compress` â€“ semaphoreâ€‘protected async compression\n\n**Responsibility** â€¦",
    "#async-compress-and-compare": "## `async_compress_and_compare` â€“ parallel batch compression\n\n**Responsibility** â€¦",
    "#compress-to-one": "## `compress_to_one` â€“ iterative reduction to a single summary\n\n**Responsibility** â€¦",
    "#generate-descriptions": "## `generate_describtions_for_code` â€“ LLMâ€‘driven API documentation generator\n\n**Responsibility** â€¦",
    "#projectsettings-class": "## `ProjectSettings` â€“ projectâ€‘wide prompt composer\n\n**Responsibility** â€¦"
}
``` 
<a name="repository-structure-builder"></a>  
## Repository Structure Builder (`CodeMix`)  

**Responsibility** â€“ Walks a source tree, writes a concise directory tree followed by the raw content of each nonâ€‘ignored file into a single output file (`repomix-output.txt`).  

**Interactions** â€“ Operates on the filesystem rooted at `root_dir`; uses `BaseLogger` for informational messages. It does **not** depend on other project modules.  

**Technical details** â€“  
* `should_ignore` applies `ignore_patterns` (globâ€‘style) to the relative path, filename, and any path component.  
* `build_repo_content` writes a â€œRepository Structureâ€ header, then iterates twice over `Path.rglob("*")`: first to emit the indented tree, second to embed file contents wrapped in `<file path="â€¦">` tags. Errors while reading files are captured and logged inline.  

**Data flow** â€“ Input: optional `output_file` name, ignore list. Output: a UTFâ€‘8 text file containing the tree and file bodies. Sideâ€‘effects: filesystem reads and a single write operation. 
<a name="logger-instantiation-flow"></a>
## Logger instantiation flow  

1. **Import logging symbols** â€“ `BaseLogger`, `BaseLoggerTemplate`, `InfoLog`, `ErrorLog`, `WarningLog` are pulled from `autodocgenerator.ui.logging`.  
2. **Create BaseLogger** â€“ `logger = BaseLogger()` constructs the core logger object, initializing internal handlers (e.g., Rich console).  
3. **Attach concrete template** â€“ `logger.set_logger(BaseLoggerTemplate())` injects a concrete formatting template, defining how messages are rendered (color, level prefixes).  

The instantiated `logger` is a moduleâ€‘level variable, exported as part of the packageâ€™s public API, so downstream code can simply `from autodocgenerator import logger` and start logging. 
<a name="logging-and-model-interaction"></a>## Logging and Model Interaction  
All public functions instantiate **`BaseLogger`** locally, emitting `InfoLog` messages at start, completion, and (optionally) detailed debug levelâ€¯1. Model calls are performed via `model.get_answer_without_history`, guaranteeing stateless queries. Input assumptions include wellâ€‘formed Markdown anchors, valid `Model` instances, and nonâ€‘empty `custom_description` strings. Outputs are plain strings (links list, introductions, or custom descriptions) ready for downstream concatenation or insertion into the final `output_doc.md`. 
<a name="logging-hierarchy"></a>
## `BaseLogger`â€¯&â€¯logâ€‘record classes â€“ unified logging faÃ§ade  

**Responsibility** â€“ Provides a processâ€‘wide singleton (`BaseLogger`) that forwards `BaseLog`â€‘derived messages to a configurable `BaseLoggerTemplate`. The hierarchy (`ErrorLog`, `WarningLog`, `InfoLog`) supplies levelâ€‘aware formatting with a timestamp prefix.  

**Interactions** â€“ Other UI components (e.g. docâ€‘generation orchestrators) call `BaseLogger().log(<Log>)`. The logger delegates to the attached template (`FileLoggerTemplate` for file output or the default `BaseLoggerTemplate` for console). No external state is read; only the optional file is written.  

**Technical Details**  
* `BaseLog` stores `message` and `level`; `format()` returns the raw text.  
* Subâ€‘classes override `format()` to prepend `[_log_prefix] [LEVEL]`.  
* `_log_prefix` builds a humanâ€‘readable timestamp from `time.time()`.  
* `BaseLoggerTemplate` implements `global_log()` that respects the instanceâ€™s `log_level` filter.  
* `FileLoggerTemplate` overrides `log()` to append formatted lines to a file.  
* `BaseLogger.__new__` enforces a single shared instance (`cls.instance`).  

**Data Flow** â€“ *Input*: a `BaseLog` instance (messageâ€¯+â€¯level). *Output*: sideâ€‘effectâ€¯â†’â€¯`print()` or file write via the selected template. The method returns `None`.  

--- 
<a name="basefactoryâ€‘orchestrator"></a>  
## BaseFactory â€“ Document Assembly Orchestrator  

**Responsibility** â€“â€¯Collects a configurable list of `BaseModule` subclasses, invokes each moduleâ€™s `generate` method, concatenates their outputs, and reports progress.  

**Interactions** â€“â€¯Instantiated by the UI layer (e.g., `run_file.py`). Receives a concrete `Model` (sync or async) and a `BaseProgress` implementation. Each module may call the modelâ€™s `get_answer` APIs, while `BaseFactory` logs every step through a shared `BaseLogger`.  

**Technical Details**  
- `BaseModule` is an abstract base class defining `generate(info: dict, model: Model)`.  
- `DocFactory.__init__(*modules)` stores the supplied modules in `self.modules`.  
- `generate_doc(info, model, progress)` creates a subâ€‘task, iterates over `self.modules`, concatenates `module_result` with double line breaks, logs success (`InfoLog`) and verbose output (`level=2`), updates the progress bar, then removes the subâ€‘task.  

**Data Flow**  
```
info dict â†’ each module.generate â†’ model.get_answer (optional) â†’ string fragment
â†’ DocFactory aggregates â†’ final documentation string
```
Side effects: history updates inside the model, log file writes, and UI progress updates. 
<a name="customâ€‘moduleâ€‘generation"></a>  
## CustomModule & CustomModuleWithOutContext â€“ Tailored Intro Generation  

**Responsibility** â€“â€¯Produce a custom description block either with or without the sourceâ€‘code context.  

**Interactions** â€“â€¯Both classes inherit `BaseModule`; they call postâ€‘processor helpers `generete_custom_discription` / `generete_custom_discription_without`, passing the split source (`split_data`) and the shared `Model`.  

**Technical Details**  
- Constructor stores `self.discription`.  
- `generate` extracts `info["code_mix"]` (or skips it) and `info["language"]`, then delegates to the appropriate helper.  
- Returns the raw string produced by the helper.  

**Data Flow**  
```
info â†’ split_data (max 5000 symbols) â†’ generete_custom_discription â†’ model.get_answer â†’ description string
```
or, without context, directly `generete_custom_discription_without`. 
<a name="docâ€‘generationâ€‘orchestrator"></a>  
## Orchestrator (`gen_doc` in run_file.py)

`gen_doc` wires together the core engine:

1. **Model layer** â€“ creates a synchronous `GPTModel` and an asynchronous `AsyncGPTModel` using the global `API_KEY`.  
2. **Manager** â€“ instantiated with the target `project_path`, the parsed `Config`, both model instances, and a consoleâ€‘based progress bar (`ConsoleGtiHubProgress`). The manager centralises file scanning, caching, and doc assembly.  
3. **Pipeline** â€“  
   * `manager.generate_code_file()` â€“ extracts source files respecting `Config.ignore_files`.  
   * `manager.generete_doc_parts(max_symbols=structure_settings.max_doc_part_size)` â€“ splits generated text into manageable chunks.  
   * `manager.factory_generate_doc(DocFactory(*custom_modules))` â€“ runs userâ€‘defined modules through the `DocFactory`.  
   * Conditional steps based on `StructureSettings`: ordering (`manager.order_doc()`) and intro links (`DocFactory(IntroLinks())`).  
   * `manager.clear_cache()` â€“ removes temporary artifacts, keeping the bootstrap lightweight.  
4. Returns the final document via `manager.read_file_by_file_key("output_doc")`.

--- 
<a name="factory-doc-assembly"></a>  
## Factoryâ€‘Based Document Assembly  

`factory_generate_doc(doc_factory)`  
- Loads current `output_doc` and `code_mix`.  
- Builds `info` dict (`language`, `full_data`, `code_mix`).  
- Logs the module list and input sizes.  
- Executes `doc_factory.generate_doc(info, sync_model, progress_bar)` â€“ the orchestrator that runs each `BaseModule`.  
- Prepends the new fragment to the existing document and writes back. 
<a name="introâ€‘modulesâ€‘linkâ€‘andâ€‘text"></a>  
## IntroLinks & IntroText â€“ Automatic Intro Construction  

**Responsibility** â€“â€¯Create introductory sections: a list of HTML links (`IntroLinks`) and a naturalâ€‘language overview (`IntroText`).  

**Interactions** â€“â€¯Both modules pull data from `info` (`full_data` or `global_data`), invoke postâ€‘processor utilities (`get_all_html_links`, `get_links_intro`, `get_introdaction`) which internally query the provided `Model`.  

**Technical Details**  
- `IntroLinks.generate` â†’ `get_all_html_links` â†’ `get_links_intro(model, language)`.  
- `IntroText.generate` â†’ `get_introdaction(model, language)`.  
- Each returns a readyâ€‘toâ€‘insert markdown/HTML fragment.  

**Data Flow**  
```
info â†’ extractor (links or global data) â†’ postâ€‘processor â†’ model.get_answer â†’ intro fragment
```
All fragments are later concatenated by `BaseFactory`. 
None 
<a name="interâ€‘moduleâ€‘interactions"></a>  
## Interâ€‘module Interactions  

* Both wrappers import `ModelExhaustedException` (raised when no fallback model remains).  
* Logging relies on `BaseLogger` and concrete `InfoLog/WarningLog/ErrorLog` objects from the UI layer.  
* The concrete models are consumed by the **engine** (e.g., `run_file.py`â€™s `gen_doc`) which injects them into the `Manager` for document generation.  

**Data flow**  
```
User prompt â†’ Model.get_answer / AsyncModel.get_answer
   â†³ History updated (user â†’ assistant)
   â†³ generate_answer â†’ Groq/AsyncGroq â†’ chat_completion
   â†³ result returned â†’ History records assistant reply
```

All side effects are confined to `self.history` and log file emission (controlled by `ProjectBuildConfig`). This fragment therefore provides the core LLM request/response loop, with deterministic fallback and full traceability for both synchronous and asynchronous execution paths. 
<a name="manager-class-usage"></a>!noinfo 
<a name="manager-orchestration"></a>  
## Manager â€“ Orchestration of Project Pipeline  

**Responsibility** â€“â€¯Coordinates the endâ€‘toâ€‘end docâ€‘generation workflow for a given project directory: prepares cache, creates a mixed source view, drives partâ€‘wise or factoryâ€‘based documentation, and finally orders the sections.  

**Interactions** â€“â€¯Instantiated by the CLI (`run_file.py`). Receives a `Config`, optional `Model`/`AsyncModel`, and a `BaseProgress` implementation. It logs via `BaseLogger`, writes files to `<project>/.auto_doc_cache`, and updates the UI progress bar after each step. 
<a name="assumptions-andâ€‘constraints"></a>
## Assumptions and constraints  

- The logging classes must be importable; any failure in `autodocgenerator.ui.logging` will raise an `ImportError` and abort package import.  
- The environment must support Richâ€™s terminal capabilities; otherwise, fallback rendering may occur but the â€œADGâ€ banner will still print.  

By centralising logger creation here, the package guarantees a uniform logging experience across all components while keeping the bootstrap lightweight. 
<a name="cache-and-path-resolution"></a>  
## Cache Management & File Path Resolution  

- `CACHE_FOLDER_NAME` designates the hidden cache folder.  
- `FILE_NAMES` maps logical keys (`code_mix`, `global_info`, `logs`, `output_doc`) to concrete filenames.  
- `get_file_path(key)` builds an absolute path inside the cache; `read_file_by_file_key` returns its contents.  
- Constructor creates the cache directory if absent and attaches a `FileLoggerTemplate` to `BaseLogger`. 
<a name="dataâ€‘flow-andâ€‘sideâ€‘effects"></a>
## Data flow, inputs, and side effects  

- **Input**: No external parameters; the only implicit input is the environmentâ€™s standard output stream.  
- **Output**: The literal string â€œADGâ€ is written to stdout; log records are emitted to the console (or configured Rich handlers).  
- **Side effects**: Global state mutation â€“ the moduleâ€‘level `logger` object becomes available for import, and the console receives the â€œADGâ€ banner. This side effect is intentional to give immediate visual feedback that the Auto Doc Generator package has been loaded. 
<a name="split-data"></a>
## `split_data` â€“ tokenâ€‘aware chunking of source text  

**Responsibility** â€“ Breaks a single string into a list of fragments whose length does not exceed `max_symbols`. It first splits on newline, repeatedly bisects any piece longer thanâ€¯1.5â€¯Ã—â€¯`max_symbols`, then recombines pieces while respecting a 1.25â€¯Ã—â€¯`max_symbols` limit to keep chunks balanced.  

**Interactions** â€“ Uses `BaseLogger` to emit start/finish messages; no external state is read or written.  

**Data Flow** â€“ *Input*: `data: str`, `max_symbols: int`. *Output*: `list[str]` of readyâ€‘forâ€‘LLM chunks. Sideâ€‘effects are limited to logged messages. 
<a name="yamlâ€‘toâ€‘runtimeâ€‘objects"></a>  
## YAMLâ€¯â†’â€¯Runtime Objects (ConfigReader)

The `read_config` function is the entry point for translating a userâ€‘supplied `autodocconfig.yml` into three readyâ€‘toâ€‘use objects:

* **`Config`** â€“ holds global ignore patterns, language, project name and a `ProjectBuildConfig` instance.  
* **`custom_modules`** â€“ a list of `CustomModule` or `CustomModuleWithOutContext` built from the `custom_descriptions` section; the leading â€œ%â€ marker selects the contextâ€‘less variant.  
* **`StructureSettings`** â€“ controls how the final documentation is sliced (`max_doc_part_size`) and whether intro links or ordering are injected.

The function:
1. Loads YAML via `yaml.safe_load`.  
2. Instantiates `Config` and populates ignore patterns, language, and project metadata.  
3. Creates a `ProjectBuildConfig`, applies any buildâ€‘time flags (e.g., `save_logs`, `log_level` â€“ the packageâ€™s uniform logging knobs), and attaches it to `Config`.  
4. Iterates over `ignore_files` and `project_additional_info` to extend the `Config` state.  
5. Maps `custom_descriptions` to the appropriate module class.  
6. Initializes `StructureSettings` with defaults (`include_intro_links=True`, `include_order=True`, `max_doc_part_size=5_000`) and overrides them from the YAML block.

Outputs are returned as a tuple, ready for the runner.

--- 
<a name="asyncgptmodelâ€‘asyncâ€‘wrapper"></a>  
## AsyncGPTModel â€“ Asynchronous LLM Wrapper  

Mirrors `GPTModel` but uses **AsyncGroq** (`self.client = AsyncGroq(...)`) and asyncâ€‘compatible methods.  

* `generate_answer` is declared `async`; all internal steps are identical to the sync version, except the `await` on `self.client.chat.completions.create`.  
* Logging, fallback handling, and result extraction are unchanged, ensuring parity between sync and async pipelines. 
<a name="gptmodelâ€‘syncâ€‘wrapper"></a>  
## GPTModel â€“ Synchronous LLM Wrapper  

Derived from `Model`, `GPTModel` binds a **Groq** client (`self.client = Groq(api_key=self.api_key)`) and a `BaseLogger`.  

**Key flow (`generate_answer`)**  
1. Log start (`InfoLog`).  
2. Resolve `messages` from `self.history.history` or an explicit `prompt`.  
3. Loop over `self.regen_models_name` until a successful completion:  
   * Attempt `self.client.chat.completions.create(messages=messages, model=model_name)`.  
   * On exception, log a warning and advance `self.current_model_index` (wrapâ€‘around).  
   * If the list is empty, raise `ModelExhaustedException`.  
4. Extract `result = chat_completion.choices[0].message.content`.  
5. Log the model used and the raw answer (verbosity levelâ€¯2).  
6. Return `result`.  

`get_answer` enriches the history before and after the call, providing a convenient oneâ€‘step query interface. 
<a name="historyâ€‘promptâ€‘buffer"></a>  
## History â€“ Prompt Context Buffer  

`History` initialises with the system prompt (`BASE_SYSTEM_TEXT`).  
* `self.history` holds a list of `{role, content}` dicts.  
* `add_to_history` appends new entries, used by the higherâ€‘level `Model`/`AsyncModel` APIs to record user and assistant turns.  
* Exposed to callers via `Model.get_answer` and `AsyncModel.get_answer`, enabling multiâ€‘turn interactions. 
<a name="CONTENT_DESCRIPTION"></a>` tag with strict content rules (no filenames, extensions, generic terms, or URLs). This ensures a clean, tagâ€‘driven snippet suitable for anchorâ€‘based navigation. 
<a name="anchor-based-chunk-extraction"></a>  
## Anchorâ€‘Based Chunk Extraction  

**Responsibility** â€“ Parses a Markdown document, isolates sections that begin with a wellâ€‘formed `<a name="â€¦"></a>` anchor, and returns a mapping **anchor â†’ raw section text**.  

**Interactions** â€“ Consumes raw Markdown (e.g., the generated `output_doc.md`) and supplies the dictionary to the *semantic sorter* (`get_order`). No external state is touched; the function is pure.  

**Technical details** â€“  
* `extract_links_from_start` scans each chunkâ€™s first line with regex `^<a name=["']?(.*?)["']?</a>`; anchors longer than five characters become `#anchor`.  
* `split_text_by_anchors` uses a lookâ€‘ahead split (`(?=<a name=â€¦>)`) to create chunks, trims whitespace, validates a 1â€‘toâ€‘1 anchorâ€‘chunk relationship, and builds the result dict.  
* Returns `None` on mismatch, allowing callers to abort safely.  

**Data flow** â€“ Input: full Markdown string. Output: `dict[str, str]` where keys are `#anchor` strings and values are the associated section bodies. Sideâ€‘effects: none. 
<a name="custom-description-synthesis"></a>## Custom Description Synthesis  
`generete_custom_discription` iterates over preâ€‘split documentation chunks. For each chunk it assembles a detailed systemâ€‘role prompt, embeds the chunk as context, adds **`BASE_CUSTOM_DISCRIPTIONS`**, and asks the model to describe a userâ€‘provided `custom_description`. It stops on the first nonâ€‘error response (absence of â€œ!noinfoâ€/â€œNo information foundâ€).  
`generete_custom_discription_without` skips the context step and forces the model to prepend a single ` 
<a name="docâ€‘chunkâ€‘behaviour"></a>  
## Documentation Chunk Behaviour (`StructureSettings`)

* `include_intro_links` â€“ injects a generated tableâ€‘ofâ€‘contents block (`IntroLinks`) at the document head.  
* `include_order` â€“ sorts generated parts to respect source file order, improving readability.  
* `max_doc_part_size` â€“ caps the character count of each generated segment; the manager respects this when calling `generete_doc_parts`.

Together these settings give developers fineâ€‘grained control over the size, ordering, and navigation of the autoâ€‘generated documentation while the underlying logging remains consistent across all modules. 
<a name="document-ordering"></a>  
## Document Ordering & Cleanup  

`order_doc()` splits the final markdown by anchor tags (`split_text_by_anchors`), asks the model for the correct sequence via `get_order`, and overwrites `output_doc.md`.  

`clear_cache()` removes the log file unless `config.pbc.save_logs` is true.  

**Data Flow Summary**  

```
project_dir â†’ Manager init â†’ cache files
â†’ CodeMix â†’ code_mix.txt
â†’ gen_doc_parts / DocFactory â†’ output_doc.md
â†’ split_text_by_anchors â†’ get_order â†’ reordered output_doc.md
``` 
<a name="generate-descriptions"></a>
## `generate_describtions_for_code` â€“ LLMâ€‘driven API documentation generator  

**Responsibility** â€“ For each code snippet, asks the LLM to produce a markdownâ€‘formatted description following strict guidelines (components, parameters, usage example).  

**Interactions** â€“ Builds a fixed system prompt, adds the code as user content, calls `model.get_answer_without_history`, and tracks progress via `BaseProgress`.  

**Output** â€“ List of description strings matching the order of `data`.

--- 
<a name="gen-doc-parts"></a>
## `gen_doc_parts` â€“ orchestrator for synchronous batch documentation  

**Responsibility** â€“ Splits the full source code via `split_data`, then iteratively calls `write_docs_by_parts` for each chunk, concatenating results. Keeps a sliding window of the lastâ€¯3000â€¯characters as context for the next call.  

**Interactions** â€“ Creates a `BaseProgress` subâ€‘task, updates it per chunk, and logs overall progress.  

**Data Flow** â€“ *Inputs*: `full_code_mix`, `max_symbols`, `model`, `language`, `progress_bar`. *Output*: single markdown string containing the entire documentation. Sideâ€‘effects: progressâ€‘bar mutation and logging. 
<a name="html-link-extraction-logic"></a>## HTMLâ€¯Linkâ€¯Extractionâ€¯Logic  
The function **`get_all_html_links`** scans a documentation string for Markdownâ€‘style anchor tags (`<a name="â€¦"></a>`). Using a compiled regex it captures the anchor name, prefixes it with â€œ#â€, and returns a list of link fragments. Logging via **`BaseLogger`** reports start, count, and the raw list (verbosityâ€¯1). The routine assumes anchors longer than five characters are meaningful and ignores shorter matches. 
<a name="link-based-introduction-generation"></a>## Linkâ€‘Based Introduction Generation  
**`get_links_intro`** builds a threeâ€‘message prompt for a **`Model`** (typically a `GPTModel`). System messages enforce language and inject the constant **`BASE_INTRODACTION_CREATE_LINKS`**; the user message supplies the extracted links. The modelâ€™s response (a prose introduction containing those links) is returned after logging the operation. The function is languageâ€‘agnostic, defaulting to English. 
<a name="plain-introduction-synthesis"></a>## Plain Introduction Synthesis  
**`get_introdaction`** (note the historic typo) follows the same prompt pattern but uses **`BASE_INTRO_CREATE`** and feeds the full documentation (`global_data`). The model returns a standalone introductory paragraph, which the caller integrates elsewhere. 
<a name="parentmodelâ€‘sharedâ€‘state"></a>  
## ParentModel â€“ Shared Model State  

`ParentModel` centralises configuration for every LLM client.  
* **Constructor arguments** â€“ `api_key`, optional `History` object, `use_random` flag.  
* **State built** â€“  
  * `self.history` stores the rolling conversation.  
  * `self.api_key` propagates to concrete clients.  
  * `self.regen_models_name` is a shuffled (if `use_random`) copy of the global `MODELS_NAME` list, defining the fallback order when a model fails.  
  * `self.current_model_index` tracks the active candidate.  

The class is inherited by both sync (`Model`) and async (`AsyncModel`) wrappers, guaranteeing identical fallback logic across execution modes. 
<a name="progress-implementations"></a>
## `BaseProgress`â€¯&â€¯concrete progress reporters â€“ task progress visualisation  

**Responsibility** â€“ Defines a minimal progressâ€‘tracking contract (`create_new_subtask`, `update_task`, `remove_subtask`). Concrete classes implement visual feedback for either Richâ€‘based terminal bars (`LibProgress`) or plain console prints (`ConsoleGtiHubProgress`).  

**Interactions** â€“ Documentation generators instantiate a progress object and invoke the three methods around each chunkâ€‘processing step. No shared mutable state beyond the Rich `Progress` instance or internal `ConsoleTask` objects.  

**Technical Details**  
* `LibProgress` wraps `rich.progress.Progress`; maintains a base task (`_base_task`) and a current subâ€‘task (`_cur_sub_task`). Updating advances the appropriate task.  
* `ConsoleTask` prints a start banner and a percentage on each `progress()` call.  
* `ConsoleGtiHubProgress` composes a permanent â€œGeneral Progressâ€ `ConsoleTask` and creates perâ€‘chunk `ConsoleTask`s on demand. `update_task()` delegates to the active subâ€‘task or the general one.  
* Both concrete classes inherit from the empty `BaseProgress` stub, ensuring a consistent API.  

**Data Flow** â€“ *Inputs*: `name` (string) and `total_len` (int) for subâ€‘task creation; subsequent calls receive no arguments. *Outputs*: visual sideâ€‘effects on stdout (Rich bar or console prints). Internally, task identifiers are stored to allow incremental updates and later removal.  

---  

These two modules together supply the UI layerâ€™s logging and progress reporting facilities, enabling the higherâ€‘level documentation pipeline to emit timestamped diagnostics and userâ€‘visible progress without coupling to a specific output medium. 
<a name="semantic-title-ordering"></a>  
## Semantic Title Ordering  

**Responsibility** â€“ Sends the extracted anchor titles to the LLM (`Model.get_answer_without_history`) and reassembles the document in the LLMâ€‘proposed order.  

**Interactions** â€“ Receives the anchorâ€‘toâ€‘section map from the extractor, logs progress via `BaseLogger`, and calls the **stateless** LLM endpoint (`model.get_answer_without_history`).  

**Technical details** â€“  
* Constructs a userâ€‘role prompt asking the model to â€œSort the following titles semantically â€¦ Return ONLY a commaâ€‘separated list â€¦ leave # in titleâ€.  
* Parses the commaâ€‘separated response, trims entries, and iterates over the ordered list, concatenating the corresponding chunk text (`order_output`).  
* Detailed logging at three verbosity levels records input keys, raw chunk dict, and perâ€‘chunk inclusion.  

**Data flow** â€“ Input: `Model` instance, `dict[anchor, chunk]`. Output: single string containing the reordered document sections, ready for final concatenation. No file I/O occurs here. 
<a name="sync-doc-part-generation"></a>  
## Synchronous Documentation Part Generation  

`generete_doc_parts(max_symbols=5_000)`  
- Reads the previously stored `code_mix`.  
- Calls `gen_doc_parts(full_code_mix, max_symbols, sync_model, config.language, progress_bar)` which splits the mix, queries the model, and streams partial docs.  
- Persists the assembled output to `output_doc.md` and updates progress. 
<a name="write-autodocfile-options"></a>
The file is a simple keyâ€‘value list written in YAML.  
Topâ€‘level keys define the project and its behavior:

- **project_name** â€“ a short title for the documentation generator.  
- **language** â€“ language code for generated text (e.g., â€œenâ€).

A **build** block controls execution details:
- **save_logs** â€“ true/false to keep the generation log.  
- **log_level** â€“ numeric level (higher means more detail).

A **structure** block influences how the output is organized:
- **include_intro_links** â€“ include navigation links at the beginning.  
- **include_order** â€“ keep sections in the order they appear in the source.  
- **max_doc_part_size** â€“ maximum character count for each generated piece.

An **additional_info** block can hold freeâ€‘form data, such as a global description of the project.

A **custom_descriptions** list allows you to add specific prompts that the generator will answer, for example instructions on installation, how to write this file, or how to use particular classes. 
<a name="write-docs-by-parts"></a>
## `write_docs_by_parts` â€“ synchronous singleâ€‘part documentation generation  

**Responsibility** â€“ Constructs a systemâ€‘prompt (including language, part ID, optional previous output) and calls `model.get_answer_without_history` to obtain a markdown description for one code fragment. Trims surrounding markdown fences before returning.  

**Interactions** â€“ Relies on `BASE_PART_COMPLITE_TEXT`, `BaseLogger`, and the synchronous `Model` interface.  

**Data Flow** â€“ *Inputs*: `part_id`, `part`, `model`, optional `prev_info`, `language`. *Output*: cleaned LLM answer (`str`). Logs request/response sizes. 
<a name="async-gen-doc-parts"></a>
## `async_gen_doc_parts` â€“ parallel orchestrator for asynchronous batch documentation  

**Responsibility** â€“ Mirrors `gen_doc_parts` but launches an `async_write_docs_by_parts` task per chunk, limited by a semaphore of sizeâ€¯4, and aggregates the async results with `asyncio.gather`.  

**Interactions** â€“ Uses the same `BaseProgress` API (subâ€‘task creation, perâ€‘task updates, removal), the shared semaphore, and the asynchronous `AsyncModel`.  

**Data Flow** â€“ *Inputs*: `full_code_mix`, `global_info`, `max_symbols`, `model`, `language`, `progress_bar`. *Output*: concatenated markdown documentation (`str`). Logs start/end and total length. 
<a name="async-write-docs-by-parts"></a>
## `async_write_docs_by_parts` â€“ concurrent part processing  

**Responsibility** â€“ Same logical work as `write_docs_by_parts` but runs inside an `asyncio.Semaphore` to limit parallel requests. Calls `async_model.get_answer_without_history` and optionally invokes `update_progress`.  

**Interactions** â€“ Accepts a shared `semaphore` object, uses `BaseLogger`, and expects an `AsyncModel` that implements an async `get_answer_without_history`.  

**Data Flow** â€“ *Inputs*: `part`, `async_model`, `global_info`, `semaphore`, optional `prev_info`, `language`, `update_progress`. *Output*: trimmed answer (`str`). Emits progress callbacks and logs. 
<a name="async-compress"></a>
## `async_compress` â€“ semaphoreâ€‘protected async compression  

**Responsibility** â€“ Performs the same LLM call as `compress` but within an `asyncio.Semaphore` to limit concurrency and updates the async progress bar.  

**Interactions** â€“ Awaits `AsyncModel.get_answer_without_history`; updates `BaseProgress`.  

**Flow** â€“ Build prompt (identical to `compress`), `await model.get_answer_without_history`, then `progress_bar.update_task()`.

**Data** â€“ Input: `data: str`, `project_settings`, `model: AsyncModel`, `compress_power`, `semaphore`, `progress_bar`.  
Output: `str` compressed answer.

--- 
<a name="async-compress-and-compare"></a>
## `async_compress_and_compare` â€“ parallel batch compression  

**Responsibility** â€“ Dispatches `async_compress` for every element in `data` (default concurrencyâ€¯=â€¯4), gathers results, then reâ€‘chunks them into groups of `compress_power`.  

**Interactions** â€“ Creates an `asyncio.Semaphore(4)`, populates `tasks`, awaits `asyncio.gather`, uses `BaseProgress` for a subâ€‘task.  

**Result** â€“ Returns `list[str]` where each entry is a newlineâ€‘joined group of compressed chunks.

--- 
<a name="compress-and-compare-sync"></a>
## `compress_and_compare` â€“ batch sync compression  

**Responsibility** â€“ Groups input files into chunks of size `compress_power`, compresses each file with `compress`, concatenates results per chunk, and reports progress.  

**Interactions** â€“ Uses `BaseProgress` to create/update a subâ€‘task; repeatedly calls `compress`.  

**Logic** â€“  
* Allocate result list sized to `ceil(len(data)/compress_power)`.  
* Loop over `data`, compute `curr_index = i // compress_power`, append each `compress` result plus newline.  
* Update progress bar each iteration, then remove subâ€‘task.  

**Data** â€“ Input: `data: list[str]`, `model`, `project_settings`, `compress_power`, `progress_bar`.  
Output: `list[str]` where each element contains the concatenated compressed chunk.

--- 
<a name="compress-single-pass"></a>
## `compress` â€“ singleâ€‘pass LLM compression  

**Responsibility** â€“ Sends a raw code string to the LLM together with the projectâ€‘wide system prompt and a sizeâ€‘adjusted compression prompt, returning the modelâ€™s answer.  

**Interactions** â€“ Calls `Model.get_answer_without_history`. No filesystem or network sideâ€‘effects other than the LLM request.  

**Technical flow** â€“  
1. Build `prompt` list (`system`â€¯â†’â€¯project prompt, `system`â€¯â†’â€¯base compress text, `user`â€¯â†’â€¯`data`).  
2. Invoke `model.get_answer_without_history(prompt=prompt)`.  
3. Return the answer string.  

**Data** â€“ Input: `data: str`, `project_settings: ProjectSettings`, `model: Model`, `compress_power: int`.  
Output: compressed `str`. No mutation of arguments.

--- 
<a name="compress-to-one"></a>
## `compress_to_one` â€“ iterative reduction to a single summary  

**Responsibility** â€“ Repeatedly compresses the list of strings until only one element remains, optionally using the async path.  

**Interactions** â€“ Calls either `compress_and_compare` or `async_compress_and_compare` inside a `while len(data) > 1` loop; updates a `BaseProgress` subâ€‘task each iteration.  

**Data** â€“ Input: `data: list[str]`, `model`, `project_settings`, `compress_power`, `use_async`, `progress_bar`.  
Output: final aggregated string (`data[0]`).

--- 
<a name="code-mix-generation"></a>  
## Code Mix Generation Workflow  

`generate_code_file()`  
1. Logs start.  
2. Instantiates `CodeMix(project_directory, config.ignore_files)`.  
3. Calls `build_repo_content` â†’ writes a concatenated source snapshot to `code_mix.txt`.  
4. Logs completion and advances the progress bar. 

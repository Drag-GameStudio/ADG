[2026-01-25 14:31:14.844339] [INFO] Starting code mix generation...
[2026-01-25 14:31:14.849304] [INFO] Ignored: /home/runner/work/ADG/ADG/.auto_doc_cache
[2026-01-25 14:31:14.849545] [INFO] Ignored: /home/runner/work/ADG/ADG/.auto_doc_cache/code_mix.txt
[2026-01-25 14:31:14.849761] [INFO] Ignored: /home/runner/work/ADG/ADG/.auto_doc_cache/report.txt
[2026-01-25 14:31:14.850167] [INFO] Ignored: /home/runner/work/ADG/ADG/.git
[2026-01-25 14:31:14.850416] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/FETCH_HEAD
[2026-01-25 14:31:14.850643] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/HEAD
[2026-01-25 14:31:14.850866] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/config
[2026-01-25 14:31:14.851109] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/config.worktree
[2026-01-25 14:31:14.851347] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/description
[2026-01-25 14:31:14.851570] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks
[2026-01-25 14:31:14.851811] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/applypatch-msg.sample
[2026-01-25 14:31:14.852052] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/commit-msg.sample
[2026-01-25 14:31:14.852310] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/fsmonitor-watchman.sample
[2026-01-25 14:31:14.852549] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/post-update.sample
[2026-01-25 14:31:14.852784] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-applypatch.sample
[2026-01-25 14:31:14.853019] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-commit.sample
[2026-01-25 14:31:14.853271] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-merge-commit.sample
[2026-01-25 14:31:14.853510] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-push.sample
[2026-01-25 14:31:14.853745] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-rebase.sample
[2026-01-25 14:31:14.853978] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-receive.sample
[2026-01-25 14:31:14.854236] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/prepare-commit-msg.sample
[2026-01-25 14:31:14.854477] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/push-to-checkout.sample
[2026-01-25 14:31:14.854719] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/sendemail-validate.sample
[2026-01-25 14:31:14.854953] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/update.sample
[2026-01-25 14:31:14.855190] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/index
[2026-01-25 14:31:14.855416] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/info
[2026-01-25 14:31:14.855650] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/info/exclude
[2026-01-25 14:31:14.855867] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs
[2026-01-25 14:31:14.856118] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/HEAD
[2026-01-25 14:31:14.856358] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs
[2026-01-25 14:31:14.856605] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/heads
[2026-01-25 14:31:14.856868] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/heads/main
[2026-01-25 14:31:14.857142] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/remotes
[2026-01-25 14:31:14.857415] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/remotes/origin
[2026-01-25 14:31:14.857706] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/remotes/origin/main
[2026-01-25 14:31:14.857927] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects
[2026-01-25 14:31:14.858187] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/00
[2026-01-25 14:31:14.858447] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/00/3ed33f03ba24320be674acc37e2f484440960d
[2026-01-25 14:31:14.858681] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/08
[2026-01-25 14:31:14.858930] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/08/3bb9598f2f3dee44e7b6d3358c592538e0c708
[2026-01-25 14:31:14.859187] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/16
[2026-01-25 14:31:14.859447] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/16/c6bd78d24630228f2f308279feee5992b421d5
[2026-01-25 14:31:14.859835] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/26
[2026-01-25 14:31:14.860252] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/26/232583984feff25da44d47972f553bd00e4daa
[2026-01-25 14:31:14.860517] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/39
[2026-01-25 14:31:14.860771] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/39/cfc01845012732724d5882ee373a829a8f3ac7
[2026-01-25 14:31:14.861009] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/45
[2026-01-25 14:31:14.861287] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/45/933fef400c288e7935a8532c86c09854697bc6
[2026-01-25 14:31:14.861544] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/45/c8c97b68d9edc9608f17662ae5a56937db4722
[2026-01-25 14:31:14.861778] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/49
[2026-01-25 14:31:14.862028] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/49/c6af63931d6ccd36e62c0fd5a8bdf8d7dc2267
[2026-01-25 14:31:14.862307] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/49/ff01a4dbbedc8418925e8d09ca08726ae99a2a
[2026-01-25 14:31:14.862547] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/4d
[2026-01-25 14:31:14.862796] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/4d/887c5d0ba32a7517cce5fc00b7935bfd365b6e
[2026-01-25 14:31:14.863028] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/50
[2026-01-25 14:31:14.863303] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/50/de1f7cf4fe170128039ffa6d0cc6f3f6d6cc33
[2026-01-25 14:31:14.863542] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/60
[2026-01-25 14:31:14.863793] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/60/9cf414f047c3e4ede56647a5345d9a9f9491bd
[2026-01-25 14:31:14.864023] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/62
[2026-01-25 14:31:14.864293] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/62/18e73336f4ccb562643cd4202a628d14cf784f
[2026-01-25 14:31:14.864531] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6a
[2026-01-25 14:31:14.864788] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6a/2dd502eb649bb8e804238af60031db553c13ea
[2026-01-25 14:31:14.865037] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6a/4ee663203935f06c6f32d4d056257d4f4478a7
[2026-01-25 14:31:14.865315] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6a/82f901020b547af2ebea8d38d3eb6edc2ed27a
[2026-01-25 14:31:14.865551] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6f
[2026-01-25 14:31:14.865812] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6f/130bfccfc73d6fe93fe33cd3772d640379f909
[2026-01-25 14:31:14.866043] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/76
[2026-01-25 14:31:14.866318] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/76/8b9b034724a0c1590521857e7a874a831a410e
[2026-01-25 14:31:14.866556] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7a
[2026-01-25 14:31:14.866803] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7a/972b37d81fa3afd7c12c29ac0519b75b8965e1
[2026-01-25 14:31:14.867032] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7e
[2026-01-25 14:31:14.867299] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7e/f15dd6d3c3dd489f000b4267a54b0d72c0d559
[2026-01-25 14:31:14.867534] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/84
[2026-01-25 14:31:14.867779] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/84/00c2b5c8734df41836d301824574b989b0ec41
[2026-01-25 14:31:14.868011] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8a
[2026-01-25 14:31:14.868277] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8a/b47c5bc8110cef05224baba95761fd822aa22a
[2026-01-25 14:31:14.868512] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8b
[2026-01-25 14:31:14.868760] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8b/245728fc7d88241811d1ac98534b1b6217ca3e
[2026-01-25 14:31:14.868989] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8c
[2026-01-25 14:31:14.869253] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8c/c99da379760aec8ce1f9b9e8a40f24a3068630
[2026-01-25 14:31:14.869498] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8e
[2026-01-25 14:31:14.869746] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8e/bfe0362de1ecb03cac77ca927047968df656c5
[2026-01-25 14:31:14.869975] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/94
[2026-01-25 14:31:14.870247] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/94/e480e5d5e7e16b2be7889ae9e81f561e9bdb50
[2026-01-25 14:31:14.870486] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/95
[2026-01-25 14:31:14.870732] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/95/461ed5aa7c078b32cb3c2b1cd7ffe1d8228960
[2026-01-25 14:31:14.870962] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a8
[2026-01-25 14:31:14.871233] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a8/476127e3f807c0aa99fa808560ec2e5584dff2
[2026-01-25 14:31:14.871473] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/b6
[2026-01-25 14:31:14.871722] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/b6/d812aceb9c792cb2019b9e244e1dbaa4ad74a1
[2026-01-25 14:31:14.871953] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ca
[2026-01-25 14:31:14.872219] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ca/3f28b6a4a95a7176cdf2d78622ceb1151e186d
[2026-01-25 14:31:14.872460] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/cf
[2026-01-25 14:31:14.872711] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/cf/ece141dd11de038a6a441c2ae64efcaae9b9d2
[2026-01-25 14:31:14.872944] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d9
[2026-01-25 14:31:14.873218] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d9/c5e7426b4d3d800f9cca0b469d6f0c60fe27b7
[2026-01-25 14:31:14.873457] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/db
[2026-01-25 14:31:14.873705] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/db/80ec4419abf97b42fd51f286a6dd599d8d77ce
[2026-01-25 14:31:14.873937] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/dc
[2026-01-25 14:31:14.874210] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/dc/7530f616e6bcc2654f9d74d88e70324ba15136
[2026-01-25 14:31:14.874466] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/dc/dbbc7b902f7e693e5e9d6bccb587bd1428f11d
[2026-01-25 14:31:14.874707] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e6
[2026-01-25 14:31:14.874956] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391
[2026-01-25 14:31:14.875209] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e7
[2026-01-25 14:31:14.875463] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e7/746f47e059234c8b861cf312d97132a810e145
[2026-01-25 14:31:14.875695] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/eb
[2026-01-25 14:31:14.875944] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/eb/61cc69ae7750572309f5065b8ed92fd5025302
[2026-01-25 14:31:14.876195] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ef
[2026-01-25 14:31:14.876448] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ef/9fb37763d620555f3e448337bcdb1ac580b349
[2026-01-25 14:31:14.876679] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/f7
[2026-01-25 14:31:14.876925] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/f7/1ce45887ac33204270a6b1fea988ffe72b2a99
[2026-01-25 14:31:14.877176] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fb
[2026-01-25 14:31:14.877431] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fb/8b56ad03d86c61f46cb2c93a162215b23ce129
[2026-01-25 14:31:14.877663] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fe
[2026-01-25 14:31:14.877911] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fe/882abdc27cff326fa1dd9bf9027176af633c8c
[2026-01-25 14:31:14.878169] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/info
[2026-01-25 14:31:14.878410] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/pack
[2026-01-25 14:31:14.878628] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs
[2026-01-25 14:31:14.878861] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/heads
[2026-01-25 14:31:14.879139] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/heads/main
[2026-01-25 14:31:14.879381] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/remotes
[2026-01-25 14:31:14.879631] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/remotes/origin
[2026-01-25 14:31:14.879895] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/remotes/origin/main
[2026-01-25 14:31:14.880148] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/tags
[2026-01-25 14:31:14.880373] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/shallow
[2026-01-25 14:31:14.882533] [INFO] Ignored: /home/runner/work/ADG/ADG/.gitignore
[2026-01-25 14:31:14.882771] [INFO] Ignored: /home/runner/work/ADG/ADG/README.md
[2026-01-25 14:31:14.883818] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/__pycache__
[2026-01-25 14:31:14.884049] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/__pycache__/__init__.cpython-312.pyc
[2026-01-25 14:31:14.884293] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/__pycache__/manage.cpython-312.pyc
[2026-01-25 14:31:14.884823] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/auto_runner/__pycache__
[2026-01-25 14:31:14.885059] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/auto_runner/__pycache__/config_reader.cpython-312.pyc
[2026-01-25 14:31:14.885313] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/auto_runner/__pycache__/run_file.cpython-312.pyc
[2026-01-25 14:31:14.886818] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/__pycache__
[2026-01-25 14:31:14.887062] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/__pycache__/__init__.cpython-312.pyc
[2026-01-25 14:31:14.887317] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/__pycache__/exceptions.cpython-312.pyc
[2026-01-25 14:31:14.887880] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/config/__pycache__
[2026-01-25 14:31:14.888152] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/config/__pycache__/config.cpython-312.pyc
[2026-01-25 14:31:14.889386] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/models/__pycache__
[2026-01-25 14:31:14.889644] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/models/__pycache__/gpt_model.cpython-312.pyc
[2026-01-25 14:31:14.889894] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/models/__pycache__/model.cpython-312.pyc
[2026-01-25 14:31:14.891443] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/__pycache__
[2026-01-25 14:31:14.891691] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/__pycache__/__init__.cpython-312.pyc
[2026-01-25 14:31:14.891935] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/__pycache__/base_factory.cpython-312.pyc
[2026-01-25 14:31:14.892817] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/modules/__pycache__
[2026-01-25 14:31:14.893072] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/modules/__pycache__/general_modules.cpython-312.pyc
[2026-01-25 14:31:14.893343] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/modules/__pycache__/intro.cpython-312.pyc
[2026-01-25 14:31:14.894851] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__
[2026-01-25 14:31:14.895116] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/code_mix.cpython-312.pyc
[2026-01-25 14:31:14.895364] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/compressor.cpython-312.pyc
[2026-01-25 14:31:14.895600] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/postprocess.cpython-312.pyc
[2026-01-25 14:31:14.895835] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/settings.cpython-312.pyc
[2026-01-25 14:31:14.896068] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/spliter.cpython-312.pyc
[2026-01-25 14:31:14.898512] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__
[2026-01-25 14:31:14.898765] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__/__init__.cpython-312.pyc
[2026-01-25 14:31:14.899000] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__/logging.cpython-312.pyc
[2026-01-25 14:31:14.899260] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__/progress_base.cpython-312.pyc
[2026-01-25 14:31:14.935588] [INFO] Code mix generation completed.
[2026-01-25 14:31:14.936973] [INFO] Starting synchronous documentation generation by parts...
[2026-01-25 14:31:14.937256] [INFO] Starting data splitting...
[2026-01-25 14:31:14.937328] [INFO] Data split into 12 parts based on max symbols 5000.
[2026-01-25 14:31:14.937354] [INFO] Starting documentation generation by parts...
[2026-01-25 14:31:14.937410] [INFO] Generating documentation for a part...
[2026-01-25 14:31:14.937452] [INFO] Generating answer...
[2026-01-25 14:31:16.923181] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:31:16.923244] [INFO] Answer: <a name="package-initialization"></a>
## üì¶ Package Initialization ‚Äì‚ÄØAutomatic Logger Bootstrap

**File:** `autodocgenerator/__init__.py`

### Responsibility
This tiny entry‚Äëpoint prepares the **Auto Doc Generator (ADG)** package for use by:

1. Emitting a short banner (`print("ADG")`) the moment the package is imported ‚Äì useful for CI visibility.  
2. Exposing a **singleton logger** (`logger`) that the rest of the codebase (`engine`, `factory`, `ui`, etc.) can import without having to instantiate their own logger objects.

### Core Logic
```python
print("ADG")
from .ui.logging import BaseLogger, BaseLoggerTemplate, InfoLog, ErrorLog, WarningLog

logger = BaseLogger()
logger.set_logger(BaseLoggerTemplate())
```

| Element | What it does |
|---------|--------------|
| `print("ADG")` | Writes a one‚Äëline identifier to stdout during import. |
| `BaseLogger` | Central logging fa√ßade (wraps Python‚Äôs `logging` module). |
| `BaseLoggerTemplate` | Supplies default handlers/formatters (e.g., Rich‚Äëstyled console output). |
| `logger = BaseLogger()` | Creates a **global logger instance** that lives for the lifetime of the interpreter. |
| `logger.set_logger(BaseLoggerTemplate())` | Configures the global logger with the default template, enabling `InfoLog`, `ErrorLog`, `WarningLog` helpers throughout the project. |

### Interaction with Other Components
- **Engine & Factory modules** import `logger` (`from autodocgenerator import logger`) to record progress, errors, and debug information.
- **UI components** (`autodocgenerator/ui/logging.py`) define the concrete logging behaviour; this file merely wires the concrete implementation into a shared instance.
- Because the logger is created at import time, any sub‚Äëmodule loaded **after** `autodocgenerator` will automatically share the same configured logger, guaranteeing consistent log formatting and destination.

### Assumptions & Side Effects
- **Assumption:** The `ui.logging` module provides `BaseLogger`, `BaseLoggerTemplate`, and the log‚Äëlevel helpers. If those symbols are missing or renamed, import will fail, breaking the entire package initialization.
- **Side effect:** The `print` statement produces output on every import, which may be noisy in production but is intentional for CI diagnostics.
- **Output:** No return value; the module‚Äôs sole export is the `logger` object.

### Usage Example
```python
# In any sub‚Äëmodule:
from autodocgenerator import logger

logger.info("Documentation generation started")
```

The above call will emit a formatted info message using the globally configured `BaseLoggerTemplate`.
[2026-01-25 14:31:16.923292] [INFO] Documentation for part generated. total length: 2593
[2026-01-25 14:31:16.923317] [INFO] Part Documentation: <a name="package-initialization"></a>
## üì¶ Package Initialization ‚Äì‚ÄØAutomatic Logger Bootstrap

**File:** `autodocgenerator/__init__.py`

### Responsibility
This tiny entry‚Äëpoint prepares the **Auto Doc Generator (ADG)** package for use by:

1. Emitting a short banner (`print("ADG")`) the moment the package is imported ‚Äì useful for CI visibility.  
2. Exposing a **singleton logger** (`logger`) that the rest of the codebase (`engine`, `factory`, `ui`, etc.) can import without having to instantiate their own logger objects.

### Core Logic
```python
print("ADG")
from .ui.logging import BaseLogger, BaseLoggerTemplate, InfoLog, ErrorLog, WarningLog

logger = BaseLogger()
logger.set_logger(BaseLoggerTemplate())
```

| Element | What it does |
|---------|--------------|
| `print("ADG")` | Writes a one‚Äëline identifier to stdout during import. |
| `BaseLogger` | Central logging fa√ßade (wraps Python‚Äôs `logging` module). |
| `BaseLoggerTemplate` | Supplies default handlers/formatters (e.g., Rich‚Äëstyled console output). |
| `logger = BaseLogger()` | Creates a **global logger instance** that lives for the lifetime of the interpreter. |
| `logger.set_logger(BaseLoggerTemplate())` | Configures the global logger with the default template, enabling `InfoLog`, `ErrorLog`, `WarningLog` helpers throughout the project. |

### Interaction with Other Components
- **Engine & Factory modules** import `logger` (`from autodocgenerator import logger`) to record progress, errors, and debug information.
- **UI components** (`autodocgenerator/ui/logging.py`) define the concrete logging behaviour; this file merely wires the concrete implementation into a shared instance.
- Because the logger is created at import time, any sub‚Äëmodule loaded **after** `autodocgenerator` will automatically share the same configured logger, guaranteeing consistent log formatting and destination.

### Assumptions & Side Effects
- **Assumption:** The `ui.logging` module provides `BaseLogger`, `BaseLoggerTemplate`, and the log‚Äëlevel helpers. If those symbols are missing or renamed, import will fail, breaking the entire package initialization.
- **Side effect:** The `print` statement produces output on every import, which may be noisy in production but is intentional for CI diagnostics.
- **Output:** No return value; the module‚Äôs sole export is the `logger` object.

### Usage Example
```python
# In any sub‚Äëmodule:
from autodocgenerator import logger

logger.info("Documentation generation started")
```

The above call will emit a formatted info message using the globally configured `BaseLoggerTemplate`.
[2026-01-25 14:31:16.923388] [INFO] Generating documentation for a part...
[2026-01-25 14:31:16.923416] [INFO] Generating answer...
[2026-01-25 14:31:18.526595] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:31:18.526651] [INFO] Answer: **<a name="config-reader"></a>Config‚ÄØReader & Settings Builder**  
`config_reader.py` centralises all static configuration for the autodoc pipeline.  
* **Responsibility** ‚Äì Parse the user‚Äëprovided `autodocconfig.yml`, materialise a rich `Config` object and expose ready‚Äëto‚Äëuse factories.  
* **Key classes**  
  * `ProjectConfigSettings` ‚Äì tiny container for runtime flags (`save_logs`, `log_level`). `load_settings` mutates the instance from a dict.  
  * `Config` ‚Äì aggregates ignore patterns, language, project metadata, custom modules and a `ProjectConfigSettings` instance. Fluent setters (`set_language`, `add_ignore_file`, ‚Ä¶) return `self` for chaining.  
  * `read_config` ‚Äì entry‚Äëpoint called by the runner; it loads YAML, populates a `Config`, registers custom modules (`CustomModule`) and returns the populated object.  
* **Interactions** ‚Äì `Config.get_project_settings` builds a `ProjectSettings` (used later by `Manager`). `Config.get_doc_factory` creates two `DocFactory` instances: one for user‚Äëdefined custom modules, another for built‚Äëin intro modules (`IntroLinks`, optionally `IntroText`).  

---

**<a name="run-file"></a>Run‚ÄØFile ‚Äì Orchestrating Documentation Generation**  
`run_file.py` glues together the parsed configuration, language models, and the core `Manager`.  

* **Responsibility** ‚Äì Initialise GPT models, construct a `Manager` with all runtime parameters, invoke the generation steps, clean the cache and return the final markdown document.  
* **Core function** `gen_doc`  
  * **Inputs** ‚Äì `ProjectSettings`, `ProjectConfigSettings`, ignore list, project root, and two `DocFactory` objects.  
  * **Flow** ‚Äì  
    1. Create `GPTModel` (sync) and `AsyncGPTModel`.  
    2. Instantiate `Manager` (provides file scanning, chunking, LLM calls, progress UI).  
    3. Run a fixed sequence of generation methods: source‚Äëcode docs, global info, doc parts, then the two factories.  
    4. Clear temporary cache and read the assembled output via `manager.read_file_by_file_key("output_doc")`.  
* **Side‚Äëeffects** ‚Äì Writes intermediate files to the `.auto_doc_cache` directory, logs progress via `ConsoleGtiHubProgress`, and may produce a file‚Äëbased logger if `pcs.save_logs` is enabled.  

Together, these modules translate a declarative YAML config into a fully‚Äërendered documentation artefact, feeding the rest of the system (e.g., UI, CI hooks) with a single string result.
[2026-01-25 14:31:18.526697] [INFO] Documentation for part generated. total length: 2409
[2026-01-25 14:31:18.526722] [INFO] Part Documentation: **<a name="config-reader"></a>Config‚ÄØReader & Settings Builder**  
`config_reader.py` centralises all static configuration for the autodoc pipeline.  
* **Responsibility** ‚Äì Parse the user‚Äëprovided `autodocconfig.yml`, materialise a rich `Config` object and expose ready‚Äëto‚Äëuse factories.  
* **Key classes**  
  * `ProjectConfigSettings` ‚Äì tiny container for runtime flags (`save_logs`, `log_level`). `load_settings` mutates the instance from a dict.  
  * `Config` ‚Äì aggregates ignore patterns, language, project metadata, custom modules and a `ProjectConfigSettings` instance. Fluent setters (`set_language`, `add_ignore_file`, ‚Ä¶) return `self` for chaining.  
  * `read_config` ‚Äì entry‚Äëpoint called by the runner; it loads YAML, populates a `Config`, registers custom modules (`CustomModule`) and returns the populated object.  
* **Interactions** ‚Äì `Config.get_project_settings` builds a `ProjectSettings` (used later by `Manager`). `Config.get_doc_factory` creates two `DocFactory` instances: one for user‚Äëdefined custom modules, another for built‚Äëin intro modules (`IntroLinks`, optionally `IntroText`).  

---

**<a name="run-file"></a>Run‚ÄØFile ‚Äì Orchestrating Documentation Generation**  
`run_file.py` glues together the parsed configuration, language models, and the core `Manager`.  

* **Responsibility** ‚Äì Initialise GPT models, construct a `Manager` with all runtime parameters, invoke the generation steps, clean the cache and return the final markdown document.  
* **Core function** `gen_doc`  
  * **Inputs** ‚Äì `ProjectSettings`, `ProjectConfigSettings`, ignore list, project root, and two `DocFactory` objects.  
  * **Flow** ‚Äì  
    1. Create `GPTModel` (sync) and `AsyncGPTModel`.  
    2. Instantiate `Manager` (provides file scanning, chunking, LLM calls, progress UI).  
    3. Run a fixed sequence of generation methods: source‚Äëcode docs, global info, doc parts, then the two factories.  
    4. Clear temporary cache and read the assembled output via `manager.read_file_by_file_key("output_doc")`.  
* **Side‚Äëeffects** ‚Äì Writes intermediate files to the `.auto_doc_cache` directory, logs progress via `ConsoleGtiHubProgress`, and may produce a file‚Äëbased logger if `pcs.save_logs` is enabled.  

Together, these modules translate a declarative YAML config into a fully‚Äërendered documentation artefact, feeding the rest of the system (e.g., UI, CI hooks) with a single string result.
[2026-01-25 14:31:18.526783] [INFO] Generating documentation for a part...
[2026-01-25 14:31:18.526809] [INFO] Generating answer...
[2026-01-25 14:31:33.172312] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:31:33.172370] [INFO] Answer: **## <a name="prompt-templates"></a>Prompt templates ‚Äì static instruction blocks**  
The module houses the core textual prompts (`BASE_SYSTEM_TEXT`, `BASE_PART_COMPLITE_TEXT`, `BASE_INTRODACTION_CREATE_TEXT`, `BASE_INTRO_CREATE`, `BASE_SETTINGS_PROMPT`).  
These strings are injected into the LLM request payloads by the **PromptFactory** (see the factories section). They define the conversational role, step‚Äëby‚Äëstep workflow, and output format for every generation stage (system‚Äëlevel, part‚Äëlevel, executive‚Äëtree, project overview, and settings memorisation). No runtime logic resides here ‚Äì they are immutable resources.

**## <a name="compression-prompt-factory"></a>Dynamic compression prompt generator**  
`get_BASE_COMPRESS_TEXT(start, power)` builds a sized‚Äëaware prompt for ‚Äúlarge‚Äësnippet compression‚Äù.  
*Inputs*: `start` (estimated max characters of the snippet) and `power` (division factor controlling summary length).  
*Output*: A formatted multi‚Äëline string that instructs the model to analyse, summarise, and provide a strict usage example. The function is called by the **CompressFactory** when the engine decides a snippet exceeds the configured threshold. No side‚Äëeffects besides returning the string.

**## <a name="environment-setup"></a>Environment initialisation**  
At import time the module loads `.env` via `dotenv.load_dotenv()`, extracts `API_KEY`, and raises an exception if it is missing. It also defines `MODELS_NAME`, the list of model identifiers used by the **ModelProvider** throughout the system. This step guarantees that downstream components (LLM client, progress reporters) have the required credentials and model catalogue available.  

*Assumptions*: `.env` exists in the working directory; the key is named `API_KEY`.  
*Side‚Äëeffects*: environment variable lookup, possible `Exception` on missing key.
[2026-01-25 14:31:33.172413] [INFO] Documentation for part generated. total length: 1845
[2026-01-25 14:31:33.172437] [INFO] Part Documentation: **## <a name="prompt-templates"></a>Prompt templates ‚Äì static instruction blocks**  
The module houses the core textual prompts (`BASE_SYSTEM_TEXT`, `BASE_PART_COMPLITE_TEXT`, `BASE_INTRODACTION_CREATE_TEXT`, `BASE_INTRO_CREATE`, `BASE_SETTINGS_PROMPT`).  
These strings are injected into the LLM request payloads by the **PromptFactory** (see the factories section). They define the conversational role, step‚Äëby‚Äëstep workflow, and output format for every generation stage (system‚Äëlevel, part‚Äëlevel, executive‚Äëtree, project overview, and settings memorisation). No runtime logic resides here ‚Äì they are immutable resources.

**## <a name="compression-prompt-factory"></a>Dynamic compression prompt generator**  
`get_BASE_COMPRESS_TEXT(start, power)` builds a sized‚Äëaware prompt for ‚Äúlarge‚Äësnippet compression‚Äù.  
*Inputs*: `start` (estimated max characters of the snippet) and `power` (division factor controlling summary length).  
*Output*: A formatted multi‚Äëline string that instructs the model to analyse, summarise, and provide a strict usage example. The function is called by the **CompressFactory** when the engine decides a snippet exceeds the configured threshold. No side‚Äëeffects besides returning the string.

**## <a name="environment-setup"></a>Environment initialisation**  
At import time the module loads `.env` via `dotenv.load_dotenv()`, extracts `API_KEY`, and raises an exception if it is missing. It also defines `MODELS_NAME`, the list of model identifiers used by the **ModelProvider** throughout the system. This step guarantees that downstream components (LLM client, progress reporters) have the required credentials and model catalogue available.  

*Assumptions*: `.env` exists in the working directory; the key is named `API_KEY`.  
*Side‚Äëeffects*: environment variable lookup, possible `Exception` on missing key.
[2026-01-25 14:31:33.172496] [INFO] Generating documentation for a part...
[2026-01-25 14:31:33.172523] [INFO] Generating answer...
[2026-01-25 14:31:58.594156] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:31:58.594217] [INFO] Answer: **## <a name="model-exception"></a>ModelExhaustedException**  
Raised when the internal list `regen_models_name` becomes empty ‚Äì i.e. every configured model has failed. It bubbles up to the calling factory (`CompressFactory`/`GenerateFactory`) so the caller can abort or report a fatal error. No side‚Äëeffects besides the exception itself.

**## <a name="model-base"></a>Base model hierarchy (`ParentModel`, `Model`, `AsyncModel`)**  
*Responsibility*: Provide shared infrastructure for all LLM wrappers.  
*Key logic*:  
- Loads `API_KEY` and `MODELS_NAME` from the global config.  
- Stores a mutable `History` (system prompt + later messages).  
- Randomises `regen_models_name` when `use_random=True` to spread load across models.  
- Tracks `current_model_index` for round‚Äërobin fallback.  
*Interfaces*:  
- `generate_answer` (sync/async placeholder).  
- `get_answer` / `get_answer_without_history` ‚Äì add user/assistant entries to `History` and delegate to `generate_answer`.  
*Assumptions*: `BASE_SYSTEM_TEXT`, `API_KEY`, `MODELS_NAME` are defined; `History` is cheap to copy.

**## <a name="gpt-model"></a>Concrete implementations (`GPTModel`, `AsyncGPTModel`)**  
Both inherit from the respective base class and instantiate a Groq client (`Groq` or `AsyncGroq`) with the supplied `api_key`.  
`generate_answer` builds the message payload (`self.history.history` or a raw `prompt`), then enters a **fallback loop**:  
1. Pick `model_name = regen_models_name[current_model_index]`.  
2. Call `client.chat.completions.create`.  
3. On any exception log a warning, advance `current_model_index` (wrap‚Äëaround), and retry.  
If the list is exhausted, `ModelExhaustedException` is raised.  
Successful calls return `chat_completion.choices[0].message.content` and emit informational logs via `BaseLogger`.  
*Side‚Äëeffects*: network request, logging, possible change of `current_model_index`.  

These components are the bridge between the **factory layer** (which selects `GPTModel` or `AsyncGPTModel`) and the **UI/logging subsystem**, ensuring robust model selection, history management, and transparent error handling.
[2026-01-25 14:31:58.594263] [INFO] Documentation for part generated. total length: 2124
[2026-01-25 14:31:58.594287] [INFO] Part Documentation: **## <a name="model-exception"></a>ModelExhaustedException**  
Raised when the internal list `regen_models_name` becomes empty ‚Äì i.e. every configured model has failed. It bubbles up to the calling factory (`CompressFactory`/`GenerateFactory`) so the caller can abort or report a fatal error. No side‚Äëeffects besides the exception itself.

**## <a name="model-base"></a>Base model hierarchy (`ParentModel`, `Model`, `AsyncModel`)**  
*Responsibility*: Provide shared infrastructure for all LLM wrappers.  
*Key logic*:  
- Loads `API_KEY` and `MODELS_NAME` from the global config.  
- Stores a mutable `History` (system prompt + later messages).  
- Randomises `regen_models_name` when `use_random=True` to spread load across models.  
- Tracks `current_model_index` for round‚Äërobin fallback.  
*Interfaces*:  
- `generate_answer` (sync/async placeholder).  
- `get_answer` / `get_answer_without_history` ‚Äì add user/assistant entries to `History` and delegate to `generate_answer`.  
*Assumptions*: `BASE_SYSTEM_TEXT`, `API_KEY`, `MODELS_NAME` are defined; `History` is cheap to copy.

**## <a name="gpt-model"></a>Concrete implementations (`GPTModel`, `AsyncGPTModel`)**  
Both inherit from the respective base class and instantiate a Groq client (`Groq` or `AsyncGroq`) with the supplied `api_key`.  
`generate_answer` builds the message payload (`self.history.history` or a raw `prompt`), then enters a **fallback loop**:  
1. Pick `model_name = regen_models_name[current_model_index]`.  
2. Call `client.chat.completions.create`.  
3. On any exception log a warning, advance `current_model_index` (wrap‚Äëaround), and retry.  
If the list is exhausted, `ModelExhaustedException` is raised.  
Successful calls return `chat_completion.choices[0].message.content` and emit informational logs via `BaseLogger`.  
*Side‚Äëeffects*: network request, logging, possible change of `current_model_index`.  

These components are the bridge between the **factory layer** (which selects `GPTModel` or `AsyncGPTModel`) and the **UI/logging subsystem**, ensuring robust model selection, history management, and transparent error handling.
[2026-01-25 14:31:58.594362] [INFO] Generating documentation for a part...
[2026-01-25 14:31:58.594389] [INFO] Generating answer...
[2026-01-25 14:32:14.096454] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:32:14.096514] [INFO] Answer: <a name="base_module_abstract_interface"></a>
## BaseModule ‚Äì Abstract Interface for Documentation Generators  

`BaseModule` defines the minimal contract for all factory‚Äëlevel generators.  
* **Responsibility** ‚Äì Provide a uniform `generate(info, model)` method that each concrete module must implement.  
* **Interaction** ‚Äì Modules receive a plain‚Äëdictionary `info` (containing pre‚Äëprocessed code, language, etc.) and a `Model` instance (sync or async) from the engine layer. No direct I/O occurs here; the method should be pure‚Äëlogic.  
* **Key Points**  
  * Inherits from `ABC`; the abstract method forces implementation in subclasses.  
  * Constructor does nothing but may be extended by subclasses for configuration.  

<a name="doc_factory_orchestrator"></a>
## DocFactory ‚Äì Orchestrator of Module Execution  

`DocFactory` wires together a list of `BaseModule` instances, handles progress reporting and logging, and concatenates their outputs.  

* **Responsibility** ‚Äì Sequentially run each module, aggregate their Markdown/HTML snippets, and expose a single documentation string.  
* **Inputs**  
  * `*modules` ‚Äì variadic list of instantiated `BaseModule` subclasses.  
  * `info: dict` ‚Äì shared context (code fragments, language, global data).  
  * `model: Model` ‚Äì the language model used by modules for LLM calls.  
  * `progress: BaseProgress` ‚Äì UI component that visualises sub‚Äëtask creation, updates and removal.  
* **Outputs** ‚Äì A single string `output` containing each module‚Äôs result separated by double new‚Äëlines.  
* **Side‚Äëeffects** ‚Äì  
  * Calls `progress.create_new_subtask`/`update_task`/`remove_subtask`.  
  * Emits `InfoLog` entries via `BaseLogger` for each module execution and its raw result (log level‚ÄØ2 for detailed output).  
* **Logic Flow**  
  1. Initialise empty `output`.  
  2. Register a sub‚Äëtask in the UI matching the number of modules.  
  3. For each module:  
     * `module_result = module.generate(info, model)`  
     * Append result to `output`.  
     * Log success and the raw snippet.  
     * Notify progress UI of completion.  
  4. Tear down the sub‚Äëtask and return the concatenated documentation.  

<a name="custom_module_user_defined_section"></a>
## CustomModule ‚Äì User‚ÄëDefined Section Generator  

Located in `factory/modules/general_modules.py`.  

* **Responsibility** ‚Äì Produce a custom description block (e.g., ‚ÄúArchitecture Overview‚Äù) based on a user‚Äësupplied caption.  
* **Key Flow**  
  1. Split the mixed source code (`info["code_mix"]`) into chunks ‚â§‚ÄØ7000 symbols via `split_data`.  
  2. Call `generete_custom_discription` (LLM‚Äëbacked) with the chunks, the shared `model`, the user `discription`, and target `language`.  
  3. Return the generated text.  

* **Assumptions** ‚Äì `info` contains keys `"code_mix"` and `"language"`; the model can handle the chunked payload.  

<a name="intro_modules_links_and_text"></a>
## IntroLinks & IntroText ‚Äì Front‚ÄëMatter Builders  

Found in `factory/modules/intro.py`.  

* **IntroLinks**  
  * Extracts all HTML anchors from `info["full_data"]` using `get_all_html_links`.  
  * Passes the link list to `get_links_intro`, which asks the LLM to craft a concise introduction to the project's linked resources.  
  * Returns the generated paragraph.  

* **IntroText**  
  * Retrieves the high‚Äëlevel project description from `info["global_data"]`.  
  * Calls `get_introdaction` (LLM‚Äëdriven) to turn that data into a natural‚Äëlanguage introduction.  

Both modules rely exclusively on the engine‚Äôs `Model` for generation and emit no side‚Äëeffects beyond the returned strings.  

---

**Overall Placement** ‚Äì These factory components sit between the **engine layer** (model selection, retry logic) and the **UI/logging subsystem**. They encapsulate domain‚Äëspecific documentation fragments, coordinate progress feedback, and centralise logging, allowing the higher‚Äëlevel `DocFactory` to produce a coherent, multi‚Äësection document from heterogeneous code bases.
[2026-01-25 14:32:14.096559] [INFO] Documentation for part generated. total length: 3971
[2026-01-25 14:32:14.096582] [INFO] Part Documentation: <a name="base_module_abstract_interface"></a>
## BaseModule ‚Äì Abstract Interface for Documentation Generators  

`BaseModule` defines the minimal contract for all factory‚Äëlevel generators.  
* **Responsibility** ‚Äì Provide a uniform `generate(info, model)` method that each concrete module must implement.  
* **Interaction** ‚Äì Modules receive a plain‚Äëdictionary `info` (containing pre‚Äëprocessed code, language, etc.) and a `Model` instance (sync or async) from the engine layer. No direct I/O occurs here; the method should be pure‚Äëlogic.  
* **Key Points**  
  * Inherits from `ABC`; the abstract method forces implementation in subclasses.  
  * Constructor does nothing but may be extended by subclasses for configuration.  

<a name="doc_factory_orchestrator"></a>
## DocFactory ‚Äì Orchestrator of Module Execution  

`DocFactory` wires together a list of `BaseModule` instances, handles progress reporting and logging, and concatenates their outputs.  

* **Responsibility** ‚Äì Sequentially run each module, aggregate their Markdown/HTML snippets, and expose a single documentation string.  
* **Inputs**  
  * `*modules` ‚Äì variadic list of instantiated `BaseModule` subclasses.  
  * `info: dict` ‚Äì shared context (code fragments, language, global data).  
  * `model: Model` ‚Äì the language model used by modules for LLM calls.  
  * `progress: BaseProgress` ‚Äì UI component that visualises sub‚Äëtask creation, updates and removal.  
* **Outputs** ‚Äì A single string `output` containing each module‚Äôs result separated by double new‚Äëlines.  
* **Side‚Äëeffects** ‚Äì  
  * Calls `progress.create_new_subtask`/`update_task`/`remove_subtask`.  
  * Emits `InfoLog` entries via `BaseLogger` for each module execution and its raw result (log level‚ÄØ2 for detailed output).  
* **Logic Flow**  
  1. Initialise empty `output`.  
  2. Register a sub‚Äëtask in the UI matching the number of modules.  
  3. For each module:  
     * `module_result = module.generate(info, model)`  
     * Append result to `output`.  
     * Log success and the raw snippet.  
     * Notify progress UI of completion.  
  4. Tear down the sub‚Äëtask and return the concatenated documentation.  

<a name="custom_module_user_defined_section"></a>
## CustomModule ‚Äì User‚ÄëDefined Section Generator  

Located in `factory/modules/general_modules.py`.  

* **Responsibility** ‚Äì Produce a custom description block (e.g., ‚ÄúArchitecture Overview‚Äù) based on a user‚Äësupplied caption.  
* **Key Flow**  
  1. Split the mixed source code (`info["code_mix"]`) into chunks ‚â§‚ÄØ7000 symbols via `split_data`.  
  2. Call `generete_custom_discription` (LLM‚Äëbacked) with the chunks, the shared `model`, the user `discription`, and target `language`.  
  3. Return the generated text.  

* **Assumptions** ‚Äì `info` contains keys `"code_mix"` and `"language"`; the model can handle the chunked payload.  

<a name="intro_modules_links_and_text"></a>
## IntroLinks & IntroText ‚Äì Front‚ÄëMatter Builders  

Found in `factory/modules/intro.py`.  

* **IntroLinks**  
  * Extracts all HTML anchors from `info["full_data"]` using `get_all_html_links`.  
  * Passes the link list to `get_links_intro`, which asks the LLM to craft a concise introduction to the project's linked resources.  
  * Returns the generated paragraph.  

* **IntroText**  
  * Retrieves the high‚Äëlevel project description from `info["global_data"]`.  
  * Calls `get_introdaction` (LLM‚Äëdriven) to turn that data into a natural‚Äëlanguage introduction.  

Both modules rely exclusively on the engine‚Äôs `Model` for generation and emit no side‚Äëeffects beyond the returned strings.  

---

**Overall Placement** ‚Äì These factory components sit between the **engine layer** (model selection, retry logic) and the **UI/logging subsystem**. They encapsulate domain‚Äëspecific documentation fragments, coordinate progress feedback, and centralise logging, allowing the higher‚Äëlevel `DocFactory` to produce a coherent, multi‚Äësection document from heterogeneous code bases.
[2026-01-25 14:32:14.096676] [INFO] Generating documentation for a part...
[2026-01-25 14:32:14.096704] [INFO] Generating answer...
[2026-01-25 14:32:43.529058] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:32:43.529145] [INFO] Answer: <a name="manager_overview"></a>
## Manager ‚Äì High‚ÄëLevel Orchestration Layer  

The **`Manager`** class lives in `autodocgenerator/manage.py` and coordinates the end‚Äëto‚Äëend documentation pipeline. It glues together the *pre‚Äëprocessor* (code mixing, splitting, compression), the *engine* (LLM models), the *factory* (modular doc sections), and the *UI* (progress bars & logging). All artefacts are cached in a hidden folder `.<project>/.auto_doc_cache` to avoid recomputation.

<a name="manager_state"></a>
### Core State  

| Attribute | Meaning |
|-----------|---------|
| `project_directory` | Root of the source repository. |
| `ignore_files` | Paths excluded from the code‚Äëmix. |
| `language` | Target documentation language (default **en**). |
| `project_settings` / `pcs` | User‚Äëdefined configuration objects. |
| `sync_model`, `async_model` | Instances of `Model` / `AsyncModel` used for LLM calls. |
| `logger` | `BaseLogger` writing to `<cache>/report.txt`. |
| `progress_bar` | `BaseProgress` instance visualising each stage. |

<a name="manager_methods"></a>
### Public Methods  

| Method | Purpose | Side‚Äëeffects |
|--------|---------|--------------|
| **`generate_code_file`** | Builds a *code‚Äëmix* (concatenated source) via `CodeMix` and writes it to `code_mix.txt`. | Logs start/completion, updates progress UI. |
| **`generate_global_info_file`** | Intended to compress the code‚Äëmix into a high‚Äëlevel overview (currently stubbed ‚Äì writes ‚Äúss‚Äù). | Logs, updates progress UI. |
| **`generete_doc_parts`** *(sic)* | Splits the mixed code and global info into ‚â§‚ÄØ`max_symbols` chunks, then runs `gen_doc_parts` **or** `async_gen_doc_parts` to produce a partial Markdown document (`output_doc.md`). | Logs start/completion, writes result, updates UI. |
| **`factory_generate_doc`** | Collects `global_data`, current `output_doc`, and raw `code_mix` into an `info` dict, then delegates to a `DocFactory` (which runs modules such as `IntroLinks`, `IntroText`, `CustomModule`). The factory output is prepended to the existing doc and persisted. | Detailed logging (module list & input sizes), UI update. |
| **`clear_cache`** | Optionally removes the log file if `pcs.save_logs` is `False`. | File system side‚Äëeffect. |
| **`read_file_by_file_key`** / **`get_file_path`** | Helper utilities for reading/writing cached artefacts. | Pure I/O. |

<a name="interaction_flow"></a>
### Interaction Flow  

1. **Code Mix** ‚Üí `generate_code_file` creates `code_mix.txt`.  
2. **Global Summary** ‚Üí `generate_global_info_file` (future compression).  
3. **Chunked Generation** ‚Üí `generete_doc_parts` invokes the *spliter/compressor* pipeline, optionally async.  
4. **Modular Assembly** ‚Üí `factory_generate_doc` hands the `info` dict to a `DocFactory`; each module uses the shared `Model` to call the LLM and returns a Markdown snippet.  
5. **Progress & Logging** ‚Üí Every stage reports via `BaseProgress` and writes `InfoLog` entries to the cache log file.

<a name="assumptions_and_limits"></a>
### Assumptions & Constraints  

* The cache directory exists or can be created.  
* `sync_model` / `async_model` are valid `Model` instances; if `None`, LLM calls will fail.  
* `generate_global_info_file` is a placeholder ‚Äì real implementation must call `compress_to_one`.  
* `progress_bar.update_task()` is safe to call even if no sub‚Äëtasks were registered.

---  

*This documentation is intended for developers onboarding the `autodocgenerator` codebase and needing a quick mental map of the `Manager` orchestrator.*
[2026-01-25 14:32:43.529196] [INFO] Documentation for part generated. total length: 3516
[2026-01-25 14:32:43.529221] [INFO] Part Documentation: <a name="manager_overview"></a>
## Manager ‚Äì High‚ÄëLevel Orchestration Layer  

The **`Manager`** class lives in `autodocgenerator/manage.py` and coordinates the end‚Äëto‚Äëend documentation pipeline. It glues together the *pre‚Äëprocessor* (code mixing, splitting, compression), the *engine* (LLM models), the *factory* (modular doc sections), and the *UI* (progress bars & logging). All artefacts are cached in a hidden folder `.<project>/.auto_doc_cache` to avoid recomputation.

<a name="manager_state"></a>
### Core State  

| Attribute | Meaning |
|-----------|---------|
| `project_directory` | Root of the source repository. |
| `ignore_files` | Paths excluded from the code‚Äëmix. |
| `language` | Target documentation language (default **en**). |
| `project_settings` / `pcs` | User‚Äëdefined configuration objects. |
| `sync_model`, `async_model` | Instances of `Model` / `AsyncModel` used for LLM calls. |
| `logger` | `BaseLogger` writing to `<cache>/report.txt`. |
| `progress_bar` | `BaseProgress` instance visualising each stage. |

<a name="manager_methods"></a>
### Public Methods  

| Method | Purpose | Side‚Äëeffects |
|--------|---------|--------------|
| **`generate_code_file`** | Builds a *code‚Äëmix* (concatenated source) via `CodeMix` and writes it to `code_mix.txt`. | Logs start/completion, updates progress UI. |
| **`generate_global_info_file`** | Intended to compress the code‚Äëmix into a high‚Äëlevel overview (currently stubbed ‚Äì writes ‚Äúss‚Äù). | Logs, updates progress UI. |
| **`generete_doc_parts`** *(sic)* | Splits the mixed code and global info into ‚â§‚ÄØ`max_symbols` chunks, then runs `gen_doc_parts` **or** `async_gen_doc_parts` to produce a partial Markdown document (`output_doc.md`). | Logs start/completion, writes result, updates UI. |
| **`factory_generate_doc`** | Collects `global_data`, current `output_doc`, and raw `code_mix` into an `info` dict, then delegates to a `DocFactory` (which runs modules such as `IntroLinks`, `IntroText`, `CustomModule`). The factory output is prepended to the existing doc and persisted. | Detailed logging (module list & input sizes), UI update. |
| **`clear_cache`** | Optionally removes the log file if `pcs.save_logs` is `False`. | File system side‚Äëeffect. |
| **`read_file_by_file_key`** / **`get_file_path`** | Helper utilities for reading/writing cached artefacts. | Pure I/O. |

<a name="interaction_flow"></a>
### Interaction Flow  

1. **Code Mix** ‚Üí `generate_code_file` creates `code_mix.txt`.  
2. **Global Summary** ‚Üí `generate_global_info_file` (future compression).  
3. **Chunked Generation** ‚Üí `generete_doc_parts` invokes the *spliter/compressor* pipeline, optionally async.  
4. **Modular Assembly** ‚Üí `factory_generate_doc` hands the `info` dict to a `DocFactory`; each module uses the shared `Model` to call the LLM and returns a Markdown snippet.  
5. **Progress & Logging** ‚Üí Every stage reports via `BaseProgress` and writes `InfoLog` entries to the cache log file.

<a name="assumptions_and_limits"></a>
### Assumptions & Constraints  

* The cache directory exists or can be created.  
* `sync_model` / `async_model` are valid `Model` instances; if `None`, LLM calls will fail.  
* `generate_global_info_file` is a placeholder ‚Äì real implementation must call `compress_to_one`.  
* `progress_bar.update_task()` is safe to call even if no sub‚Äëtasks were registered.

---  

*This documentation is intended for developers onboarding the `autodocgenerator` codebase and needing a quick mental map of the `Manager` orchestrator.*
[2026-01-25 14:32:43.529310] [INFO] Generating documentation for a part...
[2026-01-25 14:32:43.529338] [INFO] Generating answer...
[2026-01-25 14:33:03.681186] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:33:03.681260] [INFO] Answer: <a name="code_mix_overview"></a>## CodeMix ‚Äì Repository‚ÄØ‚Üî‚ÄØText‚ÄØPackager  

The **CodeMix** class lives in `autodocgenerator.preprocessor.code_mix`. Its sole purpose is to turn a source tree into a single, LLM‚Äëfriendly text file (`codemix.txt`). The manager (`Manager.generate_code_file`) instantiates this component, feeds it the project root and an ignore list, then stores the produced file for later chunk‚Äësplitting and documentation generation.

<a name="code_mix_responsibility"></a>### Responsibility  
* Walk the directory tree rooted at `root_dir`.  
* Skip files/folders that match any pattern in `ignore_patterns`.  
* Emit a readable *structure* section followed by the raw contents of each accepted file, wrapped in `<file path="‚Ä¶">` tags.

<a name="code_mix_interaction"></a>### Interaction with the System  
* **Input**: `root_dir` (project directory), `ignore_patterns` (global `ignore_list`).  
* **Output**: Text file (`output_file`) stored in the cache; the manager later reads it via `Manager.read_file_by_file_key`.  
* **Side‚Äëeffects**: Writes to disk, logs each ignored path through `BaseLogger` ‚Üí `InfoLog`.  

<a name="code_mix_key_functions"></a>### Key Functions  

| Function | Role |
|----------|------|
| `__init__(root_dir, ignore_patterns)` | Resolve the absolute project path, store ignore patterns, create a logger. |
| `should_ignore(path)` | Convert `path` to a relative string and test it against every glob in `ignore_patterns` (full‚Äëpath, basename, and each path component). Returns‚ÄØ`True`‚ÄØ‚Üí‚ÄØskip. |
| `build_repo_content(output_file)` | 1Ô∏è‚É£ Write a *tree view* of the repository (indented by depth). 2Ô∏è‚É£ Insert a separator line. 3Ô∏è‚É£ Append each non‚Äëignored file‚Äôs content wrapped in `<file ‚Ä¶>` tags. Errors while reading a file are captured and written as plain text. |

<a name="code_mix_assumptions"></a>### Assumptions & Limits  

* The supplied `root_dir` exists and is readable.  
* `ignore_patterns` are valid Unix‚Äëstyle globs; patterns like `"*.md"` will also suppress markdown docs (the system later generates its own).  
* All files are opened with UTF‚Äë8‚ÄØ+‚ÄØ`errors="ignore"` ‚Äì binary files may produce garbled output but will not crash the process.  

<a name="code_mix_usage"></a>### Typical Usage (as seen in `__main__`)  

```python
packer = CodeMix(root_dir=PROJECT_PATH, ignore_patterns=ignore_list)
packer.build_repo_content("codemix.txt")
```

The generated `codemix.txt` becomes the *code‚Äëmix* consumed by the rest of the autodoc pipeline.
[2026-01-25 14:33:03.681295] [INFO] Documentation for part generated. total length: 2495
[2026-01-25 14:33:03.681318] [INFO] Part Documentation: <a name="code_mix_overview"></a>## CodeMix ‚Äì Repository‚ÄØ‚Üî‚ÄØText‚ÄØPackager  

The **CodeMix** class lives in `autodocgenerator.preprocessor.code_mix`. Its sole purpose is to turn a source tree into a single, LLM‚Äëfriendly text file (`codemix.txt`). The manager (`Manager.generate_code_file`) instantiates this component, feeds it the project root and an ignore list, then stores the produced file for later chunk‚Äësplitting and documentation generation.

<a name="code_mix_responsibility"></a>### Responsibility  
* Walk the directory tree rooted at `root_dir`.  
* Skip files/folders that match any pattern in `ignore_patterns`.  
* Emit a readable *structure* section followed by the raw contents of each accepted file, wrapped in `<file path="‚Ä¶">` tags.

<a name="code_mix_interaction"></a>### Interaction with the System  
* **Input**: `root_dir` (project directory), `ignore_patterns` (global `ignore_list`).  
* **Output**: Text file (`output_file`) stored in the cache; the manager later reads it via `Manager.read_file_by_file_key`.  
* **Side‚Äëeffects**: Writes to disk, logs each ignored path through `BaseLogger` ‚Üí `InfoLog`.  

<a name="code_mix_key_functions"></a>### Key Functions  

| Function | Role |
|----------|------|
| `__init__(root_dir, ignore_patterns)` | Resolve the absolute project path, store ignore patterns, create a logger. |
| `should_ignore(path)` | Convert `path` to a relative string and test it against every glob in `ignore_patterns` (full‚Äëpath, basename, and each path component). Returns‚ÄØ`True`‚ÄØ‚Üí‚ÄØskip. |
| `build_repo_content(output_file)` | 1Ô∏è‚É£ Write a *tree view* of the repository (indented by depth). 2Ô∏è‚É£ Insert a separator line. 3Ô∏è‚É£ Append each non‚Äëignored file‚Äôs content wrapped in `<file ‚Ä¶>` tags. Errors while reading a file are captured and written as plain text. |

<a name="code_mix_assumptions"></a>### Assumptions & Limits  

* The supplied `root_dir` exists and is readable.  
* `ignore_patterns` are valid Unix‚Äëstyle globs; patterns like `"*.md"` will also suppress markdown docs (the system later generates its own).  
* All files are opened with UTF‚Äë8‚ÄØ+‚ÄØ`errors="ignore"` ‚Äì binary files may produce garbled output but will not crash the process.  

<a name="code_mix_usage"></a>### Typical Usage (as seen in `__main__`)  

```python
packer = CodeMix(root_dir=PROJECT_PATH, ignore_patterns=ignore_list)
packer.build_repo_content("codemix.txt")
```

The generated `codemix.txt` becomes the *code‚Äëmix* consumed by the rest of the autodoc pipeline.
[2026-01-25 14:33:03.681391] [INFO] Generating documentation for a part...
[2026-01-25 14:33:03.681418] [INFO] Generating answer...
[2026-01-25 14:33:28.750245] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:33:28.750310] [INFO] Answer: <a name="compressor_overview"></a>##‚ÄØCompressor‚ÄØPre‚Äëprocessor (autodocgenerator/preprocessor/compressor.py)

The **compressor** module reduces large code‚Äëbase texts to compact summaries that can be fed to downstream LLM stages (e.g., similarity comparison, description generation). It works together with the **engine** (GPT‚Äëmodel wrappers) and the **UI** progress bar, while respecting project‚Äëwide prompts defined in `settings.ProjectSettings`.

---

<a name="core_functions"></a>### Core Functions  

| Function | Responsibility | Key I/O | Side‚Äëeffects |
|----------|----------------|---------|--------------|
| **compress** | Builds a 3‚Äëpart prompt (project system prompt, compression instruction from `get_BASE_COMPRESS_TEXT`, user payload) and calls `model.get_answer_without_history`. Returns the compressed string. | `data: str`, `project_settings: ProjectSettings`, `model: Model`, `compress_power: int` ‚Üí `str` | None (pure LLM call). |
| **compress_and_compare** | Synchronously compresses a list of files, concatenating *compress_power* items per chunk to enable later ‚Äúcompare‚Äëall‚Äù steps. Updates a `BaseProgress` sub‚Äëtask. | `data: List[str]` ‚Üí `List[str]` (chunks) | Progress bar updates. |
| **async_compress** | Same as `compress` but runs inside a semaphore‚Äëprotected coroutine, allowing limited parallelism. | `data: str` ‚Üí `str` (awaitable) | Progress bar update after each call. |
| **async_compress_and_compare** | Fires many `async_compress` coroutines (default concurrency‚ÄØ=‚ÄØ4), gathers results, then re‚Äëchunks them like the sync version. | `data: List[str]` ‚Üí `List[str]` (awaitable) | Progress bar sub‚Äëtask handling. |
| **compress_to_one** | Repeatedly compresses until a single aggregated chunk remains. Chooses synchronous or asynchronous path per `use_async`. | `data: List[str]` ‚Üí `str` (final compressed blob) | May invoke the event loop (`asyncio.run`). |
| **generate_discribtions_for_code** | Sends each compressed code block to the LLM with a strict ‚Äúwrite a developer‚Äëfacing guide‚Äù system prompt. Collects the generated descriptions. | `data: List[str]` ‚Üí `List[str]` (descriptions) | Progress bar updates. |

---

<a name="interaction_flow"></a>### Interaction Flow  

1. **Input preparation** ‚Äì Earlier stages (e.g., `CodeMix`) produce a list of raw file contents.  
2. **Compression** ‚Äì `compress_to_one` (or its helpers) iteratively reduces the list, using either the sync or async pipeline.  
3. **Description** ‚Äì The final compressed blobs are passed to `generate_discribtions_for_code`, which yields markdown‚Äëready developer guides.  
4. **Output** ‚Äì The resulting strings are written to the *code‚Äëmix* artifact consumed by the rest of the autodoc pipeline.

---

<a name="assumptions"></a>### Assumptions & Constraints  

* `project_settings.prompt` is a valid system‚Äëprompt string.  
* `compress_power` ‚â•‚ÄØ2; if the remaining data size is smaller, the function automatically lowers it to‚ÄØ2.  
* Models supplied must implement `get_answer_without_history` (sync) or its async counterpart.  
* The UI progress bar is thread‚Äësafe for the modest concurrency used (semaphore‚ÄØ=‚ÄØ4).  

---  

*This module is the bridge between raw source files and the LLM‚Äëdriven documentation generation, providing scalable, progress‚Äëtracked compression.*
[2026-01-25 14:33:28.750360] [INFO] Documentation for part generated. total length: 3279
[2026-01-25 14:33:28.750384] [INFO] Part Documentation: <a name="compressor_overview"></a>##‚ÄØCompressor‚ÄØPre‚Äëprocessor (autodocgenerator/preprocessor/compressor.py)

The **compressor** module reduces large code‚Äëbase texts to compact summaries that can be fed to downstream LLM stages (e.g., similarity comparison, description generation). It works together with the **engine** (GPT‚Äëmodel wrappers) and the **UI** progress bar, while respecting project‚Äëwide prompts defined in `settings.ProjectSettings`.

---

<a name="core_functions"></a>### Core Functions  

| Function | Responsibility | Key I/O | Side‚Äëeffects |
|----------|----------------|---------|--------------|
| **compress** | Builds a 3‚Äëpart prompt (project system prompt, compression instruction from `get_BASE_COMPRESS_TEXT`, user payload) and calls `model.get_answer_without_history`. Returns the compressed string. | `data: str`, `project_settings: ProjectSettings`, `model: Model`, `compress_power: int` ‚Üí `str` | None (pure LLM call). |
| **compress_and_compare** | Synchronously compresses a list of files, concatenating *compress_power* items per chunk to enable later ‚Äúcompare‚Äëall‚Äù steps. Updates a `BaseProgress` sub‚Äëtask. | `data: List[str]` ‚Üí `List[str]` (chunks) | Progress bar updates. |
| **async_compress** | Same as `compress` but runs inside a semaphore‚Äëprotected coroutine, allowing limited parallelism. | `data: str` ‚Üí `str` (awaitable) | Progress bar update after each call. |
| **async_compress_and_compare** | Fires many `async_compress` coroutines (default concurrency‚ÄØ=‚ÄØ4), gathers results, then re‚Äëchunks them like the sync version. | `data: List[str]` ‚Üí `List[str]` (awaitable) | Progress bar sub‚Äëtask handling. |
| **compress_to_one** | Repeatedly compresses until a single aggregated chunk remains. Chooses synchronous or asynchronous path per `use_async`. | `data: List[str]` ‚Üí `str` (final compressed blob) | May invoke the event loop (`asyncio.run`). |
| **generate_discribtions_for_code** | Sends each compressed code block to the LLM with a strict ‚Äúwrite a developer‚Äëfacing guide‚Äù system prompt. Collects the generated descriptions. | `data: List[str]` ‚Üí `List[str]` (descriptions) | Progress bar updates. |

---

<a name="interaction_flow"></a>### Interaction Flow  

1. **Input preparation** ‚Äì Earlier stages (e.g., `CodeMix`) produce a list of raw file contents.  
2. **Compression** ‚Äì `compress_to_one` (or its helpers) iteratively reduces the list, using either the sync or async pipeline.  
3. **Description** ‚Äì The final compressed blobs are passed to `generate_discribtions_for_code`, which yields markdown‚Äëready developer guides.  
4. **Output** ‚Äì The resulting strings are written to the *code‚Äëmix* artifact consumed by the rest of the autodoc pipeline.

---

<a name="assumptions"></a>### Assumptions & Constraints  

* `project_settings.prompt` is a valid system‚Äëprompt string.  
* `compress_power` ‚â•‚ÄØ2; if the remaining data size is smaller, the function automatically lowers it to‚ÄØ2.  
* Models supplied must implement `get_answer_without_history` (sync) or its async counterpart.  
* The UI progress bar is thread‚Äësafe for the modest concurrency used (semaphore‚ÄØ=‚ÄØ4).  

---  

*This module is the bridge between raw source files and the LLM‚Äëdriven documentation generation, providing scalable, progress‚Äëtracked compression.*
[2026-01-25 14:33:28.750460] [INFO] Generating documentation for a part...
[2026-01-25 14:33:28.750488] [INFO] Generating answer...
[2026-01-25 14:33:59.191436] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:33:59.191496] [INFO] Answer: <a name="postprocess-module"></a>##‚ÄØ`autodocgenerator.preprocessor.postprocess` ‚Äì Final‚Äëstage Markdown polishing  

**Responsibility**  
Transforms the raw LLM‚Äëgenerated documentation into a navigation‚Äëfriendly Markdown file. It extracts headings, creates HTML‚Äëstyle anchors, builds a table‚Äëof‚Äëcontents‚Äëlike list of links, and can ask the model to write custom sections that follow a strict ‚Äúno‚Äëhallucination‚Äù policy.

**Key functions**

| Function | Purpose | I/O | Side‚Äëeffects |
|----------|---------|-----|--------------|
| `generate_markdown_anchor(header)` | Normalises a heading string and returns a Markdown anchor (`#slug`). | `header: str` ‚Üí `str` | Pure function. |
| `get_all_topics(data)` | Scans a Markdown document for level‚Äë2 headings (`## ‚Ä¶`), returns the plain titles and their generated anchors. | `data: str` ‚Üí `(titles: list[str], anchors: list[str])` | Pure. |
| `get_all_html_links(data)` | Finds existing `<a name=‚Ä¶>` tags, extracts the name (max‚ÄØ25‚ÄØchars) and returns them as `#name` links. | `data: str` ‚Üí `list[str]` | Logs extraction progress via `BaseLogger`. |
| `get_links_intro(links, model, language='en')` | Sends the list of links to the LLM with the *BASE_INTRODACTION_CREATE_TEXT* system prompt and returns a generated introductory paragraph. | `links: list[str]`, `model: Model`, `language: str` ‚Üí `str` | Calls the model (`model.get_answer_without_history`). |
| `get_introdaction(global_data, model, language='en')` | Wraps the whole document in a system prompt (*BASE_INTRO_CREATE*) and asks the model for a high‚Äëlevel introduction. | `global_data: str` ‚Üí `str` | Same LLM call. |
| `generete_custom_discription(splited_data, model, custom_description, language='en')` | Iterates over pre‚Äësplit chunks, asks the model to produce a concise description (or ‚Äú!noinfo‚Äù) respecting a strict rule set, and returns the first non‚Äëempty result. | `splited_data: str`, `custom_description: str` ‚Üí `str` | May log intermediate steps; performs multiple LLM calls until a valid answer is found. |

**Interaction with other components**  

* **Engine (`Model` / `GPTModel`)** ‚Äì All functions that need LLM output delegate to `model.get_answer_without_history`. The model must implement the synchronous interface defined in `engine.models.model`.  
* **UI (`BaseLogger`)** ‚Äì Used for informational messages; no UI state is mutated besides console/file logging.  
* **Settings (`ProjectSettings`)** ‚Äì The system prompts (`BASE_INTRODACTION_CREATE_TEXT`, `BASE_INTRO_CREATE`) are defined globally and injected here; they rely on the project‚Äëwide prompt built elsewhere.  

**Assumptions & constraints**  

* Input Markdown uses `## ` for top‚Äëlevel sections; other heading levels are ignored.  
* `<a name=` tags are well‚Äëformed and short (‚â§‚ÄØ25‚ÄØchars); longer names are skipped.  
* The supplied `model` conforms to the `Model` protocol and is ready for synchronous calls.  
* Logging level‚ÄØ1 is used for verbose link dumps; the UI expects `InfoLog` objects.  

**Typical flow**  

1. After compression (`compress_to_one`) the pipeline receives a large Markdown string.  
2. `get_all_topics` builds a TOC; `get_all_html_links` extracts any manual anchors.  
3. `get_links_intro` creates a short paragraph introducing the link list.  
4. `get_introdaction` produces a global introduction for the whole document.  
5. Optional custom sections are added via `generete_custom_discription`.  

The module thus finalises the raw LLM output into a well‚Äëstructured, navigable Markdown artifact ready for downstream consumption (e.g., writing to the *code‚Äëmix* artifact).
[2026-01-25 14:33:59.191550] [INFO] Documentation for part generated. total length: 3585
[2026-01-25 14:33:59.191575] [INFO] Part Documentation: <a name="postprocess-module"></a>##‚ÄØ`autodocgenerator.preprocessor.postprocess` ‚Äì Final‚Äëstage Markdown polishing  

**Responsibility**  
Transforms the raw LLM‚Äëgenerated documentation into a navigation‚Äëfriendly Markdown file. It extracts headings, creates HTML‚Äëstyle anchors, builds a table‚Äëof‚Äëcontents‚Äëlike list of links, and can ask the model to write custom sections that follow a strict ‚Äúno‚Äëhallucination‚Äù policy.

**Key functions**

| Function | Purpose | I/O | Side‚Äëeffects |
|----------|---------|-----|--------------|
| `generate_markdown_anchor(header)` | Normalises a heading string and returns a Markdown anchor (`#slug`). | `header: str` ‚Üí `str` | Pure function. |
| `get_all_topics(data)` | Scans a Markdown document for level‚Äë2 headings (`## ‚Ä¶`), returns the plain titles and their generated anchors. | `data: str` ‚Üí `(titles: list[str], anchors: list[str])` | Pure. |
| `get_all_html_links(data)` | Finds existing `<a name=‚Ä¶>` tags, extracts the name (max‚ÄØ25‚ÄØchars) and returns them as `#name` links. | `data: str` ‚Üí `list[str]` | Logs extraction progress via `BaseLogger`. |
| `get_links_intro(links, model, language='en')` | Sends the list of links to the LLM with the *BASE_INTRODACTION_CREATE_TEXT* system prompt and returns a generated introductory paragraph. | `links: list[str]`, `model: Model`, `language: str` ‚Üí `str` | Calls the model (`model.get_answer_without_history`). |
| `get_introdaction(global_data, model, language='en')` | Wraps the whole document in a system prompt (*BASE_INTRO_CREATE*) and asks the model for a high‚Äëlevel introduction. | `global_data: str` ‚Üí `str` | Same LLM call. |
| `generete_custom_discription(splited_data, model, custom_description, language='en')` | Iterates over pre‚Äësplit chunks, asks the model to produce a concise description (or ‚Äú!noinfo‚Äù) respecting a strict rule set, and returns the first non‚Äëempty result. | `splited_data: str`, `custom_description: str` ‚Üí `str` | May log intermediate steps; performs multiple LLM calls until a valid answer is found. |

**Interaction with other components**  

* **Engine (`Model` / `GPTModel`)** ‚Äì All functions that need LLM output delegate to `model.get_answer_without_history`. The model must implement the synchronous interface defined in `engine.models.model`.  
* **UI (`BaseLogger`)** ‚Äì Used for informational messages; no UI state is mutated besides console/file logging.  
* **Settings (`ProjectSettings`)** ‚Äì The system prompts (`BASE_INTRODACTION_CREATE_TEXT`, `BASE_INTRO_CREATE`) are defined globally and injected here; they rely on the project‚Äëwide prompt built elsewhere.  

**Assumptions & constraints**  

* Input Markdown uses `## ` for top‚Äëlevel sections; other heading levels are ignored.  
* `<a name=` tags are well‚Äëformed and short (‚â§‚ÄØ25‚ÄØchars); longer names are skipped.  
* The supplied `model` conforms to the `Model` protocol and is ready for synchronous calls.  
* Logging level‚ÄØ1 is used for verbose link dumps; the UI expects `InfoLog` objects.  

**Typical flow**  

1. After compression (`compress_to_one`) the pipeline receives a large Markdown string.  
2. `get_all_topics` builds a TOC; `get_all_html_links` extracts any manual anchors.  
3. `get_links_intro` creates a short paragraph introducing the link list.  
4. `get_introdaction` produces a global introduction for the whole document.  
5. Optional custom sections are added via `generete_custom_discription`.  

The module thus finalises the raw LLM output into a well‚Äëstructured, navigable Markdown artifact ready for downstream consumption (e.g., writing to the *code‚Äëmix* artifact).
[2026-01-25 14:33:59.191646] [INFO] Generating documentation for a part...
[2026-01-25 14:33:59.191674] [INFO] Generating answer...
[2026-01-25 14:34:29.100831] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:34:29.100890] [INFO] Answer: <a name="overview"></a>## Overview  
The module turns a large *code‚Äëmix* Markdown blob into a complete, navigable documentation set. It first **splits** the input into size‚Äëbounded chunks, then **feeds each chunk to an LLM** (sync‚ÄØ`Model` or async‚ÄØ`AsyncModel`) using a fixed system prompt (`BASE_PART_COMPLITE_TEXT`). Results are concatenated, trimmed, and reported through `BaseLogger` and `BaseProgress`.

<a name="split-data"></a>## `split_data` (helper)  
*Inputs*: `full_code_mix: str`, `max_symbols: int`  
*Outputs*: `list[str]` ‚Äì chunks ‚â§‚ÄØ`max_symbols‚ÄØ√ó‚ÄØ1.5`.  
Logic: recursively cuts overly long pieces, then packs them into groups respecting a 1.25‚ÄØ√ó‚ÄØlimit. Purely deterministic; no side effects.

<a name="write-sync"></a>## `write_docs_by_parts`  
Generates documentation for a single chunk.  
*Flow*: builds a chat‚ÄØprompt (language system message + `BASE_PART_COMPLITE_TEXT` + optional previous part), calls `model.get_answer_without_history`, strips surrounding ````` fences, logs length and content.  
*Side effects*: logs via `BaseLogger`.

<a name="write-async"></a>## `async_write_docs_by_parts`  
Async counterpart; runs under a semaphore to limit parallel LLM calls. Accepts an `update_progress` callback to tick `BaseProgress`. Otherwise mirrors the sync version.

<a name="gen-sync"></a>## `gen_doc_parts`  
Orchestrates synchronous generation:  
1. `split_data` ‚Üí `splited_data`.  
2. For each chunk call `write_docs_by_parts`, append result, keep the last‚ÄØ3‚ÄØk chars as context for the next chunk.  
3. Updates a sub‚Äëtask in `progress_bar`.  
Returns the full assembled documentation.

<a name="gen-async"></a>## `async_gen_doc_parts`  
Same orchestration but launches `async_write_docs_by_parts` tasks (max‚ÄØ4 concurrent) and gathers results. Progress is driven by the provided callback.

**Interactions** ‚Äì `Model/AsyncModel` (LLM engine), `BaseLogger` (info logs), `BaseProgress` (UI progress bar). All functions assume well‚Äëformed Markdown input and a ready LLM conforming to the `Model` protocol.
[2026-01-25 14:34:29.100935] [INFO] Documentation for part generated. total length: 2025
[2026-01-25 14:34:29.100959] [INFO] Part Documentation: <a name="overview"></a>## Overview  
The module turns a large *code‚Äëmix* Markdown blob into a complete, navigable documentation set. It first **splits** the input into size‚Äëbounded chunks, then **feeds each chunk to an LLM** (sync‚ÄØ`Model` or async‚ÄØ`AsyncModel`) using a fixed system prompt (`BASE_PART_COMPLITE_TEXT`). Results are concatenated, trimmed, and reported through `BaseLogger` and `BaseProgress`.

<a name="split-data"></a>## `split_data` (helper)  
*Inputs*: `full_code_mix: str`, `max_symbols: int`  
*Outputs*: `list[str]` ‚Äì chunks ‚â§‚ÄØ`max_symbols‚ÄØ√ó‚ÄØ1.5`.  
Logic: recursively cuts overly long pieces, then packs them into groups respecting a 1.25‚ÄØ√ó‚ÄØlimit. Purely deterministic; no side effects.

<a name="write-sync"></a>## `write_docs_by_parts`  
Generates documentation for a single chunk.  
*Flow*: builds a chat‚ÄØprompt (language system message + `BASE_PART_COMPLITE_TEXT` + optional previous part), calls `model.get_answer_without_history`, strips surrounding ````` fences, logs length and content.  
*Side effects*: logs via `BaseLogger`.

<a name="write-async"></a>## `async_write_docs_by_parts`  
Async counterpart; runs under a semaphore to limit parallel LLM calls. Accepts an `update_progress` callback to tick `BaseProgress`. Otherwise mirrors the sync version.

<a name="gen-sync"></a>## `gen_doc_parts`  
Orchestrates synchronous generation:  
1. `split_data` ‚Üí `splited_data`.  
2. For each chunk call `write_docs_by_parts`, append result, keep the last‚ÄØ3‚ÄØk chars as context for the next chunk.  
3. Updates a sub‚Äëtask in `progress_bar`.  
Returns the full assembled documentation.

<a name="gen-async"></a>## `async_gen_doc_parts`  
Same orchestration but launches `async_write_docs_by_parts` tasks (max‚ÄØ4 concurrent) and gathers results. Progress is driven by the provided callback.

**Interactions** ‚Äì `Model/AsyncModel` (LLM engine), `BaseLogger` (info logs), `BaseProgress` (UI progress bar). All functions assume well‚Äëformed Markdown input and a ready LLM conforming to the `Model` protocol.
[2026-01-25 14:34:29.101098] [INFO] Generating documentation for a part...
[2026-01-25 14:34:29.101140] [INFO] Generating answer...
[2026-01-25 14:34:56.632581] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:34:56.632645] [INFO] Answer: <a name="logger"></a>## `autodocgenerator/ui/logging.py` ‚Äì Centralised, singleton logging  

**Responsibility** ‚Äì Provides lightweight, level‚Äëaware logging that can be routed either to the console or to a file. The module supplies tiny log‚Äërecord objects (`ErrorLog`, `WarningLog`, `InfoLog`) and a `BaseLogger` singleton that forwards records to a configurable *logger template*.

**Key classes / flow**  

| Class | Role |
|-------|------|
| `BaseLog` | Holds a message and a numeric *level*; formats plain text; supplies a timestamp prefix via `_log_prefix`. |
| `ErrorLog`, `WarningLog`, `InfoLog` | Override `format()` to prepend `[ERROR]`, `[WARNING]`, `[INFO]` respectively. |
| `BaseLoggerTemplate` | Base for concrete back‚Äëends. Implements `log()` (default ‚Üí `print`) and `global_log()` which respects the `log_level` filter (`log_level < 0` means ‚Äúlog everything‚Äù). |
| `FileLoggerTemplate` | Writes formatted logs to a given file, one line per record. |
| `BaseLogger` | **Singleton** (`__new__` caches a single instance). `set_logger()` injects a `BaseLoggerTemplate`; `log()` forwards to `logger_template.global_log()`. |

**Interactions** ‚Äì Other components (e.g., the document‚Äëgeneration pipeline) obtain the global logger via `BaseLogger()` and call `log(ErrorLog(...))`. The logger forwards to the selected template, which may be a `FileLoggerTemplate` used by CI pipelines or the default console logger for interactive runs.

**Assumptions / side‚Äëeffects**  

* Caller supplies a `BaseLog`‚Äëderived object; the `level` is compared against the configured filter.  
* `FileLoggerTemplate` opens the file on every call ‚Äì acceptable for low‚Äëvolume logs.  
* No thread‚Äësafety guarantees; intended for single‚Äëthreaded or externally synchronized contexts.

---

<a name="progress"></a>## `autodocgenerator/ui/progress_base.py` ‚Äì Progress‚Äëbar abstraction  

**Responsibility** ‚Äì Supplies a thin abstraction (`BaseProgress`) over concrete progress reporters, allowing the generation engine to report sub‚Äëtasks without caring whether the output goes to a rich terminal UI or a simple console printout.

**Key classes / flow**  

| Class | Implementation | Notes |
|-------|----------------|-------|
| `BaseProgress` | Abstract interface (`create_new_subtask`, `update_task`, `remove_subtask`). | Used by generators to drive progress. |
| `LibProgress` | Wraps **rich**‚Äôs `Progress`. Creates a *base* task (`General progress`) and optional *sub‚Äëtasks*; `update_task()` advances the current task, falling back to the base task. |
| `ConsoleTask` | Helper that prints start‚Äëup and incremental percentage updates. |
| `ConsoleGtiHubProgress` | Implements `BaseProgress` with `ConsoleTask` objects; suitable when Rich is unavailable (e.g., plain CI logs). |

**Interaction pattern**  

1. The document generation orchestrator instantiates a concrete `BaseProgress` (chosen by configuration).  
2. Before processing a chunk it calls `create_new_subtask(name, total_len)`.  
3. After each LLM call it invokes `update_task()`.  
4. When the chunk finishes it calls `remove_subtask()` to reset the context.

**Assumptions / side‚Äëeffects**  

* `total_len` reflects the number of LLM calls for the chunk; callers must supply an accurate count.  
* `LibProgress` assumes a live `rich.Progress` instance; `ConsoleGtiHubProgress` assumes a terminal that can accept `print`.  
* No persistence ‚Äì progress state lives only in memory for the duration of the run.
[2026-01-25 14:34:56.632683] [INFO] Documentation for part generated. total length: 3453
[2026-01-25 14:34:56.632709] [INFO] Part Documentation: <a name="logger"></a>## `autodocgenerator/ui/logging.py` ‚Äì Centralised, singleton logging  

**Responsibility** ‚Äì Provides lightweight, level‚Äëaware logging that can be routed either to the console or to a file. The module supplies tiny log‚Äërecord objects (`ErrorLog`, `WarningLog`, `InfoLog`) and a `BaseLogger` singleton that forwards records to a configurable *logger template*.

**Key classes / flow**  

| Class | Role |
|-------|------|
| `BaseLog` | Holds a message and a numeric *level*; formats plain text; supplies a timestamp prefix via `_log_prefix`. |
| `ErrorLog`, `WarningLog`, `InfoLog` | Override `format()` to prepend `[ERROR]`, `[WARNING]`, `[INFO]` respectively. |
| `BaseLoggerTemplate` | Base for concrete back‚Äëends. Implements `log()` (default ‚Üí `print`) and `global_log()` which respects the `log_level` filter (`log_level < 0` means ‚Äúlog everything‚Äù). |
| `FileLoggerTemplate` | Writes formatted logs to a given file, one line per record. |
| `BaseLogger` | **Singleton** (`__new__` caches a single instance). `set_logger()` injects a `BaseLoggerTemplate`; `log()` forwards to `logger_template.global_log()`. |

**Interactions** ‚Äì Other components (e.g., the document‚Äëgeneration pipeline) obtain the global logger via `BaseLogger()` and call `log(ErrorLog(...))`. The logger forwards to the selected template, which may be a `FileLoggerTemplate` used by CI pipelines or the default console logger for interactive runs.

**Assumptions / side‚Äëeffects**  

* Caller supplies a `BaseLog`‚Äëderived object; the `level` is compared against the configured filter.  
* `FileLoggerTemplate` opens the file on every call ‚Äì acceptable for low‚Äëvolume logs.  
* No thread‚Äësafety guarantees; intended for single‚Äëthreaded or externally synchronized contexts.

---

<a name="progress"></a>## `autodocgenerator/ui/progress_base.py` ‚Äì Progress‚Äëbar abstraction  

**Responsibility** ‚Äì Supplies a thin abstraction (`BaseProgress`) over concrete progress reporters, allowing the generation engine to report sub‚Äëtasks without caring whether the output goes to a rich terminal UI or a simple console printout.

**Key classes / flow**  

| Class | Implementation | Notes |
|-------|----------------|-------|
| `BaseProgress` | Abstract interface (`create_new_subtask`, `update_task`, `remove_subtask`). | Used by generators to drive progress. |
| `LibProgress` | Wraps **rich**‚Äôs `Progress`. Creates a *base* task (`General progress`) and optional *sub‚Äëtasks*; `update_task()` advances the current task, falling back to the base task. |
| `ConsoleTask` | Helper that prints start‚Äëup and incremental percentage updates. |
| `ConsoleGtiHubProgress` | Implements `BaseProgress` with `ConsoleTask` objects; suitable when Rich is unavailable (e.g., plain CI logs). |

**Interaction pattern**  

1. The document generation orchestrator instantiates a concrete `BaseProgress` (chosen by configuration).  
2. Before processing a chunk it calls `create_new_subtask(name, total_len)`.  
3. After each LLM call it invokes `update_task()`.  
4. When the chunk finishes it calls `remove_subtask()` to reset the context.

**Assumptions / side‚Äëeffects**  

* `total_len` reflects the number of LLM calls for the chunk; callers must supply an accurate count.  
* `LibProgress` assumes a live `rich.Progress` instance; `ConsoleGtiHubProgress` assumes a terminal that can accept `print`.  
* No persistence ‚Äì progress state lives only in memory for the duration of the run.
[2026-01-25 14:34:56.632795] [INFO] Generating documentation for a part...
[2026-01-25 14:34:56.632823] [INFO] Generating answer...
[2026-01-25 14:35:19.343305] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:35:19.343369] [INFO] Answer: <a name="metadata"></a>## Project metadata  

`pyproject.toml` defines the **autodocgenerator** package (v‚ÄØ0.8.0).  
* **Description** ‚Äì a helper that automatically creates documentation for a code‚Äëbase.  
* **Authors** ‚Äì dima‚Äëon (‚ÄØsinica911@gmail.com‚ÄØ).  
* **License** ‚Äì MIT, bundled as plain‚Äëtext.  
* **Python requirement** ‚Äì‚ÄØ‚â•‚ÄØ3.11‚ÄØ&&‚ÄØ<‚ÄØ4.0, ensuring use of modern typing & async features.  

<a name="dependencies"></a>## Core runtime dependencies  

| Category | Key packages | Rationale |
|----------|--------------|-----------|
| **LLM / AI** | `openai`, `google‚Äëgenai`, `groq` | Unified wrappers for multiple LLM providers used by the generation engine. |
| **Async & HTTP** | `anyio`, `httpx`, `httpcore`, `websockets` | Provide async‚Äëfriendly networking for API calls and streaming responses. |
| **Data handling** | `pydantic`, `annotated‚Äëtypes`, `pyyaml`, `msgmsg`, `zstandard` | Strong validation, config loading, and efficient (de)compression of large artefacts. |
| **CLI / UX** | `rich`, `rich_progress`, `tqdm`, `colorama`, `cleo` | Render colourful terminals, progress bars and command‚Äëline entry points. |
| **Utilities** | `requests`, `tenacity`, `cachecontrol`, `tomlkit`, `platformdirs` | HTTP retries, caching, config file handling and cross‚Äëplatform temp dirs. |
| **Misc** | `certifi`, `urllib3`, `charset‚Äënormalizer`, `idna` | Secure TLS and robust URL handling.  

The list purposefully pins exact versions (pinned via `==`) to guarantee reproducible builds and avoid breaking changes in CI pipelines.

<a name="buildsystem"></a>## Build system  

```toml
[build-system]
requires = ["poetry-core>=2.0.0"]
build-backend = "poetry.core.masonry.api"
```  

Poetry is the chosen build front‚Äëend; `poetry-core` supplies the PEP‚ÄØ517 backend that reads this file, resolves the locked dependency graph, and builds a wheel or sdist.  

**Interaction** ‚Äì All internal modules import the declared libraries directly; the global logger, progress abstractions, and document‚Äëgeneration pipeline rely on the async, UI and LLM packages listed above. No runtime side‚Äëeffects occur during import ‚Äì the file is solely consumed by packaging tools.
[2026-01-25 14:35:19.343418] [INFO] Documentation for part generated. total length: 2156
[2026-01-25 14:35:19.343442] [INFO] Part Documentation: <a name="metadata"></a>## Project metadata  

`pyproject.toml` defines the **autodocgenerator** package (v‚ÄØ0.8.0).  
* **Description** ‚Äì a helper that automatically creates documentation for a code‚Äëbase.  
* **Authors** ‚Äì dima‚Äëon (‚ÄØsinica911@gmail.com‚ÄØ).  
* **License** ‚Äì MIT, bundled as plain‚Äëtext.  
* **Python requirement** ‚Äì‚ÄØ‚â•‚ÄØ3.11‚ÄØ&&‚ÄØ<‚ÄØ4.0, ensuring use of modern typing & async features.  

<a name="dependencies"></a>## Core runtime dependencies  

| Category | Key packages | Rationale |
|----------|--------------|-----------|
| **LLM / AI** | `openai`, `google‚Äëgenai`, `groq` | Unified wrappers for multiple LLM providers used by the generation engine. |
| **Async & HTTP** | `anyio`, `httpx`, `httpcore`, `websockets` | Provide async‚Äëfriendly networking for API calls and streaming responses. |
| **Data handling** | `pydantic`, `annotated‚Äëtypes`, `pyyaml`, `msgmsg`, `zstandard` | Strong validation, config loading, and efficient (de)compression of large artefacts. |
| **CLI / UX** | `rich`, `rich_progress`, `tqdm`, `colorama`, `cleo` | Render colourful terminals, progress bars and command‚Äëline entry points. |
| **Utilities** | `requests`, `tenacity`, `cachecontrol`, `tomlkit`, `platformdirs` | HTTP retries, caching, config file handling and cross‚Äëplatform temp dirs. |
| **Misc** | `certifi`, `urllib3`, `charset‚Äënormalizer`, `idna` | Secure TLS and robust URL handling.  

The list purposefully pins exact versions (pinned via `==`) to guarantee reproducible builds and avoid breaking changes in CI pipelines.

<a name="buildsystem"></a>## Build system  

```toml
[build-system]
requires = ["poetry-core>=2.0.0"]
build-backend = "poetry.core.masonry.api"
```  

Poetry is the chosen build front‚Äëend; `poetry-core` supplies the PEP‚ÄØ517 backend that reads this file, resolves the locked dependency graph, and builds a wheel or sdist.  

**Interaction** ‚Äì All internal modules import the declared libraries directly; the global logger, progress abstractions, and document‚Äëgeneration pipeline rely on the async, UI and LLM packages listed above. No runtime side‚Äëeffects occur during import ‚Äì the file is solely consumed by packaging tools.
[2026-01-25 14:35:19.343516] [INFO] Documentation generation by parts completed.

                       Total documentation length: 33475
[2026-01-25 14:35:19.343650] [INFO] Documentation: <a name="package-initialization"></a>
## üì¶ Package Initialization ‚Äì‚ÄØAutomatic Logger Bootstrap

**File:** `autodocgenerator/__init__.py`

### Responsibility
This tiny entry‚Äëpoint prepares the **Auto Doc Generator (ADG)** package for use by:

1. Emitting a short banner (`print("ADG")`) the moment the package is imported ‚Äì useful for CI visibility.  
2. Exposing a **singleton logger** (`logger`) that the rest of the codebase (`engine`, `factory`, `ui`, etc.) can import without having to instantiate their own logger objects.

### Core Logic
```python
print("ADG")
from .ui.logging import BaseLogger, BaseLoggerTemplate, InfoLog, ErrorLog, WarningLog

logger = BaseLogger()
logger.set_logger(BaseLoggerTemplate())
```

| Element | What it does |
|---------|--------------|
| `print("ADG")` | Writes a one‚Äëline identifier to stdout during import. |
| `BaseLogger` | Central logging fa√ßade (wraps Python‚Äôs `logging` module). |
| `BaseLoggerTemplate` | Supplies default handlers/formatters (e.g., Rich‚Äëstyled console output). |
| `logger = BaseLogger()` | Creates a **global logger instance** that lives for the lifetime of the interpreter. |
| `logger.set_logger(BaseLoggerTemplate())` | Configures the global logger with the default template, enabling `InfoLog`, `ErrorLog`, `WarningLog` helpers throughout the project. |

### Interaction with Other Components
- **Engine & Factory modules** import `logger` (`from autodocgenerator import logger`) to record progress, errors, and debug information.
- **UI components** (`autodocgenerator/ui/logging.py`) define the concrete logging behaviour; this file merely wires the concrete implementation into a shared instance.
- Because the logger is created at import time, any sub‚Äëmodule loaded **after** `autodocgenerator` will automatically share the same configured logger, guaranteeing consistent log formatting and destination.

### Assumptions & Side Effects
- **Assumption:** The `ui.logging` module provides `BaseLogger`, `BaseLoggerTemplate`, and the log‚Äëlevel helpers. If those symbols are missing or renamed, import will fail, breaking the entire package initialization.
- **Side effect:** The `print` statement produces output on every import, which may be noisy in production but is intentional for CI diagnostics.
- **Output:** No return value; the module‚Äôs sole export is the `logger` object.

### Usage Example
```python
# In any sub‚Äëmodule:
from autodocgenerator import logger

logger.info("Documentation generation started")
```

The above call will emit a formatted info message using the globally configured `BaseLoggerTemplate`.

**<a name="config-reader"></a>Config‚ÄØReader & Settings Builder**  
`config_reader.py` centralises all static configuration for the autodoc pipeline.  
* **Responsibility** ‚Äì Parse the user‚Äëprovided `autodocconfig.yml`, materialise a rich `Config` object and expose ready‚Äëto‚Äëuse factories.  
* **Key classes**  
  * `ProjectConfigSettings` ‚Äì tiny container for runtime flags (`save_logs`, `log_level`). `load_settings` mutates the instance from a dict.  
  * `Config` ‚Äì aggregates ignore patterns, language, project metadata, custom modules and a `ProjectConfigSettings` instance. Fluent setters (`set_language`, `add_ignore_file`, ‚Ä¶) return `self` for chaining.  
  * `read_config` ‚Äì entry‚Äëpoint called by the runner; it loads YAML, populates a `Config`, registers custom modules (`CustomModule`) and returns the populated object.  
* **Interactions** ‚Äì `Config.get_project_settings` builds a `ProjectSettings` (used later by `Manager`). `Config.get_doc_factory` creates two `DocFactory` instances: one for user‚Äëdefined custom modules, another for built‚Äëin intro modules (`IntroLinks`, optionally `IntroText`).  

---

**<a name="run-file"></a>Run‚ÄØFile ‚Äì Orchestrating Documentation Generation**  
`run_file.py` glues together the parsed configuration, language models, and the core `Manager`.  

* **Responsibility** ‚Äì Initialise GPT models, construct a `Manager` with all runtime parameters, invoke the generation steps, clean the cache and return the final markdown document.  
* **Core function** `gen_doc`  
  * **Inputs** ‚Äì `ProjectSettings`, `ProjectConfigSettings`, ignore list, project root, and two `DocFactory` objects.  
  * **Flow** ‚Äì  
    1. Create `GPTModel` (sync) and `AsyncGPTModel`.  
    2. Instantiate `Manager` (provides file scanning, chunking, LLM calls, progress UI).  
    3. Run a fixed sequence of generation methods: source‚Äëcode docs, global info, doc parts, then the two factories.  
    4. Clear temporary cache and read the assembled output via `manager.read_file_by_file_key("output_doc")`.  
* **Side‚Äëeffects** ‚Äì Writes intermediate files to the `.auto_doc_cache` directory, logs progress via `ConsoleGtiHubProgress`, and may produce a file‚Äëbased logger if `pcs.save_logs` is enabled.  

Together, these modules translate a declarative YAML config into a fully‚Äërendered documentation artefact, feeding the rest of the system (e.g., UI, CI hooks) with a single string result.

**## <a name="prompt-templates"></a>Prompt templates ‚Äì static instruction blocks**  
The module houses the core textual prompts (`BASE_SYSTEM_TEXT`, `BASE_PART_COMPLITE_TEXT`, `BASE_INTRODACTION_CREATE_TEXT`, `BASE_INTRO_CREATE`, `BASE_SETTINGS_PROMPT`).  
These strings are injected into the LLM request payloads by the **PromptFactory** (see the factories section). They define the conversational role, step‚Äëby‚Äëstep workflow, and output format for every generation stage (system‚Äëlevel, part‚Äëlevel, executive‚Äëtree, project overview, and settings memorisation). No runtime logic resides here ‚Äì they are immutable resources.

**## <a name="compression-prompt-factory"></a>Dynamic compression prompt generator**  
`get_BASE_COMPRESS_TEXT(start, power)` builds a sized‚Äëaware prompt for ‚Äúlarge‚Äësnippet compression‚Äù.  
*Inputs*: `start` (estimated max characters of the snippet) and `power` (division factor controlling summary length).  
*Output*: A formatted multi‚Äëline string that instructs the model to analyse, summarise, and provide a strict usage example. The function is called by the **CompressFactory** when the engine decides a snippet exceeds the configured threshold. No side‚Äëeffects besides returning the string.

**## <a name="environment-setup"></a>Environment initialisation**  
At import time the module loads `.env` via `dotenv.load_dotenv()`, extracts `API_KEY`, and raises an exception if it is missing. It also defines `MODELS_NAME`, the list of model identifiers used by the **ModelProvider** throughout the system. This step guarantees that downstream components (LLM client, progress reporters) have the required credentials and model catalogue available.  

*Assumptions*: `.env` exists in the working directory; the key is named `API_KEY`.  
*Side‚Äëeffects*: environment variable lookup, possible `Exception` on missing key.

**## <a name="model-exception"></a>ModelExhaustedException**  
Raised when the internal list `regen_models_name` becomes empty ‚Äì i.e. every configured model has failed. It bubbles up to the calling factory (`CompressFactory`/`GenerateFactory`) so the caller can abort or report a fatal error. No side‚Äëeffects besides the exception itself.

**## <a name="model-base"></a>Base model hierarchy (`ParentModel`, `Model`, `AsyncModel`)**  
*Responsibility*: Provide shared infrastructure for all LLM wrappers.  
*Key logic*:  
- Loads `API_KEY` and `MODELS_NAME` from the global config.  
- Stores a mutable `History` (system prompt + later messages).  
- Randomises `regen_models_name` when `use_random=True` to spread load across models.  
- Tracks `current_model_index` for round‚Äërobin fallback.  
*Interfaces*:  
- `generate_answer` (sync/async placeholder).  
- `get_answer` / `get_answer_without_history` ‚Äì add user/assistant entries to `History` and delegate to `generate_answer`.  
*Assumptions*: `BASE_SYSTEM_TEXT`, `API_KEY`, `MODELS_NAME` are defined; `History` is cheap to copy.

**## <a name="gpt-model"></a>Concrete implementations (`GPTModel`, `AsyncGPTModel`)**  
Both inherit from the respective base class and instantiate a Groq client (`Groq` or `AsyncGroq`) with the supplied `api_key`.  
`generate_answer` builds the message payload (`self.history.history` or a raw `prompt`), then enters a **fallback loop**:  
1. Pick `model_name = regen_models_name[current_model_index]`.  
2. Call `client.chat.completions.create`.  
3. On any exception log a warning, advance `current_model_index` (wrap‚Äëaround), and retry.  
If the list is exhausted, `ModelExhaustedException` is raised.  
Successful calls return `chat_completion.choices[0].message.content` and emit informational logs via `BaseLogger`.  
*Side‚Äëeffects*: network request, logging, possible change of `current_model_index`.  

These components are the bridge between the **factory layer** (which selects `GPTModel` or `AsyncGPTModel`) and the **UI/logging subsystem**, ensuring robust model selection, history management, and transparent error handling.

<a name="base_module_abstract_interface"></a>
## BaseModule ‚Äì Abstract Interface for Documentation Generators  

`BaseModule` defines the minimal contract for all factory‚Äëlevel generators.  
* **Responsibility** ‚Äì Provide a uniform `generate(info, model)` method that each concrete module must implement.  
* **Interaction** ‚Äì Modules receive a plain‚Äëdictionary `info` (containing pre‚Äëprocessed code, language, etc.) and a `Model` instance (sync or async) from the engine layer. No direct I/O occurs here; the method should be pure‚Äëlogic.  
* **Key Points**  
  * Inherits from `ABC`; the abstract method forces implementation in subclasses.  
  * Constructor does nothing but may be extended by subclasses for configuration.  

<a name="doc_factory_orchestrator"></a>
## DocFactory ‚Äì Orchestrator of Module Execution  

`DocFactory` wires together a list of `BaseModule` instances, handles progress reporting and logging, and concatenates their outputs.  

* **Responsibility** ‚Äì Sequentially run each module, aggregate their Markdown/HTML snippets, and expose a single documentation string.  
* **Inputs**  
  * `*modules` ‚Äì variadic list of instantiated `BaseModule` subclasses.  
  * `info: dict` ‚Äì shared context (code fragments, language, global data).  
  * `model: Model` ‚Äì the language model used by modules for LLM calls.  
  * `progress: BaseProgress` ‚Äì UI component that visualises sub‚Äëtask creation, updates and removal.  
* **Outputs** ‚Äì A single string `output` containing each module‚Äôs result separated by double new‚Äëlines.  
* **Side‚Äëeffects** ‚Äì  
  * Calls `progress.create_new_subtask`/`update_task`/`remove_subtask`.  
  * Emits `InfoLog` entries via `BaseLogger` for each module execution and its raw result (log level‚ÄØ2 for detailed output).  
* **Logic Flow**  
  1. Initialise empty `output`.  
  2. Register a sub‚Äëtask in the UI matching the number of modules.  
  3. For each module:  
     * `module_result = module.generate(info, model)`  
     * Append result to `output`.  
     * Log success and the raw snippet.  
     * Notify progress UI of completion.  
  4. Tear down the sub‚Äëtask and return the concatenated documentation.  

<a name="custom_module_user_defined_section"></a>
## CustomModule ‚Äì User‚ÄëDefined Section Generator  

Located in `factory/modules/general_modules.py`.  

* **Responsibility** ‚Äì Produce a custom description block (e.g., ‚ÄúArchitecture Overview‚Äù) based on a user‚Äësupplied caption.  
* **Key Flow**  
  1. Split the mixed source code (`info["code_mix"]`) into chunks ‚â§‚ÄØ7000 symbols via `split_data`.  
  2. Call `generete_custom_discription` (LLM‚Äëbacked) with the chunks, the shared `model`, the user `discription`, and target `language`.  
  3. Return the generated text.  

* **Assumptions** ‚Äì `info` contains keys `"code_mix"` and `"language"`; the model can handle the chunked payload.  

<a name="intro_modules_links_and_text"></a>
## IntroLinks & IntroText ‚Äì Front‚ÄëMatter Builders  

Found in `factory/modules/intro.py`.  

* **IntroLinks**  
  * Extracts all HTML anchors from `info["full_data"]` using `get_all_html_links`.  
  * Passes the link list to `get_links_intro`, which asks the LLM to craft a concise introduction to the project's linked resources.  
  * Returns the generated paragraph.  

* **IntroText**  
  * Retrieves the high‚Äëlevel project description from `info["global_data"]`.  
  * Calls `get_introdaction` (LLM‚Äëdriven) to turn that data into a natural‚Äëlanguage introduction.  

Both modules rely exclusively on the engine‚Äôs `Model` for generation and emit no side‚Äëeffects beyond the returned strings.  

---

**Overall Placement** ‚Äì These factory components sit between the **engine layer** (model selection, retry logic) and the **UI/logging subsystem**. They encapsulate domain‚Äëspecific documentation fragments, coordinate progress feedback, and centralise logging, allowing the higher‚Äëlevel `DocFactory` to produce a coherent, multi‚Äësection document from heterogeneous code bases.

<a name="manager_overview"></a>
## Manager ‚Äì High‚ÄëLevel Orchestration Layer  

The **`Manager`** class lives in `autodocgenerator/manage.py` and coordinates the end‚Äëto‚Äëend documentation pipeline. It glues together the *pre‚Äëprocessor* (code mixing, splitting, compression), the *engine* (LLM models), the *factory* (modular doc sections), and the *UI* (progress bars & logging). All artefacts are cached in a hidden folder `.<project>/.auto_doc_cache` to avoid recomputation.

<a name="manager_state"></a>
### Core State  

| Attribute | Meaning |
|-----------|---------|
| `project_directory` | Root of the source repository. |
| `ignore_files` | Paths excluded from the code‚Äëmix. |
| `language` | Target documentation language (default **en**). |
| `project_settings` / `pcs` | User‚Äëdefined configuration objects. |
| `sync_model`, `async_model` | Instances of `Model` / `AsyncModel` used for LLM calls. |
| `logger` | `BaseLogger` writing to `<cache>/report.txt`. |
| `progress_bar` | `BaseProgress` instance visualising each stage. |

<a name="manager_methods"></a>
### Public Methods  

| Method | Purpose | Side‚Äëeffects |
|--------|---------|--------------|
| **`generate_code_file`** | Builds a *code‚Äëmix* (concatenated source) via `CodeMix` and writes it to `code_mix.txt`. | Logs start/completion, updates progress UI. |
| **`generate_global_info_file`** | Intended to compress the code‚Äëmix into a high‚Äëlevel overview (currently stubbed ‚Äì writes ‚Äúss‚Äù). | Logs, updates progress UI. |
| **`generete_doc_parts`** *(sic)* | Splits the mixed code and global info into ‚â§‚ÄØ`max_symbols` chunks, then runs `gen_doc_parts` **or** `async_gen_doc_parts` to produce a partial Markdown document (`output_doc.md`). | Logs start/completion, writes result, updates UI. |
| **`factory_generate_doc`** | Collects `global_data`, current `output_doc`, and raw `code_mix` into an `info` dict, then delegates to a `DocFactory` (which runs modules such as `IntroLinks`, `IntroText`, `CustomModule`). The factory output is prepended to the existing doc and persisted. | Detailed logging (module list & input sizes), UI update. |
| **`clear_cache`** | Optionally removes the log file if `pcs.save_logs` is `False`. | File system side‚Äëeffect. |
| **`read_file_by_file_key`** / **`get_file_path`** | Helper utilities for reading/writing cached artefacts. | Pure I/O. |

<a name="interaction_flow"></a>
### Interaction Flow  

1. **Code Mix** ‚Üí `generate_code_file` creates `code_mix.txt`.  
2. **Global Summary** ‚Üí `generate_global_info_file` (future compression).  
3. **Chunked Generation** ‚Üí `generete_doc_parts` invokes the *spliter/compressor* pipeline, optionally async.  
4. **Modular Assembly** ‚Üí `factory_generate_doc` hands the `info` dict to a `DocFactory`; each module uses the shared `Model` to call the LLM and returns a Markdown snippet.  
5. **Progress & Logging** ‚Üí Every stage reports via `BaseProgress` and writes `InfoLog` entries to the cache log file.

<a name="assumptions_and_limits"></a>
### Assumptions & Constraints  

* The cache directory exists or can be created.  
* `sync_model` / `async_model` are valid `Model` instances; if `None`, LLM calls will fail.  
* `generate_global_info_file` is a placeholder ‚Äì real implementation must call `compress_to_one`.  
* `progress_bar.update_task()` is safe to call even if no sub‚Äëtasks were registered.

---  

*This documentation is intended for developers onboarding the `autodocgenerator` codebase and needing a quick mental map of the `Manager` orchestrator.*

<a name="code_mix_overview"></a>## CodeMix ‚Äì Repository‚ÄØ‚Üî‚ÄØText‚ÄØPackager  

The **CodeMix** class lives in `autodocgenerator.preprocessor.code_mix`. Its sole purpose is to turn a source tree into a single, LLM‚Äëfriendly text file (`codemix.txt`). The manager (`Manager.generate_code_file`) instantiates this component, feeds it the project root and an ignore list, then stores the produced file for later chunk‚Äësplitting and documentation generation.

<a name="code_mix_responsibility"></a>### Responsibility  
* Walk the directory tree rooted at `root_dir`.  
* Skip files/folders that match any pattern in `ignore_patterns`.  
* Emit a readable *structure* section followed by the raw contents of each accepted file, wrapped in `<file path="‚Ä¶">` tags.

<a name="code_mix_interaction"></a>### Interaction with the System  
* **Input**: `root_dir` (project directory), `ignore_patterns` (global `ignore_list`).  
* **Output**: Text file (`output_file`) stored in the cache; the manager later reads it via `Manager.read_file_by_file_key`.  
* **Side‚Äëeffects**: Writes to disk, logs each ignored path through `BaseLogger` ‚Üí `InfoLog`.  

<a name="code_mix_key_functions"></a>### Key Functions  

| Function | Role |
|----------|------|
| `__init__(root_dir, ignore_patterns)` | Resolve the absolute project path, store ignore patterns, create a logger. |
| `should_ignore(path)` | Convert `path` to a relative string and test it against every glob in `ignore_patterns` (full‚Äëpath, basename, and each path component). Returns‚ÄØ`True`‚ÄØ‚Üí‚ÄØskip. |
| `build_repo_content(output_file)` | 1Ô∏è‚É£ Write a *tree view* of the repository (indented by depth). 2Ô∏è‚É£ Insert a separator line. 3Ô∏è‚É£ Append each non‚Äëignored file‚Äôs content wrapped in `<file ‚Ä¶>` tags. Errors while reading a file are captured and written as plain text. |

<a name="code_mix_assumptions"></a>### Assumptions & Limits  

* The supplied `root_dir` exists and is readable.  
* `ignore_patterns` are valid Unix‚Äëstyle globs; patterns like `"*.md"` will also suppress markdown docs (the system later generates its own).  
* All files are opened with UTF‚Äë8‚ÄØ+‚ÄØ`errors="ignore"` ‚Äì binary files may produce garbled output but will not crash the process.  

<a name="code_mix_usage"></a>### Typical Usage (as seen in `__main__`)  

```python
packer = CodeMix(root_dir=PROJECT_PATH, ignore_patterns=ignore_list)
packer.build_repo_content("codemix.txt")
```

The generated `codemix.txt` becomes the *code‚Äëmix* consumed by the rest of the autodoc pipeline.

<a name="compressor_overview"></a>##‚ÄØCompressor‚ÄØPre‚Äëprocessor (autodocgenerator/preprocessor/compressor.py)

The **compressor** module reduces large code‚Äëbase texts to compact summaries that can be fed to downstream LLM stages (e.g., similarity comparison, description generation). It works together with the **engine** (GPT‚Äëmodel wrappers) and the **UI** progress bar, while respecting project‚Äëwide prompts defined in `settings.ProjectSettings`.

---

<a name="core_functions"></a>### Core Functions  

| Function | Responsibility | Key I/O | Side‚Äëeffects |
|----------|----------------|---------|--------------|
| **compress** | Builds a 3‚Äëpart prompt (project system prompt, compression instruction from `get_BASE_COMPRESS_TEXT`, user payload) and calls `model.get_answer_without_history`. Returns the compressed string. | `data: str`, `project_settings: ProjectSettings`, `model: Model`, `compress_power: int` ‚Üí `str` | None (pure LLM call). |
| **compress_and_compare** | Synchronously compresses a list of files, concatenating *compress_power* items per chunk to enable later ‚Äúcompare‚Äëall‚Äù steps. Updates a `BaseProgress` sub‚Äëtask. | `data: List[str]` ‚Üí `List[str]` (chunks) | Progress bar updates. |
| **async_compress** | Same as `compress` but runs inside a semaphore‚Äëprotected coroutine, allowing limited parallelism. | `data: str` ‚Üí `str` (awaitable) | Progress bar update after each call. |
| **async_compress_and_compare** | Fires many `async_compress` coroutines (default concurrency‚ÄØ=‚ÄØ4), gathers results, then re‚Äëchunks them like the sync version. | `data: List[str]` ‚Üí `List[str]` (awaitable) | Progress bar sub‚Äëtask handling. |
| **compress_to_one** | Repeatedly compresses until a single aggregated chunk remains. Chooses synchronous or asynchronous path per `use_async`. | `data: List[str]` ‚Üí `str` (final compressed blob) | May invoke the event loop (`asyncio.run`). |
| **generate_discribtions_for_code** | Sends each compressed code block to the LLM with a strict ‚Äúwrite a developer‚Äëfacing guide‚Äù system prompt. Collects the generated descriptions. | `data: List[str]` ‚Üí `List[str]` (descriptions) | Progress bar updates. |

---

<a name="interaction_flow"></a>### Interaction Flow  

1. **Input preparation** ‚Äì Earlier stages (e.g., `CodeMix`) produce a list of raw file contents.  
2. **Compression** ‚Äì `compress_to_one` (or its helpers) iteratively reduces the list, using either the sync or async pipeline.  
3. **Description** ‚Äì The final compressed blobs are passed to `generate_discribtions_for_code`, which yields markdown‚Äëready developer guides.  
4. **Output** ‚Äì The resulting strings are written to the *code‚Äëmix* artifact consumed by the rest of the autodoc pipeline.

---

<a name="assumptions"></a>### Assumptions & Constraints  

* `project_settings.prompt` is a valid system‚Äëprompt string.  
* `compress_power` ‚â•‚ÄØ2; if the remaining data size is smaller, the function automatically lowers it to‚ÄØ2.  
* Models supplied must implement `get_answer_without_history` (sync) or its async counterpart.  
* The UI progress bar is thread‚Äësafe for the modest concurrency used (semaphore‚ÄØ=‚ÄØ4).  

---  

*This module is the bridge between raw source files and the LLM‚Äëdriven documentation generation, providing scalable, progress‚Äëtracked compression.*

<a name="postprocess-module"></a>##‚ÄØ`autodocgenerator.preprocessor.postprocess` ‚Äì Final‚Äëstage Markdown polishing  

**Responsibility**  
Transforms the raw LLM‚Äëgenerated documentation into a navigation‚Äëfriendly Markdown file. It extracts headings, creates HTML‚Äëstyle anchors, builds a table‚Äëof‚Äëcontents‚Äëlike list of links, and can ask the model to write custom sections that follow a strict ‚Äúno‚Äëhallucination‚Äù policy.

**Key functions**

| Function | Purpose | I/O | Side‚Äëeffects |
|----------|---------|-----|--------------|
| `generate_markdown_anchor(header)` | Normalises a heading string and returns a Markdown anchor (`#slug`). | `header: str` ‚Üí `str` | Pure function. |
| `get_all_topics(data)` | Scans a Markdown document for level‚Äë2 headings (`## ‚Ä¶`), returns the plain titles and their generated anchors. | `data: str` ‚Üí `(titles: list[str], anchors: list[str])` | Pure. |
| `get_all_html_links(data)` | Finds existing `<a name=‚Ä¶>` tags, extracts the name (max‚ÄØ25‚ÄØchars) and returns them as `#name` links. | `data: str` ‚Üí `list[str]` | Logs extraction progress via `BaseLogger`. |
| `get_links_intro(links, model, language='en')` | Sends the list of links to the LLM with the *BASE_INTRODACTION_CREATE_TEXT* system prompt and returns a generated introductory paragraph. | `links: list[str]`, `model: Model`, `language: str` ‚Üí `str` | Calls the model (`model.get_answer_without_history`). |
| `get_introdaction(global_data, model, language='en')` | Wraps the whole document in a system prompt (*BASE_INTRO_CREATE*) and asks the model for a high‚Äëlevel introduction. | `global_data: str` ‚Üí `str` | Same LLM call. |
| `generete_custom_discription(splited_data, model, custom_description, language='en')` | Iterates over pre‚Äësplit chunks, asks the model to produce a concise description (or ‚Äú!noinfo‚Äù) respecting a strict rule set, and returns the first non‚Äëempty result. | `splited_data: str`, `custom_description: str` ‚Üí `str` | May log intermediate steps; performs multiple LLM calls until a valid answer is found. |

**Interaction with other components**  

* **Engine (`Model` / `GPTModel`)** ‚Äì All functions that need LLM output delegate to `model.get_answer_without_history`. The model must implement the synchronous interface defined in `engine.models.model`.  
* **UI (`BaseLogger`)** ‚Äì Used for informational messages; no UI state is mutated besides console/file logging.  
* **Settings (`ProjectSettings`)** ‚Äì The system prompts (`BASE_INTRODACTION_CREATE_TEXT`, `BASE_INTRO_CREATE`) are defined globally and injected here; they rely on the project‚Äëwide prompt built elsewhere.  

**Assumptions & constraints**  

* Input Markdown uses `## ` for top‚Äëlevel sections; other heading levels are ignored.  
* `<a name=` tags are well‚Äëformed and short (‚â§‚ÄØ25‚ÄØchars); longer names are skipped.  
* The supplied `model` conforms to the `Model` protocol and is ready for synchronous calls.  
* Logging level‚ÄØ1 is used for verbose link dumps; the UI expects `InfoLog` objects.  

**Typical flow**  

1. After compression (`compress_to_one`) the pipeline receives a large Markdown string.  
2. `get_all_topics` builds a TOC; `get_all_html_links` extracts any manual anchors.  
3. `get_links_intro` creates a short paragraph introducing the link list.  
4. `get_introdaction` produces a global introduction for the whole document.  
5. Optional custom sections are added via `generete_custom_discription`.  

The module thus finalises the raw LLM output into a well‚Äëstructured, navigable Markdown artifact ready for downstream consumption (e.g., writing to the *code‚Äëmix* artifact).

<a name="overview"></a>## Overview  
The module turns a large *code‚Äëmix* Markdown blob into a complete, navigable documentation set. It first **splits** the input into size‚Äëbounded chunks, then **feeds each chunk to an LLM** (sync‚ÄØ`Model` or async‚ÄØ`AsyncModel`) using a fixed system prompt (`BASE_PART_COMPLITE_TEXT`). Results are concatenated, trimmed, and reported through `BaseLogger` and `BaseProgress`.

<a name="split-data"></a>## `split_data` (helper)  
*Inputs*: `full_code_mix: str`, `max_symbols: int`  
*Outputs*: `list[str]` ‚Äì chunks ‚â§‚ÄØ`max_symbols‚ÄØ√ó‚ÄØ1.5`.  
Logic: recursively cuts overly long pieces, then packs them into groups respecting a 1.25‚ÄØ√ó‚ÄØlimit. Purely deterministic; no side effects.

<a name="write-sync"></a>## `write_docs_by_parts`  
Generates documentation for a single chunk.  
*Flow*: builds a chat‚ÄØprompt (language system message + `BASE_PART_COMPLITE_TEXT` + optional previous part), calls `model.get_answer_without_history`, strips surrounding ````` fences, logs length and content.  
*Side effects*: logs via `BaseLogger`.

<a name="write-async"></a>## `async_write_docs_by_parts`  
Async counterpart; runs under a semaphore to limit parallel LLM calls. Accepts an `update_progress` callback to tick `BaseProgress`. Otherwise mirrors the sync version.

<a name="gen-sync"></a>## `gen_doc_parts`  
Orchestrates synchronous generation:  
1. `split_data` ‚Üí `splited_data`.  
2. For each chunk call `write_docs_by_parts`, append result, keep the last‚ÄØ3‚ÄØk chars as context for the next chunk.  
3. Updates a sub‚Äëtask in `progress_bar`.  
Returns the full assembled documentation.

<a name="gen-async"></a>## `async_gen_doc_parts`  
Same orchestration but launches `async_write_docs_by_parts` tasks (max‚ÄØ4 concurrent) and gathers results. Progress is driven by the provided callback.

**Interactions** ‚Äì `Model/AsyncModel` (LLM engine), `BaseLogger` (info logs), `BaseProgress` (UI progress bar). All functions assume well‚Äëformed Markdown input and a ready LLM conforming to the `Model` protocol.

<a name="logger"></a>## `autodocgenerator/ui/logging.py` ‚Äì Centralised, singleton logging  

**Responsibility** ‚Äì Provides lightweight, level‚Äëaware logging that can be routed either to the console or to a file. The module supplies tiny log‚Äërecord objects (`ErrorLog`, `WarningLog`, `InfoLog`) and a `BaseLogger` singleton that forwards records to a configurable *logger template*.

**Key classes / flow**  

| Class | Role |
|-------|------|
| `BaseLog` | Holds a message and a numeric *level*; formats plain text; supplies a timestamp prefix via `_log_prefix`. |
| `ErrorLog`, `WarningLog`, `InfoLog` | Override `format()` to prepend `[ERROR]`, `[WARNING]`, `[INFO]` respectively. |
| `BaseLoggerTemplate` | Base for concrete back‚Äëends. Implements `log()` (default ‚Üí `print`) and `global_log()` which respects the `log_level` filter (`log_level < 0` means ‚Äúlog everything‚Äù). |
| `FileLoggerTemplate` | Writes formatted logs to a given file, one line per record. |
| `BaseLogger` | **Singleton** (`__new__` caches a single instance). `set_logger()` injects a `BaseLoggerTemplate`; `log()` forwards to `logger_template.global_log()`. |

**Interactions** ‚Äì Other components (e.g., the document‚Äëgeneration pipeline) obtain the global logger via `BaseLogger()` and call `log(ErrorLog(...))`. The logger forwards to the selected template, which may be a `FileLoggerTemplate` used by CI pipelines or the default console logger for interactive runs.

**Assumptions / side‚Äëeffects**  

* Caller supplies a `BaseLog`‚Äëderived object; the `level` is compared against the configured filter.  
* `FileLoggerTemplate` opens the file on every call ‚Äì acceptable for low‚Äëvolume logs.  
* No thread‚Äësafety guarantees; intended for single‚Äëthreaded or externally synchronized contexts.

---

<a name="progress"></a>## `autodocgenerator/ui/progress_base.py` ‚Äì Progress‚Äëbar abstraction  

**Responsibility** ‚Äì Supplies a thin abstraction (`BaseProgress`) over concrete progress reporters, allowing the generation engine to report sub‚Äëtasks without caring whether the output goes to a rich terminal UI or a simple console printout.

**Key classes / flow**  

| Class | Implementation | Notes |
|-------|----------------|-------|
| `BaseProgress` | Abstract interface (`create_new_subtask`, `update_task`, `remove_subtask`). | Used by generators to drive progress. |
| `LibProgress` | Wraps **rich**‚Äôs `Progress`. Creates a *base* task (`General progress`) and optional *sub‚Äëtasks*; `update_task()` advances the current task, falling back to the base task. |
| `ConsoleTask` | Helper that prints start‚Äëup and incremental percentage updates. |
| `ConsoleGtiHubProgress` | Implements `BaseProgress` with `ConsoleTask` objects; suitable when Rich is unavailable (e.g., plain CI logs). |

**Interaction pattern**  

1. The document generation orchestrator instantiates a concrete `BaseProgress` (chosen by configuration).  
2. Before processing a chunk it calls `create_new_subtask(name, total_len)`.  
3. After each LLM call it invokes `update_task()`.  
4. When the chunk finishes it calls `remove_subtask()` to reset the context.

**Assumptions / side‚Äëeffects**  

* `total_len` reflects the number of LLM calls for the chunk; callers must supply an accurate count.  
* `LibProgress` assumes a live `rich.Progress` instance; `ConsoleGtiHubProgress` assumes a terminal that can accept `print`.  
* No persistence ‚Äì progress state lives only in memory for the duration of the run.

<a name="metadata"></a>## Project metadata  

`pyproject.toml` defines the **autodocgenerator** package (v‚ÄØ0.8.0).  
* **Description** ‚Äì a helper that automatically creates documentation for a code‚Äëbase.  
* **Authors** ‚Äì dima‚Äëon (‚ÄØsinica911@gmail.com‚ÄØ).  
* **License** ‚Äì MIT, bundled as plain‚Äëtext.  
* **Python requirement** ‚Äì‚ÄØ‚â•‚ÄØ3.11‚ÄØ&&‚ÄØ<‚ÄØ4.0, ensuring use of modern typing & async features.  

<a name="dependencies"></a>## Core runtime dependencies  

| Category | Key packages | Rationale |
|----------|--------------|-----------|
| **LLM / AI** | `openai`, `google‚Äëgenai`, `groq` | Unified wrappers for multiple LLM providers used by the generation engine. |
| **Async & HTTP** | `anyio`, `httpx`, `httpcore`, `websockets` | Provide async‚Äëfriendly networking for API calls and streaming responses. |
| **Data handling** | `pydantic`, `annotated‚Äëtypes`, `pyyaml`, `msgmsg`, `zstandard` | Strong validation, config loading, and efficient (de)compression of large artefacts. |
| **CLI / UX** | `rich`, `rich_progress`, `tqdm`, `colorama`, `cleo` | Render colourful terminals, progress bars and command‚Äëline entry points. |
| **Utilities** | `requests`, `tenacity`, `cachecontrol`, `tomlkit`, `platformdirs` | HTTP retries, caching, config file handling and cross‚Äëplatform temp dirs. |
| **Misc** | `certifi`, `urllib3`, `charset‚Äënormalizer`, `idna` | Secure TLS and robust URL handling.  

The list purposefully pins exact versions (pinned via `==`) to guarantee reproducible builds and avoid breaking changes in CI pipelines.

<a name="buildsystem"></a>## Build system  

```toml
[build-system]
requires = ["poetry-core>=2.0.0"]
build-backend = "poetry.core.masonry.api"
```  

Poetry is the chosen build front‚Äëend; `poetry-core` supplies the PEP‚ÄØ517 backend that reads this file, resolves the locked dependency graph, and builds a wheel or sdist.  

**Interaction** ‚Äì All internal modules import the declared libraries directly; the global logger, progress abstractions, and document‚Äëgeneration pipeline rely on the async, UI and LLM packages listed above. No runtime side‚Äëeffects occur during import ‚Äì the file is solely consumed by packaging tools.


[2026-01-25 14:35:19.343920] [INFO] Documentation generation by parts completed.
[2026-01-25 14:35:19.344851] [INFO] Starting factory documentation generation 

                                CustomModule CustomModule CustomModule 

                                Input params: language: 2 chars global_data: 2 chars full_data: 33475 chars code_mix: 240980 chars
                                
[2026-01-25 14:35:19.345208] [INFO] Starting data splitting...
[2026-01-25 14:35:19.345288] [INFO] Data split into 9 parts based on max symbols 7000.
[2026-01-25 14:35:19.345321] [INFO] Generating answer...
[2026-01-25 14:35:37.276147] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:35:37.276212] [INFO] Answer: <a name='install_workflow'> </a>
Explain how install workflow with `install.ps1` and `install.sh` scripts for install you should use links  
`irm https://raw.githubusercontent.com/Drag-GameStudio/ADG/main/install.ps1 | iex` for PowerShell and  
`curl -sSL https://raw.githubusercontent.com/Drag-GameStudio/ADG/main/install.sh | bash` for Linux‚Äëbased systems and also you have to add secret variable to GitHub Action `GROCK_API_KEY` with your API key from Grock docs <https://grockdocs.com> to make it work.
[2026-01-25 14:35:37.276259] [INFO] Module CustomModule generated its part of the documentation.
[2026-01-25 14:35:37.276286] [INFO] Module Output: <a name='install_workflow'> </a>
Explain how install workflow with `install.ps1` and `install.sh` scripts for install you should use links  
`irm https://raw.githubusercontent.com/Drag-GameStudio/ADG/main/install.ps1 | iex` for PowerShell and  
`curl -sSL https://raw.githubusercontent.com/Drag-GameStudio/ADG/main/install.sh | bash` for Linux‚Äëbased systems and also you have to add secret variable to GitHub Action `GROCK_API_KEY` with your API key from Grock docs <https://grockdocs.com> to make it work.
[2026-01-25 14:35:37.276633] [INFO] Starting data splitting...
[2026-01-25 14:35:37.276719] [INFO] Data split into 9 parts based on max symbols 7000.
[2026-01-25 14:35:37.276754] [INFO] Generating answer...
[2026-01-25 14:35:53.678451] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:35:53.678515] [INFO] Answer: !noinfo
[2026-01-25 14:35:53.678552] [INFO] Generating answer...
[2026-01-25 14:36:01.673770] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:36:01.673831] [INFO] Answer: <a name='Using_Manager_class'> </a>
The **Manager** class is instantiated with the following parameters:

```python
manager = Manager(
    project_path,          # str ‚Äì path to the project root
    project_settings,      # ProjectSettings ‚Äì settings for the project
    pcs,                   # ProjectConfigSettings ‚Äì additional configuration
    sync_model=sync_model, # GPTModel ‚Äì synchronous model instance
    async_model=async_model, # AsyncGPTModel ‚Äì asynchronous model instance
    ignore_files=ignore_list, # list[str] ‚Äì files to ignore
    progress_bar=ConsoleGtiHubProgress(), # progress UI object
    language="en"          # str ‚Äì language code
)
```

**Full usage example** (taken from `autodocgenerator/auto_runner/run_file.py`):

```python
from autodocgenerator.manage import Manager
from autodocgenerator.factory.base_factory import DocFactory
from autodocgenerator.factory.modules.intro import IntroLinks, IntroText
from autodocgenerator.ui.progress_base import ConsoleGtiHubProgress
from autodocgenerator.preprocessor.settings import ProjectSettings
from .config_reader import Config, read_config, ProjectConfigSettings
from autodocgenerator.engine.models.gpt_model import GPTModel, AsyncGPTModel
from autodocgenerator.engine.config.config import API_KEY

def gen_doc(project_settings: ProjectSettings,
           pcs: ProjectConfigSettings,
           ignore_list: list[str],
           project_path: str,
           doc_factory: DocFactory,
           intro_factory: DocFactory):

    sync_model = GPTModel(API_KEY, use_random=False)
    async_model = AsyncGPTModel(API_KEY)

    manager = Manager(
        project_path,
        project_settings,
        pcs,
        sync_model=sync_model,
        async_model=async_model,
        ignore_files=ignore_list,
        progress_bar=ConsoleGtiHubProgress(),
        language="en"
    )

    manager.generate_code_file()
    manager.generate_global_info_file(use_async=False, max_symbols=8000)
    manager.generete_doc_parts(use_async=False, max_symbols=5000)
    manager.factory_generate_doc(doc_factory)
    manager.factory_generate_doc(intro_factory)

    manager.clear_cache()

    return manager.read_file_by_file_key("output_doc")

if __name__ == "__main__":
    with open("autodocconfig.yml", "r", encoding="utf-8") as file:
        config_data = file.read()
    config: Config = read_config(config_data)

    project_settings = config.get_project_settings()
    doc_factory, intro_factory = config.get_doc_factory()

    output_doc = gen_doc(
        project_settings,
        config.pcs,
        config.ignore_files,
        ".",
        doc_factory,
        intro_factory
    )
```
[2026-01-25 14:36:01.673903] [INFO] Module CustomModule generated its part of the documentation.
[2026-01-25 14:36:01.673929] [INFO] Module Output: <a name='Using_Manager_class'> </a>
The **Manager** class is instantiated with the following parameters:

```python
manager = Manager(
    project_path,          # str ‚Äì path to the project root
    project_settings,      # ProjectSettings ‚Äì settings for the project
    pcs,                   # ProjectConfigSettings ‚Äì additional configuration
    sync_model=sync_model, # GPTModel ‚Äì synchronous model instance
    async_model=async_model, # AsyncGPTModel ‚Äì asynchronous model instance
    ignore_files=ignore_list, # list[str] ‚Äì files to ignore
    progress_bar=ConsoleGtiHubProgress(), # progress UI object
    language="en"          # str ‚Äì language code
)
```

**Full usage example** (taken from `autodocgenerator/auto_runner/run_file.py`):

```python
from autodocgenerator.manage import Manager
from autodocgenerator.factory.base_factory import DocFactory
from autodocgenerator.factory.modules.intro import IntroLinks, IntroText
from autodocgenerator.ui.progress_base import ConsoleGtiHubProgress
from autodocgenerator.preprocessor.settings import ProjectSettings
from .config_reader import Config, read_config, ProjectConfigSettings
from autodocgenerator.engine.models.gpt_model import GPTModel, AsyncGPTModel
from autodocgenerator.engine.config.config import API_KEY

def gen_doc(project_settings: ProjectSettings,
           pcs: ProjectConfigSettings,
           ignore_list: list[str],
           project_path: str,
           doc_factory: DocFactory,
           intro_factory: DocFactory):

    sync_model = GPTModel(API_KEY, use_random=False)
    async_model = AsyncGPTModel(API_KEY)

    manager = Manager(
        project_path,
        project_settings,
        pcs,
        sync_model=sync_model,
        async_model=async_model,
        ignore_files=ignore_list,
        progress_bar=ConsoleGtiHubProgress(),
        language="en"
    )

    manager.generate_code_file()
    manager.generate_global_info_file(use_async=False, max_symbols=8000)
    manager.generete_doc_parts(use_async=False, max_symbols=5000)
    manager.factory_generate_doc(doc_factory)
    manager.factory_generate_doc(intro_factory)

    manager.clear_cache()

    return manager.read_file_by_file_key("output_doc")

if __name__ == "__main__":
    with open("autodocconfig.yml", "r", encoding="utf-8") as file:
        config_data = file.read()
    config: Config = read_config(config_data)

    project_settings = config.get_project_settings()
    doc_factory, intro_factory = config.get_doc_factory()

    output_doc = gen_doc(
        project_settings,
        config.pcs,
        config.ignore_files,
        ".",
        doc_factory,
        intro_factory
    )
```
[2026-01-25 14:36:01.674325] [INFO] Starting data splitting...
[2026-01-25 14:36:01.674416] [INFO] Data split into 9 parts based on max symbols 7000.
[2026-01-25 14:36:01.674449] [INFO] Generating answer...
[2026-01-25 14:36:19.294402] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:36:19.294461] [INFO] Answer: <a name='autodocconfig.yml_guide'></a>
**autodocconfig.yml structure**

- `project_name` ‚Äì string, name of the documented project.  
- `language` ‚Äì string, language code for the documentation (e.g., `"en"`).  
- `project_settings` ‚Äì mapping with optional flags:  
  - `save_logs` ‚Äì boolean, enables saving of generation logs.  
  - `log_level` ‚Äì integer, sets the verbosity of logs (e.g., `0`‚Äë`3`).  
- `project_additional_info` ‚Äì mapping for any extra key‚Äëvalue pairs that should be added to the generated docs (e.g., `"global idea": "‚Ä¶"`).  
- `ignore_files` *(optional)* ‚Äì list of glob patterns for files/directories that the generator must skip.  
- `custom_descriptions` ‚Äì list of strings, each string is a custom description that will be turned into a module by the generator.  

These keys are read by `autodocgenerator.auto_runner.config_reader.read_config` and used to build the documentation.
[2026-01-25 14:36:19.294504] [INFO] Module CustomModule generated its part of the documentation.
[2026-01-25 14:36:19.294528] [INFO] Module Output: <a name='autodocconfig.yml_guide'></a>
**autodocconfig.yml structure**

- `project_name` ‚Äì string, name of the documented project.  
- `language` ‚Äì string, language code for the documentation (e.g., `"en"`).  
- `project_settings` ‚Äì mapping with optional flags:  
  - `save_logs` ‚Äì boolean, enables saving of generation logs.  
  - `log_level` ‚Äì integer, sets the verbosity of logs (e.g., `0`‚Äë`3`).  
- `project_additional_info` ‚Äì mapping for any extra key‚Äëvalue pairs that should be added to the generated docs (e.g., `"global idea": "‚Ä¶"`).  
- `ignore_files` *(optional)* ‚Äì list of glob patterns for files/directories that the generator must skip.  
- `custom_descriptions` ‚Äì list of strings, each string is a custom description that will be turned into a module by the generator.  

These keys are read by `autodocgenerator.auto_runner.config_reader.read_config` and used to build the documentation.
[2026-01-25 14:36:19.294580] [INFO] Factory documentation generation completed.
[2026-01-25 14:36:19.295281] [INFO] Starting factory documentation generation 

                                IntroLinks 

                                Input params: language: 2 chars global_data: 2 chars full_data: 37550 chars code_mix: 240980 chars
                                
[2026-01-25 14:36:19.295344] [INFO] Extracting HTML links from documentation...
[2026-01-25 14:36:19.295472] [INFO] Extracted 38 HTML links from documentation.
[2026-01-25 14:36:19.295526] [INFO] Links: ["#install_workflow'", "#Using_Manager_class'", '#autodocconfig.yml_guide', '#package-initialization', '#config-reader', '#run-file', '#prompt-templates', '#environment-setup', '#model-exception', '#model-base', '#gpt-model', '#doc_factory_orchestrator', '#manager_overview', '#manager_state', '#manager_methods', '#interaction_flow', '#assumptions_and_limits', '#code_mix_overview', '#code_mix_responsibility', '#code_mix_interaction', '#code_mix_key_functions', '#code_mix_assumptions', '#code_mix_usage', '#compressor_overview', '#core_functions', '#interaction_flow', '#assumptions', '#postprocess-module', '#split-data', '#write-sync', '#write-async', '#gen-sync', '#gen-async', '#logger', '#progress', '#metadata', '#dependencies', '#buildsystem']
[2026-01-25 14:36:19.295577] [INFO] Generating introduction with links...
[2026-01-25 14:36:19.295618] [INFO] Generating answer...
[2026-01-25 14:36:28.879334] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-25 14:36:28.879398] [INFO] Answer: ## Executive Navigation Tree
- üìÇ **Installation & Setup**
  - [Install Workflow](#install_workflow')
  - [Autodoc Config Guide](#autodocconfig.yml_guide)
  - [Package Initialization](#package-initialization)
  - [Config Reader](#config-reader)
  - [Run File](#run-file)
  - [Prompt Templates](#prompt-templates)
  - [Environment Setup](#environment-setup)

- ‚öôÔ∏è **Manager & Orchestration**
  - [Using Manager Class](#Using_Manager_class')
  - [Doc Factory Orchestrator](#doc_factory_orchestrator)
  - [Manager Overview](#manager_overview)
  - [Manager State](#manager_state)
  - [Manager Methods](#manager_methods)
  - [Interaction Flow](#interaction_flow)

- ü§ñ **Model Layer**
  - [Model Exception](#model-exception)
  - [Model Base](#model-base)
  - [GPT Model](#gpt-model)

- üß© **Code Mix**
  - [Code Mix Overview](#code_mix_overview)
  - [Code Mix Responsibility](#code_mix_responsibility)
  - [Code Mix Interaction](#code_mix_interaction)
  - [Code Mix Key Functions](#code_mix_key_functions)
  - [Code Mix Assumptions](#code_mix_assumptions)
  - [Code Mix Usage](#code_mix_usage)

- üõ†Ô∏è **Core Functions & Utilities**
  - [Core Functions](#core_functions)
  - [Compressor Overview](#compressor_overview)
  - [Postprocess Module](#postprocess-module)
  - [Split Data](#split-data)
  - [Write Sync](#write-sync)
  - [Write Async](#write-async)
  - [Gen Sync](#gen-sync)
  - [Gen Async](#gen-async)
  - [Logger](#logger)
  - [Progress](#progress)
  - [Metadata](#metadata)
  - [Dependencies](#dependencies)
  - [Buildsystem](#buildsystem)

- üìÑ **Assumptions & Limits**
  - [Assumptions and Limits](#assumptions_and_limits)
  - [Assumptions](#assumptions)
[2026-01-25 14:36:28.879435] [INFO] Introduction with links generated.
[2026-01-25 14:36:28.879461] [INFO] Introduction Links: ## Executive Navigation Tree
- üìÇ **Installation & Setup**
  - [Install Workflow](#install_workflow')
  - [Autodoc Config Guide](#autodocconfig.yml_guide)
  - [Package Initialization](#package-initialization)
  - [Config Reader](#config-reader)
  - [Run File](#run-file)
  - [Prompt Templates](#prompt-templates)
  - [Environment Setup](#environment-setup)

- ‚öôÔ∏è **Manager & Orchestration**
  - [Using Manager Class](#Using_Manager_class')
  - [Doc Factory Orchestrator](#doc_factory_orchestrator)
  - [Manager Overview](#manager_overview)
  - [Manager State](#manager_state)
  - [Manager Methods](#manager_methods)
  - [Interaction Flow](#interaction_flow)

- ü§ñ **Model Layer**
  - [Model Exception](#model-exception)
  - [Model Base](#model-base)
  - [GPT Model](#gpt-model)

- üß© **Code Mix**
  - [Code Mix Overview](#code_mix_overview)
  - [Code Mix Responsibility](#code_mix_responsibility)
  - [Code Mix Interaction](#code_mix_interaction)
  - [Code Mix Key Functions](#code_mix_key_functions)
  - [Code Mix Assumptions](#code_mix_assumptions)
  - [Code Mix Usage](#code_mix_usage)

- üõ†Ô∏è **Core Functions & Utilities**
  - [Core Functions](#core_functions)
  - [Compressor Overview](#compressor_overview)
  - [Postprocess Module](#postprocess-module)
  - [Split Data](#split-data)
  - [Write Sync](#write-sync)
  - [Write Async](#write-async)
  - [Gen Sync](#gen-sync)
  - [Gen Async](#gen-async)
  - [Logger](#logger)
  - [Progress](#progress)
  - [Metadata](#metadata)
  - [Dependencies](#dependencies)
  - [Buildsystem](#buildsystem)

- üìÑ **Assumptions & Limits**
  - [Assumptions and Limits](#assumptions_and_limits)
  - [Assumptions](#assumptions)
[2026-01-25 14:36:28.879509] [INFO] Module IntroLinks generated its part of the documentation.
[2026-01-25 14:36:28.879532] [INFO] Module Output: ## Executive Navigation Tree
- üìÇ **Installation & Setup**
  - [Install Workflow](#install_workflow')
  - [Autodoc Config Guide](#autodocconfig.yml_guide)
  - [Package Initialization](#package-initialization)
  - [Config Reader](#config-reader)
  - [Run File](#run-file)
  - [Prompt Templates](#prompt-templates)
  - [Environment Setup](#environment-setup)

- ‚öôÔ∏è **Manager & Orchestration**
  - [Using Manager Class](#Using_Manager_class')
  - [Doc Factory Orchestrator](#doc_factory_orchestrator)
  - [Manager Overview](#manager_overview)
  - [Manager State](#manager_state)
  - [Manager Methods](#manager_methods)
  - [Interaction Flow](#interaction_flow)

- ü§ñ **Model Layer**
  - [Model Exception](#model-exception)
  - [Model Base](#model-base)
  - [GPT Model](#gpt-model)

- üß© **Code Mix**
  - [Code Mix Overview](#code_mix_overview)
  - [Code Mix Responsibility](#code_mix_responsibility)
  - [Code Mix Interaction](#code_mix_interaction)
  - [Code Mix Key Functions](#code_mix_key_functions)
  - [Code Mix Assumptions](#code_mix_assumptions)
  - [Code Mix Usage](#code_mix_usage)

- üõ†Ô∏è **Core Functions & Utilities**
  - [Core Functions](#core_functions)
  - [Compressor Overview](#compressor_overview)
  - [Postprocess Module](#postprocess-module)
  - [Split Data](#split-data)
  - [Write Sync](#write-sync)
  - [Write Async](#write-async)
  - [Gen Sync](#gen-sync)
  - [Gen Async](#gen-async)
  - [Logger](#logger)
  - [Progress](#progress)
  - [Metadata](#metadata)
  - [Dependencies](#dependencies)
  - [Buildsystem](#buildsystem)

- üìÑ **Assumptions & Limits**
  - [Assumptions and Limits](#assumptions_and_limits)
  - [Assumptions](#assumptions)
[2026-01-25 14:36:28.879589] [INFO] Factory documentation generation completed.

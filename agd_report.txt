[2026-01-27 14:18:34.915404] [INFO] Starting code mix generation...
[2026-01-27 14:18:34.920882] [INFO] Ignored: /home/runner/work/ADG/ADG/.auto_doc_cache
[2026-01-27 14:18:34.921148] [INFO] Ignored: /home/runner/work/ADG/ADG/.auto_doc_cache/code_mix.txt
[2026-01-27 14:18:34.921369] [INFO] Ignored: /home/runner/work/ADG/ADG/.auto_doc_cache/report.txt
[2026-01-27 14:18:34.921743] [INFO] Ignored: /home/runner/work/ADG/ADG/.git
[2026-01-27 14:18:34.921977] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/FETCH_HEAD
[2026-01-27 14:18:34.922218] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/HEAD
[2026-01-27 14:18:34.922447] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/config
[2026-01-27 14:18:34.922736] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/config.worktree
[2026-01-27 14:18:34.922963] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/description
[2026-01-27 14:18:34.923239] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks
[2026-01-27 14:18:34.923487] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/applypatch-msg.sample
[2026-01-27 14:18:34.923723] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/commit-msg.sample
[2026-01-27 14:18:34.923954] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/fsmonitor-watchman.sample
[2026-01-27 14:18:34.924205] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/post-update.sample
[2026-01-27 14:18:34.924445] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-applypatch.sample
[2026-01-27 14:18:34.924674] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-commit.sample
[2026-01-27 14:18:34.924907] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-merge-commit.sample
[2026-01-27 14:18:34.925155] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-push.sample
[2026-01-27 14:18:34.925391] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-rebase.sample
[2026-01-27 14:18:34.925621] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/pre-receive.sample
[2026-01-27 14:18:34.925850] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/prepare-commit-msg.sample
[2026-01-27 14:18:34.926077] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/push-to-checkout.sample
[2026-01-27 14:18:34.926324] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/sendemail-validate.sample
[2026-01-27 14:18:34.926559] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/hooks/update.sample
[2026-01-27 14:18:34.926774] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/index
[2026-01-27 14:18:34.926991] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/info
[2026-01-27 14:18:34.927265] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/info/exclude
[2026-01-27 14:18:34.927490] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs
[2026-01-27 14:18:34.927720] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/HEAD
[2026-01-27 14:18:34.927953] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs
[2026-01-27 14:18:34.928219] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/heads
[2026-01-27 14:18:34.928485] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/heads/main
[2026-01-27 14:18:34.928729] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/remotes
[2026-01-27 14:18:34.928985] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/remotes/origin
[2026-01-27 14:18:34.929288] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/logs/refs/remotes/origin/main
[2026-01-27 14:18:34.929512] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects
[2026-01-27 14:18:34.929755] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/00
[2026-01-27 14:18:34.930001] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/00/3ed33f03ba24320be674acc37e2f484440960d
[2026-01-27 14:18:34.930244] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/08
[2026-01-27 14:18:34.930494] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/08/3bb9598f2f3dee44e7b6d3358c592538e0c708
[2026-01-27 14:18:34.930722] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/0a
[2026-01-27 14:18:34.930964] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/0a/09690df2c5d6837be4cd8567cd06a1f81dabfd
[2026-01-27 14:18:34.931246] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/0e
[2026-01-27 14:18:34.931504] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/0e/edce493b9e252cc52330a0f8dea4077a65a9d4
[2026-01-27 14:18:34.931731] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/14
[2026-01-27 14:18:34.931973] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/14/e39eb137452b56ad461babda21cf35edc78b42
[2026-01-27 14:18:34.932218] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/16
[2026-01-27 14:18:34.932468] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/16/c6bd78d24630228f2f308279feee5992b421d5
[2026-01-27 14:18:34.932694] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/26
[2026-01-27 14:18:34.932938] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/26/232583984feff25da44d47972f553bd00e4daa
[2026-01-27 14:18:34.933184] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/31
[2026-01-27 14:18:34.933435] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/31/947cbc29165996e70620f06feee333ab5b0a89
[2026-01-27 14:18:34.933662] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/32
[2026-01-27 14:18:34.933906] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/32/2d32b369aa997e0c5a8405e153cac81bc1bdaa
[2026-01-27 14:18:34.934147] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/42
[2026-01-27 14:18:34.934394] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/42/47e86e5d467366df91d576f9eb0961c68ae7b1
[2026-01-27 14:18:34.934639] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/42/83aae1fbab42754bbc5ce3d31a6a2c350ce52a
[2026-01-27 14:18:34.934867] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/45
[2026-01-27 14:18:34.935110] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/45/abf18a355c0af847a695af065f76d937f9c756
[2026-01-27 14:18:34.935377] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/45/c8c97b68d9edc9608f17662ae5a56937db4722
[2026-01-27 14:18:34.935606] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/49
[2026-01-27 14:18:34.935861] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/49/d37ba0c0997792ba28ad262f900d4415894f8d
[2026-01-27 14:18:34.936091] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/50
[2026-01-27 14:18:34.936358] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/50/de1f7cf4fe170128039ffa6d0cc6f3f6d6cc33
[2026-01-27 14:18:34.936596] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/62
[2026-01-27 14:18:34.936838] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/62/18e73336f4ccb562643cd4202a628d14cf784f
[2026-01-27 14:18:34.937081] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/62/5a3814f817aa0548631c3cb646b9faf7546f9e
[2026-01-27 14:18:34.937324] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6a
[2026-01-27 14:18:34.937570] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6a/2dd502eb649bb8e804238af60031db553c13ea
[2026-01-27 14:18:34.937811] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6a/4ee663203935f06c6f32d4d056257d4f4478a7
[2026-01-27 14:18:34.938037] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6d
[2026-01-27 14:18:34.938295] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6d/473cbc7d7d4cfe0662e3613a5b9ef46a450ed0
[2026-01-27 14:18:34.938529] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6f
[2026-01-27 14:18:34.938771] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/6f/130bfccfc73d6fe93fe33cd3772d640379f909
[2026-01-27 14:18:34.938998] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/73
[2026-01-27 14:18:34.939259] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/73/bd29fbc1c26bcda988b9709575c5e168dd226b
[2026-01-27 14:18:34.939493] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/78
[2026-01-27 14:18:34.939735] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/78/21493f3152cf35055f27eca355fcdd95961aa3
[2026-01-27 14:18:34.939963] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/79
[2026-01-27 14:18:34.940231] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/79/870991ab5c9685e1f4fb6fa8d59be43d19b529
[2026-01-27 14:18:34.940473] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7b
[2026-01-27 14:18:34.940716] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7b/138d5680d8295e6880bb150c91eb4aead964f3
[2026-01-27 14:18:34.940943] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7e
[2026-01-27 14:18:34.941203] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/7e/f15dd6d3c3dd489f000b4267a54b0d72c0d559
[2026-01-27 14:18:34.941437] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/81
[2026-01-27 14:18:34.941680] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/81/becbf7a8a1c3473195d0f85266ab25f85809ce
[2026-01-27 14:18:34.941909] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/82
[2026-01-27 14:18:34.942170] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/82/c9f53241e1355b8bf4f866d963626f1a64295b
[2026-01-27 14:18:34.942402] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/84
[2026-01-27 14:18:34.942647] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/84/00c2b5c8734df41836d301824574b989b0ec41
[2026-01-27 14:18:34.942875] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8a
[2026-01-27 14:18:34.943135] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/8a/b47c5bc8110cef05224baba95761fd822aa22a
[2026-01-27 14:18:34.943373] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a1
[2026-01-27 14:18:34.943617] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a1/74987040ef16bb4a027c8cad0e70eee18daf2d
[2026-01-27 14:18:34.943844] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a8
[2026-01-27 14:18:34.944086] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a8/476127e3f807c0aa99fa808560ec2e5584dff2
[2026-01-27 14:18:34.944332] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a9
[2026-01-27 14:18:34.944580] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/a9/6cb8c74b1c06329462b4168d97a8ef030014fb
[2026-01-27 14:18:34.944815] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/b2
[2026-01-27 14:18:34.945066] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/b2/298285b2f5cca6c76b4ecf8b328906cfa69867
[2026-01-27 14:18:34.945334] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/b6
[2026-01-27 14:18:34.945594] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/b6/d812aceb9c792cb2019b9e244e1dbaa4ad74a1
[2026-01-27 14:18:34.945838] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ca
[2026-01-27 14:18:34.946080] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ca/3f28b6a4a95a7176cdf2d78622ceb1151e186d
[2026-01-27 14:18:34.946327] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d4
[2026-01-27 14:18:34.946573] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d4/3b05b238cd21eac8ee676f28eee30d097e970f
[2026-01-27 14:18:34.946813] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d4/c050e6555c1575b188547f6f4267d094891f83
[2026-01-27 14:18:34.947038] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d8
[2026-01-27 14:18:34.947294] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d8/c2d752b4fb51861cfab60d91731af346511faf
[2026-01-27 14:18:34.947526] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d9
[2026-01-27 14:18:34.947767] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/d9/c5e7426b4d3d800f9cca0b469d6f0c60fe27b7
[2026-01-27 14:18:34.947992] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/db
[2026-01-27 14:18:34.948249] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/db/80ec4419abf97b42fd51f286a6dd599d8d77ce
[2026-01-27 14:18:34.948480] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e6
[2026-01-27 14:18:34.948721] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391
[2026-01-27 14:18:34.948946] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e7
[2026-01-27 14:18:34.949204] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/e7/746f47e059234c8b861cf312d97132a810e145
[2026-01-27 14:18:34.949436] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/eb
[2026-01-27 14:18:34.949678] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/eb/f82bee3666806324f04b0d8bc07c6bf5e75d97
[2026-01-27 14:18:34.949914] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ef
[2026-01-27 14:18:34.950175] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/ef/9fb37763d620555f3e448337bcdb1ac580b349
[2026-01-27 14:18:34.950409] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fb
[2026-01-27 14:18:34.950652] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fb/8b56ad03d86c61f46cb2c93a162215b23ce129
[2026-01-27 14:18:34.950880] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fd
[2026-01-27 14:18:34.951143] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/fd/2116ecd9915ae256df74ec7338b0b079f5634e
[2026-01-27 14:18:34.951379] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/info
[2026-01-27 14:18:34.951607] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/objects/pack
[2026-01-27 14:18:34.951821] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs
[2026-01-27 14:18:34.952050] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/heads
[2026-01-27 14:18:34.952309] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/heads/main
[2026-01-27 14:18:34.952542] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/remotes
[2026-01-27 14:18:34.952784] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/remotes/origin
[2026-01-27 14:18:34.953052] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/remotes/origin/main
[2026-01-27 14:18:34.953303] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/refs/tags
[2026-01-27 14:18:34.953525] [INFO] Ignored: /home/runner/work/ADG/ADG/.git/shallow
[2026-01-27 14:18:34.955731] [INFO] Ignored: /home/runner/work/ADG/ADG/.gitignore
[2026-01-27 14:18:34.955967] [INFO] Ignored: /home/runner/work/ADG/ADG/README.md
[2026-01-27 14:18:34.957258] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/__pycache__
[2026-01-27 14:18:34.957487] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/__pycache__/__init__.cpython-312.pyc
[2026-01-27 14:18:34.957705] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/__pycache__/manage.cpython-312.pyc
[2026-01-27 14:18:34.958235] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/auto_runner/__pycache__
[2026-01-27 14:18:34.958475] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/auto_runner/__pycache__/config_reader.cpython-312.pyc
[2026-01-27 14:18:34.958708] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/auto_runner/__pycache__/run_file.cpython-312.pyc
[2026-01-27 14:18:34.959854] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/config/__pycache__
[2026-01-27 14:18:34.960102] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/config/__pycache__/config.cpython-312.pyc
[2026-01-27 14:18:34.961258] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/__pycache__
[2026-01-27 14:18:34.961499] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/__pycache__/__init__.cpython-312.pyc
[2026-01-27 14:18:34.961734] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/__pycache__/exceptions.cpython-312.pyc
[2026-01-27 14:18:34.962294] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/config/__pycache__
[2026-01-27 14:18:34.962546] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/config/__pycache__/config.cpython-312.pyc
[2026-01-27 14:18:34.963749] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/models/__pycache__
[2026-01-27 14:18:34.964001] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/models/__pycache__/gpt_model.cpython-312.pyc
[2026-01-27 14:18:34.964263] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/engine/models/__pycache__/model.cpython-312.pyc
[2026-01-27 14:18:34.965754] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/__pycache__
[2026-01-27 14:18:34.965995] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/__pycache__/__init__.cpython-312.pyc
[2026-01-27 14:18:34.966248] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/__pycache__/base_factory.cpython-312.pyc
[2026-01-27 14:18:34.967132] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/modules/__pycache__
[2026-01-27 14:18:34.967389] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/modules/__pycache__/general_modules.cpython-312.pyc
[2026-01-27 14:18:34.967638] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/factory/modules/__pycache__/intro.cpython-312.pyc
[2026-01-27 14:18:34.969159] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/postprocessor/__pycache__
[2026-01-27 14:18:34.969402] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/postprocessor/__pycache__/custom_intro.cpython-312.pyc
[2026-01-27 14:18:34.969635] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/postprocessor/__pycache__/sorting.cpython-312.pyc
[2026-01-27 14:18:34.970767] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__
[2026-01-27 14:18:34.971005] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/code_mix.cpython-312.pyc
[2026-01-27 14:18:34.971252] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/compressor.cpython-312.pyc
[2026-01-27 14:18:34.971488] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/settings.cpython-312.pyc
[2026-01-27 14:18:34.971717] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/preprocessor/__pycache__/spliter.cpython-312.pyc
[2026-01-27 14:18:34.973763] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__
[2026-01-27 14:18:34.973999] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__/__init__.cpython-312.pyc
[2026-01-27 14:18:34.974247] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__/logging.cpython-312.pyc
[2026-01-27 14:18:34.974482] [INFO] Ignored: /home/runner/work/ADG/ADG/autodocgenerator/ui/__pycache__/progress_base.cpython-312.pyc
[2026-01-27 14:18:35.013828] [INFO] Code mix generation completed.
[2026-01-27 14:18:35.014725] [INFO] Starting synchronous documentation generation by parts...
[2026-01-27 14:18:35.015028] [INFO] Starting data splitting...
[2026-01-27 14:18:35.015108] [INFO] Data split into 11 parts based on max symbols 5000.
[2026-01-27 14:18:35.015180] [INFO] Starting documentation generation by parts...
[2026-01-27 14:18:35.015229] [INFO] Generating documentation for a part...
[2026-01-27 14:18:35.015278] [INFO] Generating answer...
[2026-01-27 14:18:37.070032] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:18:37.070097] [INFO] Answer: <a name="config-reader-responsibility"></a>
## Config‑Reader Responsibility
`read_config` loads a user‑supplied YAML string, transforms it into a strongly‑typed `Config` object and a list of custom documentation modules. It centralises all project‑wide settings (language, name, ignored files, additional info) and prepares module descriptors for the documentation engine.

<a name="config-reader-interaction"></a>
## Interaction with Factory Modules
The function imports `CustomModule` and `CustomModuleWithOutContext` from `autodocgenerator.factory.modules.general_modules`.  
* When a description line starts with “%” it is wrapped in `CustomModuleWithOutContext` (context‑free).  
* Otherwise the line is turned into a regular `CustomModule`.  
These objects are later consumed by the rendering pipeline to inject user‑defined sections into the generated markdown.

<a name="config-reader-technical-details"></a>
## Technical Details and Logic Flow
1. **YAML parsing** – `yaml.safe_load` converts the raw string to a Python dict.  
2. **Base `Config` creation** – a fresh `Config` instance is instantiated.  
3. **Core fields extraction** – `ignore_files`, `language`, `project_name`, and `project_additional_info` are read with sensible defaults.  
4. **Project settings handling** – a `ProjectConfigSettings` object (`pcs`) loads the `project_settings` mapping via `pcs.load_settings`.  
5. **Fluent configuration** – `config.set_language(...).set_project_name(...).set_pcs(pcs)` chains setters for readability.  
6. **Iterative population** – loops add ignore patterns and additional info via `config.add_ignore_file` and `config.add_project_additional_info`.  
7. **Custom description conversion** – each entry in `custom_descriptions` is examined; the leading character decides which module class to instantiate.  

<a name="config-reader-data-flow"></a>
## Input, Output, and Side Effects
| Aspect | Description |
|--------|-------------|
| **Input** | `file_data: str` – raw YAML content supplied by `autodocconfig.yml`. |
| **Output** | `Tuple[Config, List[CustomModule]]` – a ready‑to‑use configuration object and a collection of module descriptors. |
| **Side effects** | None; the function is pure aside from importing modules. It assumes the YAML schema contains the keys used above; missing optional keys fall back to defaults. |
| **Assumptions** | The YAML is well‑formed, custom description strings are non‑empty, and the imported module classes conform to the expected constructor signatures. |
| **Error handling** | Propagation of `yaml.YAMLError` or any constructor exception to the caller; the runner catches and logs them via the global logger. |

Together, `read_config` supplies the documentation engine with a complete, validated configuration snapshot and the custom module list required for the subsequent generation steps.
[2026-01-27 14:18:37.070175] [INFO] Documentation for part generated. total length: 2854
[2026-01-27 14:18:37.070202] [INFO] Part Documentation: <a name="config-reader-responsibility"></a>
## Config‑Reader Responsibility
`read_config` loads a user‑supplied YAML string, transforms it into a strongly‑typed `Config` object and a list of custom documentation modules. It centralises all project‑wide settings (language, name, ignored files, additional info) and prepares module descriptors for the documentation engine.

<a name="config-reader-interaction"></a>
## Interaction with Factory Modules
The function imports `CustomModule` and `CustomModuleWithOutContext` from `autodocgenerator.factory.modules.general_modules`.  
* When a description line starts with “%” it is wrapped in `CustomModuleWithOutContext` (context‑free).  
* Otherwise the line is turned into a regular `CustomModule`.  
These objects are later consumed by the rendering pipeline to inject user‑defined sections into the generated markdown.

<a name="config-reader-technical-details"></a>
## Technical Details and Logic Flow
1. **YAML parsing** – `yaml.safe_load` converts the raw string to a Python dict.  
2. **Base `Config` creation** – a fresh `Config` instance is instantiated.  
3. **Core fields extraction** – `ignore_files`, `language`, `project_name`, and `project_additional_info` are read with sensible defaults.  
4. **Project settings handling** – a `ProjectConfigSettings` object (`pcs`) loads the `project_settings` mapping via `pcs.load_settings`.  
5. **Fluent configuration** – `config.set_language(...).set_project_name(...).set_pcs(pcs)` chains setters for readability.  
6. **Iterative population** – loops add ignore patterns and additional info via `config.add_ignore_file` and `config.add_project_additional_info`.  
7. **Custom description conversion** – each entry in `custom_descriptions` is examined; the leading character decides which module class to instantiate.  

<a name="config-reader-data-flow"></a>
## Input, Output, and Side Effects
| Aspect | Description |
|--------|-------------|
| **Input** | `file_data: str` – raw YAML content supplied by `autodocconfig.yml`. |
| **Output** | `Tuple[Config, List[CustomModule]]` – a ready‑to‑use configuration object and a collection of module descriptors. |
| **Side effects** | None; the function is pure aside from importing modules. It assumes the YAML schema contains the keys used above; missing optional keys fall back to defaults. |
| **Assumptions** | The YAML is well‑formed, custom description strings are non‑empty, and the imported module classes conform to the expected constructor signatures. |
| **Error handling** | Propagation of `yaml.YAMLError` or any constructor exception to the caller; the runner catches and logs them via the global logger. |

Together, `read_config` supplies the documentation engine with a complete, validated configuration snapshot and the custom module list required for the subsequent generation steps.
[2026-01-27 14:18:37.070268] [INFO] Generating documentation for a part...
[2026-01-27 14:18:37.070297] [INFO] Generating answer...
[2026-01-27 14:18:38.848397] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:18:38.848460] [INFO] Answer: <a name="runfile-execution"></a>  
## Run‑file execution flow  

`gen_doc` orchestrates the full documentation pipeline. It instantiates a **synchronous** (`GPTModel`) and an **asynchronous** (`AsyncGPTModel`) LLM client using the global `API_KEY`. These models are injected into a `Manager` together with:  

* `project_path` – root of the source tree.  
* `config` – a `Config` instance (see *Configuration object construction*).  
* `ConsoleGtiHubProgress` – CLI progress visualiser.  

The manager then performs the ordered steps:  

1. `generate_code_file()` – parses the project and creates an internal code representation.  
2. `generete_doc_parts(max_symbols=5000)` – produces raw documentation fragments, capped at 5 k symbols each.  
3. `factory_generate_doc(DocFactory(*custom_modules))` – runs user‑supplied modules (e.g., custom extractors).  
4. `order_doc()` – re‑orders fragments according to logical sections.  
5. `factory_generate_doc(DocFactory(IntroLinks()))` – adds the autogenerated introductory link block.  
6. `clear_cache()` – removes temporary artefacts.  

Finally `read_file_by_file_key("output_doc")` returns the assembled markdown document.

<a name="configuration-object-construction"></a>  
## Configuration object construction  

`Config` aggregates global settings:  

* **ignore_files** – glob patterns excluded from scanning.  
* **language** – default `"en"`.  
* **project_name** & **project_additional_info** – descriptive metadata passed to `ProjectSettings`.  
* **pcs** – a `ProjectConfigSettings` container (log flags).  

Convenient setters (`set_language`, `set_project_name`, `add_ignore_file`, …) enable fluent building.  
`get_project_settings()` converts the stored metadata into a `ProjectSettings` instance used later by the `Manager`.

<a name="data-flow-and-side‑effects"></a>  
## Data flow and side‑effects  

* **Input** – `project_path` (string), a parsed `Config` object, and a list of instantiated custom module objects.  
* **Processing** – manager writes intermediate code files to the hidden `.auto_doc_cache` directory, updates the console progress bar, and may create temporary log files if `ProjectConfigSettings.save_logs` is true.  
* **Output** – a single string containing the complete documentation (`output_doc`).  
* **Assumptions** – a valid `API_KEY` is available, custom modules implement the `BaseModule` interface expected by `DocFactory`, and the project directory is readable.  

These steps provide the validated configuration snapshot and the custom module list required for the downstream generation engine.
[2026-01-27 14:18:38.848498] [INFO] Documentation for part generated. total length: 2591
[2026-01-27 14:18:38.848521] [INFO] Part Documentation: <a name="runfile-execution"></a>  
## Run‑file execution flow  

`gen_doc` orchestrates the full documentation pipeline. It instantiates a **synchronous** (`GPTModel`) and an **asynchronous** (`AsyncGPTModel`) LLM client using the global `API_KEY`. These models are injected into a `Manager` together with:  

* `project_path` – root of the source tree.  
* `config` – a `Config` instance (see *Configuration object construction*).  
* `ConsoleGtiHubProgress` – CLI progress visualiser.  

The manager then performs the ordered steps:  

1. `generate_code_file()` – parses the project and creates an internal code representation.  
2. `generete_doc_parts(max_symbols=5000)` – produces raw documentation fragments, capped at 5 k symbols each.  
3. `factory_generate_doc(DocFactory(*custom_modules))` – runs user‑supplied modules (e.g., custom extractors).  
4. `order_doc()` – re‑orders fragments according to logical sections.  
5. `factory_generate_doc(DocFactory(IntroLinks()))` – adds the autogenerated introductory link block.  
6. `clear_cache()` – removes temporary artefacts.  

Finally `read_file_by_file_key("output_doc")` returns the assembled markdown document.

<a name="configuration-object-construction"></a>  
## Configuration object construction  

`Config` aggregates global settings:  

* **ignore_files** – glob patterns excluded from scanning.  
* **language** – default `"en"`.  
* **project_name** & **project_additional_info** – descriptive metadata passed to `ProjectSettings`.  
* **pcs** – a `ProjectConfigSettings` container (log flags).  

Convenient setters (`set_language`, `set_project_name`, `add_ignore_file`, …) enable fluent building.  
`get_project_settings()` converts the stored metadata into a `ProjectSettings` instance used later by the `Manager`.

<a name="data-flow-and-side‑effects"></a>  
## Data flow and side‑effects  

* **Input** – `project_path` (string), a parsed `Config` object, and a list of instantiated custom module objects.  
* **Processing** – manager writes intermediate code files to the hidden `.auto_doc_cache` directory, updates the console progress bar, and may create temporary log files if `ProjectConfigSettings.save_logs` is true.  
* **Output** – a single string containing the complete documentation (`output_doc`).  
* **Assumptions** – a valid `API_KEY` is available, custom modules implement the `BaseModule` interface expected by `DocFactory`, and the project directory is readable.  

These steps provide the validated configuration snapshot and the custom module list required for the downstream generation engine.
[2026-01-27 14:18:38.848599] [INFO] Generating documentation for a part...
[2026-01-27 14:18:38.848638] [INFO] Generating answer...
[2026-01-27 14:18:40.705482] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:18:40.705545] [INFO] Answer: <a name="parentmodel-shared-configuration"></a>  
## ParentModel – Shared Model Configuration  

`ParentModel` centralises authentication, history handling, and model‑selection logic. It receives an `api_key`, a `History` instance, and a `use_random` flag. The constructor shuffles the global `MODELS_NAME` list (when randomised) and stores the ordered copy in `self.regen_models_name`. Index tracking (`self.current_model_index`) enables round‑robin fallback when a model fails.

---

<a name="history-conversation-state"></a>  
## History – Conversation State Management  

`History` builds the message array sent to the LLM. It auto‑injects the system prompt (`BASE_SYSTEM_TEXT`) and provides `add_to_history(role, content)` to append user or assistant entries. The `self.history` list is consumed directly by the model classes.

---

<a name="gptmodel-synchronous-generation"></a>  
## GPTModel – Synchronous Answer Generation  

Inherits from `Model` (which extends `ParentModel`).  
* **Responsibility** – Wraps the Groq synchronous client (`Groq`) to produce a completion.  
* **Interaction** – Uses `self.client.chat.completions.create(messages, model)`; falls back through `self.regen_models_name` on exception, logging each attempt via `BaseLogger`.  
* **Data Flow** –  
  1. Chooses `messages` from `self.history.history` or a raw `prompt`.  
  2. Calls the Groq API, extracts `choices[0].message.content`.  
  3. Logs generation steps and returns the answer.

---

<a name="asyncgptmodel-asynchronous-generation"></a>  
## AsyncGPTModel – Asynchronous Answer Generation  

Mirrors `GPTModel` but operates with `AsyncGroq`.  
* **Responsibility** – Provides `async generate_answer` for non‑blocking calls.  
* **Interaction** – Awaits `self.client.chat.completions.create`. On failure, logs a warning and advances `self.current_model_index`.  
* **Data Flow** – Same as the synchronous variant, but returns a coroutine result and logs at level 2 for the final answer.

Both model classes rely on the shared fallback mechanism defined in `ParentModel`, ensuring resilience when a specific model endpoint is unavailable.
[2026-01-27 14:18:40.705599] [INFO] Documentation for part generated. total length: 2130
[2026-01-27 14:18:40.705630] [INFO] Part Documentation: <a name="parentmodel-shared-configuration"></a>  
## ParentModel – Shared Model Configuration  

`ParentModel` centralises authentication, history handling, and model‑selection logic. It receives an `api_key`, a `History` instance, and a `use_random` flag. The constructor shuffles the global `MODELS_NAME` list (when randomised) and stores the ordered copy in `self.regen_models_name`. Index tracking (`self.current_model_index`) enables round‑robin fallback when a model fails.

---

<a name="history-conversation-state"></a>  
## History – Conversation State Management  

`History` builds the message array sent to the LLM. It auto‑injects the system prompt (`BASE_SYSTEM_TEXT`) and provides `add_to_history(role, content)` to append user or assistant entries. The `self.history` list is consumed directly by the model classes.

---

<a name="gptmodel-synchronous-generation"></a>  
## GPTModel – Synchronous Answer Generation  

Inherits from `Model` (which extends `ParentModel`).  
* **Responsibility** – Wraps the Groq synchronous client (`Groq`) to produce a completion.  
* **Interaction** – Uses `self.client.chat.completions.create(messages, model)`; falls back through `self.regen_models_name` on exception, logging each attempt via `BaseLogger`.  
* **Data Flow** –  
  1. Chooses `messages` from `self.history.history` or a raw `prompt`.  
  2. Calls the Groq API, extracts `choices[0].message.content`.  
  3. Logs generation steps and returns the answer.

---

<a name="asyncgptmodel-asynchronous-generation"></a>  
## AsyncGPTModel – Asynchronous Answer Generation  

Mirrors `GPTModel` but operates with `AsyncGroq`.  
* **Responsibility** – Provides `async generate_answer` for non‑blocking calls.  
* **Interaction** – Awaits `self.client.chat.completions.create`. On failure, logs a warning and advances `self.current_model_index`.  
* **Data Flow** – Same as the synchronous variant, but returns a coroutine result and logs at level 2 for the final answer.

Both model classes rely on the shared fallback mechanism defined in `ParentModel`, ensuring resilience when a specific model endpoint is unavailable.
[2026-01-27 14:18:40.705700] [INFO] Generating documentation for a part...
[2026-01-27 14:18:40.705737] [INFO] Generating answer...
[2026-01-27 14:18:42.867109] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:18:42.867188] [INFO] Answer: <a name="docfactory-orchestrating-documentation-generation"></a>  
## DocFactory – Orchestrating Documentation Generation  

* **Responsibility** – Accepts an ordered list of `BaseModule` instances and sequentially invokes each to produce a composite documentation string.  
* **Interaction** –  
  * Receives a `Model` (or `AsyncModel`) and a `BaseProgress` UI object from the caller.  
  * Calls `module.generate(info, model)` for every injected module.  
  * Emits `InfoLog` entries via the shared `BaseLogger`, forwarding the module name and its raw output (level 2).  
* **Technical Details** –  
  * Stores modules in `self.modules: list[BaseModule]`.  
  * Initializes a simple logger (`BaseLogger()`).  
  * Uses `progress.create_new_subtask` to allocate a sub‑task sized to the module count, `progress.update_task` after each module, and `progress.remove_subtask` on completion.  
* **Data Flow** –  
  1. Input: `info` dict, `model` instance, `progress` UI.  
  2. For each module: `module.generate` returns a string → concatenated to `output`.  
  3. Side‑effects: logging, progress bar updates.  
  4. Output: the final assembled documentation string.

<a name="custommodule-context-aware-intro-generation"></a>  
## CustomModule – Context‑Aware Intro Generation  

* **Responsibility** – Generates a custom introductory paragraph that incorporates a user‑provided description and a code‑snippet excerpt.  
* **Interaction** – Calls `split_data` to truncate `info["code_mix"]` to ≤ 5 000 symbols, then forwards the split text, the model, the description, and the target language to `generete_custom_discription`.  
* **Technical Details** –  
  * Inherits from `BaseModule`.  
  * Holds `self.discription` (typo retained from original).  
  * `generate` returns the processed string from the post‑processor.  

* **Data Flow** –  
  1. Input: `info` dict (keys `code_mix`, `language`), `model`.  
  2. Processing: `split_data → generete_custom_discription`.  
  3. Output: formatted intro paragraph.

<a name="custommodulewithoutcontext‑static-intro-generation"></a>  
## CustomModuleWithOutContext – Static Intro Generation  

* **Responsibility** – Produces a description‑only intro, omitting any code context.  
* **Interaction** – Directly invokes `generete_custom_discription_without` with the model, stored description, and language.  
* **Data Flow** – Input: `info["language"]`, `model`; Output: static intro string; No side‑effects beyond the returned text.

<a name="introlinks‑html‑link‑extraction"></a>  
## IntroLinks – HTML Link Extraction for Documentation  

* **Responsibility** – Extracts all HTML links from `info["full_data"]` and synthesizes a brief introductory block about those links.  
* **Interaction** – Uses `get_all_html_links` → `get_links_intro`, passing the model and language.  

<a name="introtext‑global‑context‑introduction"></a>  
## IntroText – Global Context Introduction  

* **Responsibility** – Generates a high‑level introduction based on `info["global_data"]`.  
* **Interaction** – Calls `get_introdaction` with the model and language, returning the result.  
[2026-01-27 14:18:42.867243] [INFO] Documentation for part generated. total length: 3116
[2026-01-27 14:18:42.867269] [INFO] Part Documentation: <a name="docfactory-orchestrating-documentation-generation"></a>  
## DocFactory – Orchestrating Documentation Generation  

* **Responsibility** – Accepts an ordered list of `BaseModule` instances and sequentially invokes each to produce a composite documentation string.  
* **Interaction** –  
  * Receives a `Model` (or `AsyncModel`) and a `BaseProgress` UI object from the caller.  
  * Calls `module.generate(info, model)` for every injected module.  
  * Emits `InfoLog` entries via the shared `BaseLogger`, forwarding the module name and its raw output (level 2).  
* **Technical Details** –  
  * Stores modules in `self.modules: list[BaseModule]`.  
  * Initializes a simple logger (`BaseLogger()`).  
  * Uses `progress.create_new_subtask` to allocate a sub‑task sized to the module count, `progress.update_task` after each module, and `progress.remove_subtask` on completion.  
* **Data Flow** –  
  1. Input: `info` dict, `model` instance, `progress` UI.  
  2. For each module: `module.generate` returns a string → concatenated to `output`.  
  3. Side‑effects: logging, progress bar updates.  
  4. Output: the final assembled documentation string.

<a name="custommodule-context-aware-intro-generation"></a>  
## CustomModule – Context‑Aware Intro Generation  

* **Responsibility** – Generates a custom introductory paragraph that incorporates a user‑provided description and a code‑snippet excerpt.  
* **Interaction** – Calls `split_data` to truncate `info["code_mix"]` to ≤ 5 000 symbols, then forwards the split text, the model, the description, and the target language to `generete_custom_discription`.  
* **Technical Details** –  
  * Inherits from `BaseModule`.  
  * Holds `self.discription` (typo retained from original).  
  * `generate` returns the processed string from the post‑processor.  

* **Data Flow** –  
  1. Input: `info` dict (keys `code_mix`, `language`), `model`.  
  2. Processing: `split_data → generete_custom_discription`.  
  3. Output: formatted intro paragraph.

<a name="custommodulewithoutcontext‑static-intro-generation"></a>  
## CustomModuleWithOutContext – Static Intro Generation  

* **Responsibility** – Produces a description‑only intro, omitting any code context.  
* **Interaction** – Directly invokes `generete_custom_discription_without` with the model, stored description, and language.  
* **Data Flow** – Input: `info["language"]`, `model`; Output: static intro string; No side‑effects beyond the returned text.

<a name="introlinks‑html‑link‑extraction"></a>  
## IntroLinks – HTML Link Extraction for Documentation  

* **Responsibility** – Extracts all HTML links from `info["full_data"]` and synthesizes a brief introductory block about those links.  
* **Interaction** – Uses `get_all_html_links` → `get_links_intro`, passing the model and language.  

<a name="introtext‑global‑context‑introduction"></a>  
## IntroText – Global Context Introduction  

* **Responsibility** – Generates a high‑level introduction based on `info["global_data"]`.  
* **Interaction** – Calls `get_introdaction` with the model and language, returning the result.  
[2026-01-27 14:18:42.867354] [INFO] Generating documentation for a part...
[2026-01-27 14:18:42.867390] [INFO] Generating answer...
[2026-01-27 14:19:01.158901] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:19:01.158956] [INFO] Answer: <a name="manager-orchestration-pipeline"></a>
## Manager – Orchestration of Documentation Pipeline  

* **Responsibility** – Coordinates the end‑to‑end generation of documentation: builds the code‑mix cache, splits large sources, invokes the factory‑based module chain, and orders the final output.  

* **Interaction** –  
  * Accepts a project root, a `Config` object, optional synchronous (`Model`) and asynchronous (`AsyncModel`) language models, and a UI progress handler (`BaseProgress`).  
  * Utilises `BaseLogger` (file‑backed via `FileLoggerTemplate`) for all informational, warning and error logs.  
  * Calls external preprocessors (`split_data`, `gen_doc_parts`, `compress_to_one`), post‑processors (`get_introdaction`, `get_all_html_links`, …) and the injected `DocFactory` to produce the final markdown.  

* **Technical Details** –  
  * Constants `CACHE_FOLDER_NAME` and `FILE_NAMES` define persistent cache locations (`code_mix.txt`, `global_info.md`, `report.txt`, `output_doc.md`).  
  * `__init__` creates the cache folder, configures the logger, and stores references to models and progress UI.  
  * Core methods:  
    - `generate_code_file` → builds a `CodeMix` instance, writes the mixed source to cache, updates progress.  
    - `generete_doc_parts` → streams the cached mix through `gen_doc_parts`, writes the partial markdown, logs progress.  
    - `factory_generate_doc` → loads current output and code mix, assembles an `info` dict, logs the module list, invokes `doc_factory.generate_doc`, prepends the new fragment to the existing document.  
    - `order_doc` → splits the markdown by anchor markers, asks the synchronous model to reorder sections via `get_order`, and rewrites the file.  
    - `clear_cache` → optionally removes the log file based on `config.pcs.save_logs`.  

* **Data Flow** –  
  1. **Input**: project directory path, `Config`, optional models, UI progress object.  
  2. **Processing**:  
     - `CodeMix.build_repo_content` → writes raw source to `code_mix.txt`.  
     - `gen_doc_parts` → consumes the mix, returns a partial markdown string.  
     - `DocFactory.generate_doc` → receives `info` (`language`, `full_data`, `code_mix`), calls each module’s `generate`, aggregates results, updates progress.  
     - `split_text_by_anchors` / `get_order` → reorders sections.  
  3. **Side‑effects**: file system writes (cache files, final `output_doc.md`), logger entries, progress‑bar updates.  
  4. **Output**: a fully assembled `output_doc.md` containing ordered documentation ready for consumption.
[2026-01-27 14:19:01.159000] [INFO] Documentation for part generated. total length: 2561
[2026-01-27 14:19:01.159024] [INFO] Part Documentation: <a name="manager-orchestration-pipeline"></a>
## Manager – Orchestration of Documentation Pipeline  

* **Responsibility** – Coordinates the end‑to‑end generation of documentation: builds the code‑mix cache, splits large sources, invokes the factory‑based module chain, and orders the final output.  

* **Interaction** –  
  * Accepts a project root, a `Config` object, optional synchronous (`Model`) and asynchronous (`AsyncModel`) language models, and a UI progress handler (`BaseProgress`).  
  * Utilises `BaseLogger` (file‑backed via `FileLoggerTemplate`) for all informational, warning and error logs.  
  * Calls external preprocessors (`split_data`, `gen_doc_parts`, `compress_to_one`), post‑processors (`get_introdaction`, `get_all_html_links`, …) and the injected `DocFactory` to produce the final markdown.  

* **Technical Details** –  
  * Constants `CACHE_FOLDER_NAME` and `FILE_NAMES` define persistent cache locations (`code_mix.txt`, `global_info.md`, `report.txt`, `output_doc.md`).  
  * `__init__` creates the cache folder, configures the logger, and stores references to models and progress UI.  
  * Core methods:  
    - `generate_code_file` → builds a `CodeMix` instance, writes the mixed source to cache, updates progress.  
    - `generete_doc_parts` → streams the cached mix through `gen_doc_parts`, writes the partial markdown, logs progress.  
    - `factory_generate_doc` → loads current output and code mix, assembles an `info` dict, logs the module list, invokes `doc_factory.generate_doc`, prepends the new fragment to the existing document.  
    - `order_doc` → splits the markdown by anchor markers, asks the synchronous model to reorder sections via `get_order`, and rewrites the file.  
    - `clear_cache` → optionally removes the log file based on `config.pcs.save_logs`.  

* **Data Flow** –  
  1. **Input**: project directory path, `Config`, optional models, UI progress object.  
  2. **Processing**:  
     - `CodeMix.build_repo_content` → writes raw source to `code_mix.txt`.  
     - `gen_doc_parts` → consumes the mix, returns a partial markdown string.  
     - `DocFactory.generate_doc` → receives `info` (`language`, `full_data`, `code_mix`), calls each module’s `generate`, aggregates results, updates progress.  
     - `split_text_by_anchors` / `get_order` → reorders sections.  
  3. **Side‑effects**: file system writes (cache files, final `output_doc.md`), logger entries, progress‑bar updates.  
  4. **Output**: a fully assembled `output_doc.md` containing ordered documentation ready for consumption.
[2026-01-27 14:19:01.159096] [INFO] Generating documentation for a part...
[2026-01-27 14:19:01.159154] [INFO] Generating answer...
[2026-01-27 14:19:15.180243] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:19:15.180304] [INFO] Answer: **<a name="html-link-extraction"></a>HTML‑Link Extraction**  
The function `get_all_html_links` scans a markdown string for anchor tags of the form `<a name="…"></a>`. It logs the start/end of the operation, builds a list of fragment identifiers prefixed with “#”, and returns that list.  

*Inputs*: `data` – full documentation text.  
*Outputs*: `list[str]` – collected anchors (e.g., `["#section‑overview"]`).  
*Side‑effects*: Writes two `InfoLog` entries via `BaseLogger`.  

---

**<a name="intro-with-links-generation"></a>Intro‑With‑Links Generation**  
`get_links_intro` receives the anchor list, a `Model` implementation, and an optional language code. It composes a three‑message system‑user prompt (language hint, `BASE_INTRODACTION_CREATE_LINKS` template, and the raw link list) and calls `model.get_answer_without_history`.  

*Inputs*: `links` (list of anchors), `model` (concrete `Model`/`GPTModel`), `language`.  
*Outputs*: String containing a generated introductory paragraph that references the provided links.  
*Side‑effects*: Logs start/completion messages and the raw LLM response.  

---

**<a name="basic-introduction-generation"></a>Basic Introduction Generation**  
`get_introdaction` (typo intentional) builds a similar prompt using `BASE_INTRO_CREATE` and the full documentation (`global_data`). It returns the LLM‑generated introductory block.  

*Inputs*: `global_data` (full docs), `model`, `language`.  
*Outputs*: Introduction string.  

---

**<a name="custom-description-loop"></a>Custom Description Loop**  
`generete_custom_discription` iterates over a pre‑split collection of documentation fragments (`splited_data`). For each fragment it sends a detailed system prompt (technical analyst role, context snippet, `BASE_CUSTOM_DISCRIPTIONS`) together with the user‑requested `custom_description`. The loop breaks when the LLM returns a substantive answer (i.e., does not contain “!noinfo” or “No information found”). The final `result` is returned.  

*Inputs*: `splited_data` (iterable of strings), `model`, `custom_description`, `language`.  
*Outputs*: Custom description text for the first fragment that yields valid content.  

---

**<a name="standalone-custom-description"></a>Standalone Custom Description**  
`generete_custom_discription_without` builds a single‑shot prompt that forces the LLM to prepend a strict `<a name="CONTENT_DESCRIPTION"></a>` tag and obey naming constraints (no filenames, extensions, generic terms, or URLs). The result is returned directly.  

*Inputs*: `model`, `custom_description`, `language`.  
*Outputs*: Tagged description string.  

---

**Interaction Summary**  
All functions rely on the shared `BaseLogger` for traceability and on the abstract `Model` interface (concretely `GPTModel`) to invoke the LLM. They do **not** perform I/O beyond logging; the calling post‑processor aggregates their outputs into the final documentation file.
[2026-01-27 14:19:15.180352] [INFO] Documentation for part generated. total length: 2920
[2026-01-27 14:19:15.180376] [INFO] Part Documentation: **<a name="html-link-extraction"></a>HTML‑Link Extraction**  
The function `get_all_html_links` scans a markdown string for anchor tags of the form `<a name="…"></a>`. It logs the start/end of the operation, builds a list of fragment identifiers prefixed with “#”, and returns that list.  

*Inputs*: `data` – full documentation text.  
*Outputs*: `list[str]` – collected anchors (e.g., `["#section‑overview"]`).  
*Side‑effects*: Writes two `InfoLog` entries via `BaseLogger`.  

---

**<a name="intro-with-links-generation"></a>Intro‑With‑Links Generation**  
`get_links_intro` receives the anchor list, a `Model` implementation, and an optional language code. It composes a three‑message system‑user prompt (language hint, `BASE_INTRODACTION_CREATE_LINKS` template, and the raw link list) and calls `model.get_answer_without_history`.  

*Inputs*: `links` (list of anchors), `model` (concrete `Model`/`GPTModel`), `language`.  
*Outputs*: String containing a generated introductory paragraph that references the provided links.  
*Side‑effects*: Logs start/completion messages and the raw LLM response.  

---

**<a name="basic-introduction-generation"></a>Basic Introduction Generation**  
`get_introdaction` (typo intentional) builds a similar prompt using `BASE_INTRO_CREATE` and the full documentation (`global_data`). It returns the LLM‑generated introductory block.  

*Inputs*: `global_data` (full docs), `model`, `language`.  
*Outputs*: Introduction string.  

---

**<a name="custom-description-loop"></a>Custom Description Loop**  
`generete_custom_discription` iterates over a pre‑split collection of documentation fragments (`splited_data`). For each fragment it sends a detailed system prompt (technical analyst role, context snippet, `BASE_CUSTOM_DISCRIPTIONS`) together with the user‑requested `custom_description`. The loop breaks when the LLM returns a substantive answer (i.e., does not contain “!noinfo” or “No information found”). The final `result` is returned.  

*Inputs*: `splited_data` (iterable of strings), `model`, `custom_description`, `language`.  
*Outputs*: Custom description text for the first fragment that yields valid content.  

---

**<a name="standalone-custom-description"></a>Standalone Custom Description**  
`generete_custom_discription_without` builds a single‑shot prompt that forces the LLM to prepend a strict `<a name="CONTENT_DESCRIPTION"></a>` tag and obey naming constraints (no filenames, extensions, generic terms, or URLs). The result is returned directly.  

*Inputs*: `model`, `custom_description`, `language`.  
*Outputs*: Tagged description string.  

---

**Interaction Summary**  
All functions rely on the shared `BaseLogger` for traceability and on the abstract `Model` interface (concretely `GPTModel`) to invoke the LLM. They do **not** perform I/O beyond logging; the calling post‑processor aggregates their outputs into the final documentation file.
[2026-01-27 14:19:15.180446] [INFO] Generating documentation for a part...
[2026-01-27 14:19:15.180473] [INFO] Generating answer...
[2026-01-27 14:19:29.337708] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:19:29.337767] [INFO] Answer: <a name="anchor-extraction-routine"></a>  
## Anchor Extraction Routine (sorting.py)

**Responsibility** – Detect HTML‑style anchors at the start of each markdown chunk and build a mapping `anchor → chunk`.  

**Interactions** – Consumes raw README text, feeds the resulting dictionary to `get_order`. Relies on `BaseLogger` for diagnostic output.  

**Technical Details** – `extract_links_from_start` uses a regex `^<a name=["']?(.*?)["']?</a>` to capture names longer than five characters, prefixing them with `#`. `split_text_by_anchors` splits the document on a look‑ahead pattern, trims empties, validates a one‑to‑one link‑chunk relationship, and returns `{anchor: chunk}` or `None`.  

**Data Flow** – Input: `text` (str). Output: `dict[str, str]` where keys are `#anchor`. Side‑effects: logging of chunk names and content; early exit if counts mismatch.  

<a name="semantic-ordering-workflow"></a>  
## Semantic Ordering Workflow (sorting.py)

**Responsibility** – Request an LLM to reorder section titles semantically and concatenate the associated chunks in that order.  

**Interactions** – Receives the anchor‑chunk map, sends a prompt to `Model.get_answer_without_history`, then assembles `order_output`.  

**Technical Details** – Constructs a user‑role prompt containing the list of titles, parses the comma‑separated LLM response, logs each step, and iterates over the ordered titles to build the final markdown string.  

**Data Flow** – Input: `Model` instance, `chanks` dict. Output: ordered markdown string. Side‑effects: extensive logging.  

<a name="repository-mix-builder"></a>  
## Repository Mix Builder (code_mix.py)

**Responsibility** – Walk a repository, emit a hierarchical file‑tree, and concatenate file contents into a single “repo mix” artifact while respecting ignore patterns.  

**Interactions** – Utilises `Path.rglob` for traversal, `should_ignore` for pattern checks, and `BaseLogger` for progress messages.  

**Technical Details** – `should_ignore` matches a path against user‑supplied glob patterns across the full relative path, basename, and individual parts. `build_repo_content` writes a tree view, inserts a delimiter, then writes each non‑ignored file wrapped in `<file path="...">` tags, handling read errors gracefully.  

**Data Flow** – Input: `root_dir` (Path), `ignore_patterns` (list). Output: `repomix-output.txt` (text file). Side‑effects: file creation, console logs, error annotations inside the output file.  
[2026-01-27 14:19:29.337802] [INFO] Documentation for part generated. total length: 2472
[2026-01-27 14:19:29.337826] [INFO] Part Documentation: <a name="anchor-extraction-routine"></a>  
## Anchor Extraction Routine (sorting.py)

**Responsibility** – Detect HTML‑style anchors at the start of each markdown chunk and build a mapping `anchor → chunk`.  

**Interactions** – Consumes raw README text, feeds the resulting dictionary to `get_order`. Relies on `BaseLogger` for diagnostic output.  

**Technical Details** – `extract_links_from_start` uses a regex `^<a name=["']?(.*?)["']?</a>` to capture names longer than five characters, prefixing them with `#`. `split_text_by_anchors` splits the document on a look‑ahead pattern, trims empties, validates a one‑to‑one link‑chunk relationship, and returns `{anchor: chunk}` or `None`.  

**Data Flow** – Input: `text` (str). Output: `dict[str, str]` where keys are `#anchor`. Side‑effects: logging of chunk names and content; early exit if counts mismatch.  

<a name="semantic-ordering-workflow"></a>  
## Semantic Ordering Workflow (sorting.py)

**Responsibility** – Request an LLM to reorder section titles semantically and concatenate the associated chunks in that order.  

**Interactions** – Receives the anchor‑chunk map, sends a prompt to `Model.get_answer_without_history`, then assembles `order_output`.  

**Technical Details** – Constructs a user‑role prompt containing the list of titles, parses the comma‑separated LLM response, logs each step, and iterates over the ordered titles to build the final markdown string.  

**Data Flow** – Input: `Model` instance, `chanks` dict. Output: ordered markdown string. Side‑effects: extensive logging.  

<a name="repository-mix-builder"></a>  
## Repository Mix Builder (code_mix.py)

**Responsibility** – Walk a repository, emit a hierarchical file‑tree, and concatenate file contents into a single “repo mix” artifact while respecting ignore patterns.  

**Interactions** – Utilises `Path.rglob` for traversal, `should_ignore` for pattern checks, and `BaseLogger` for progress messages.  

**Technical Details** – `should_ignore` matches a path against user‑supplied glob patterns across the full relative path, basename, and individual parts. `build_repo_content` writes a tree view, inserts a delimiter, then writes each non‑ignored file wrapped in `<file path="...">` tags, handling read errors gracefully.  

**Data Flow** – Input: `root_dir` (Path), `ignore_patterns` (list). Output: `repomix-output.txt` (text file). Side‑effects: file creation, console logs, error annotations inside the output file.  
[2026-01-27 14:19:29.337894] [INFO] Generating documentation for a part...
[2026-01-27 14:19:29.337921] [INFO] Generating answer...
[2026-01-27 14:19:45.831963] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:19:45.832024] [INFO] Answer: <a name="compress-function"></a>  
## `compress` – single‑pass text reduction  

**Responsibility**  
Accepts raw source `data`, builds a three‑message prompt (system settings, compression template, user payload) and returns the model’s answer, which is the compressed representation of the input.  

**Interactions**  
- Uses `ProjectSettings.prompt` for the contextual system message.  
- Calls `get_BASE_COMPRESS_TEXT` to obtain a size‑aware compression instruction.  
- Delegates the LLM request to any `Model` implementation (`GPTModel`, custom adapters).  

**Technical notes**  
- No async I/O; the model call is synchronous via `model.get_answer_without_history`.  
- Returns a plain string; callers are responsible for further aggregation.  

<a name="batch-compression"></a>  
## `compress_and_compare` – synchronous batch aggregator  

**Responsibility**  
Splits a list of file contents into groups of `compress_power` items, compresses each element with `compress`, concatenates the results per group, and tracks progress.  

**Data flow**  
- **Input:** `data` (list of strings), `model`, `project_settings`, optional `compress_power`, `progress_bar`.  
- **Output:** List of aggregated compressed chunks, length ≈ ⌈len(data)/compress_power⌉.  
- **Side‑effects:** Updates `progress_bar` (sub‑task creation, per‑item updates, removal).  

**Logic**  
1. Pre‑allocate a result list sized to the number of groups.  
2. For each element, compute its group index `i // compress_power`, append the compressed string plus newline.  
3. Progress bar reflects each processed element.  

<a name="async-batch-compression"></a>  
## `async_compress_and_compare` – parallel batch compressor  

**Responsibility**  
Performs the same grouping as `compress_and_compare` but launches up to four concurrent LLM calls using `asyncio.Semaphore`.  

**Key steps**  
- Creates a semaphore (limit = 4) and a task list.  
- Each element is wrapped in `async_compress`, which builds the identical prompt and awaits `model.get_answer_without_history`.  
- After `asyncio.gather`, results are re‑chunked according to `compress_power` and concatenated with newlines.  

**Data flow**  
- **Input/Output:** Same as the synchronous version, but returns a coroutine result.  
- **Side‑effects:** Progress bar updates inside `async_compress`; sub‑task lifecycle mirrors the sync variant.  

<a name="iterative-compression"></a>  
## `compress_to_one` – iterative reduction to a single payload  

**Responsibility**  
Repeatedly compresses the dataset until only one aggregated chunk remains, optionally using the async pipeline.  

**Algorithm**  
- While `len(data) > 1`, choose an effective `compress_power` (fallback = 2 for small lists).  
- Call either `async_compress_and_compare` (via `asyncio.run`) or `compress_and_compare`.  
- Increment iteration counter; final output is `data[0]`.  

<a name="description-generation"></a>  
## `generate_discribtions_for_code` – LLM‑driven documentation builder  

**Responsibility**  
Feeds each source file to the model with a strict instruction prompt, collects the model’s descriptive answer, and reports progress.  

**Data flow**  
- **Input:** List of code strings, `model`, `project_settings`, `progress_bar`.  
- **Output:** List of description strings (same order as input).  
- **Side‑effects:** Progress bar sub‑task updates.  

<a name="projectsettings-prompt"></a>  
## `ProjectSettings.prompt` – dynamic system context  

**Responsibility**  
Constructs a multi‑line system prompt by concatenating a base template (`BASE_SETTINGS_PROMPT`) with the project name and any key‑value pairs added via `add_info`.  

**Technical details**  
- `info` is a mutable dict; `add_info` inserts or overwrites entries.  
- Property accessor lazily builds the prompt each call, ensuring the latest `info` content is reflected.  

**Interactions**  
- Consumed by every compression or description function to provide project‑specific background to the LLM.  
[2026-01-27 14:19:45.832071] [INFO] Documentation for part generated. total length: 3987
[2026-01-27 14:19:45.832095] [INFO] Part Documentation: <a name="compress-function"></a>  
## `compress` – single‑pass text reduction  

**Responsibility**  
Accepts raw source `data`, builds a three‑message prompt (system settings, compression template, user payload) and returns the model’s answer, which is the compressed representation of the input.  

**Interactions**  
- Uses `ProjectSettings.prompt` for the contextual system message.  
- Calls `get_BASE_COMPRESS_TEXT` to obtain a size‑aware compression instruction.  
- Delegates the LLM request to any `Model` implementation (`GPTModel`, custom adapters).  

**Technical notes**  
- No async I/O; the model call is synchronous via `model.get_answer_without_history`.  
- Returns a plain string; callers are responsible for further aggregation.  

<a name="batch-compression"></a>  
## `compress_and_compare` – synchronous batch aggregator  

**Responsibility**  
Splits a list of file contents into groups of `compress_power` items, compresses each element with `compress`, concatenates the results per group, and tracks progress.  

**Data flow**  
- **Input:** `data` (list of strings), `model`, `project_settings`, optional `compress_power`, `progress_bar`.  
- **Output:** List of aggregated compressed chunks, length ≈ ⌈len(data)/compress_power⌉.  
- **Side‑effects:** Updates `progress_bar` (sub‑task creation, per‑item updates, removal).  

**Logic**  
1. Pre‑allocate a result list sized to the number of groups.  
2. For each element, compute its group index `i // compress_power`, append the compressed string plus newline.  
3. Progress bar reflects each processed element.  

<a name="async-batch-compression"></a>  
## `async_compress_and_compare` – parallel batch compressor  

**Responsibility**  
Performs the same grouping as `compress_and_compare` but launches up to four concurrent LLM calls using `asyncio.Semaphore`.  

**Key steps**  
- Creates a semaphore (limit = 4) and a task list.  
- Each element is wrapped in `async_compress`, which builds the identical prompt and awaits `model.get_answer_without_history`.  
- After `asyncio.gather`, results are re‑chunked according to `compress_power` and concatenated with newlines.  

**Data flow**  
- **Input/Output:** Same as the synchronous version, but returns a coroutine result.  
- **Side‑effects:** Progress bar updates inside `async_compress`; sub‑task lifecycle mirrors the sync variant.  

<a name="iterative-compression"></a>  
## `compress_to_one` – iterative reduction to a single payload  

**Responsibility**  
Repeatedly compresses the dataset until only one aggregated chunk remains, optionally using the async pipeline.  

**Algorithm**  
- While `len(data) > 1`, choose an effective `compress_power` (fallback = 2 for small lists).  
- Call either `async_compress_and_compare` (via `asyncio.run`) or `compress_and_compare`.  
- Increment iteration counter; final output is `data[0]`.  

<a name="description-generation"></a>  
## `generate_discribtions_for_code` – LLM‑driven documentation builder  

**Responsibility**  
Feeds each source file to the model with a strict instruction prompt, collects the model’s descriptive answer, and reports progress.  

**Data flow**  
- **Input:** List of code strings, `model`, `project_settings`, `progress_bar`.  
- **Output:** List of description strings (same order as input).  
- **Side‑effects:** Progress bar sub‑task updates.  

<a name="projectsettings-prompt"></a>  
## `ProjectSettings.prompt` – dynamic system context  

**Responsibility**  
Constructs a multi‑line system prompt by concatenating a base template (`BASE_SETTINGS_PROMPT`) with the project name and any key‑value pairs added via `add_info`.  

**Technical details**  
- `info` is a mutable dict; `add_info` inserts or overwrites entries.  
- Property accessor lazily builds the prompt each call, ensuring the latest `info` content is reflected.  

**Interactions**  
- Consumed by every compression or description function to provide project‑specific background to the LLM.  
[2026-01-27 14:19:45.832189] [INFO] Generating documentation for a part...
[2026-01-27 14:19:45.832220] [INFO] Generating answer...
[2026-01-27 14:20:08.188524] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:20:08.188595] [INFO] Answer: <a name="data-splitting-algorithm"></a>  
## `split_data` – chunking source text to fit token limits  

**Responsibility**  
Breaks a monolithic code string into a list of substrings whose lengths stay below `max_symbols` while preserving line boundaries.  

**Interactions**  
Uses `BaseLogger` for progress messages; no external state is read or modified.  

**Technical details**  
1. Initial split on newline (`"\n"`).  
2. Repeatedly scans `splited_by_files`; any element exceeding `1.5 × max_symbols` is bisected at `max_symbols/2` and re‑inserted, forcing convergence.  
3. A second pass packs the trimmed fragments into `split_objects`, starting a new part whenever the accumulated length would exceed `1.25 × max_symbols`.  

**Data flow**  
- **Input:** `data: str`, `max_symbols: int`.  
- **Output:** `list[str]` – ordered chunks ready for LLM consumption.  
- **Side‑effects:** One `InfoLog` entry before and after splitting.  

<a name="synchronous-part-doc-generation"></a>  
## `write_docs_by_parts` – generate documentation for a single chunk (sync)  

**Responsibility**  
Builds a system‑user prompt, sends it to a synchronous `Model`, and returns the cleaned LLM answer.  

**Interactions**  
Relies on `BASE_PART_COMPLITE_TEXT` (static instruction), optional `prev_info` (context from the previous chunk), and `BaseLogger`.  

**Technical details**  
- Prompt composition: two mandatory system messages (language + part ID, base instruction) + optional previous‑part context + user message containing the code chunk.  
- Calls `model.get_answer_without_history`.  
- Strips surrounding Markdown fences (```), logs answer length and full content (debug level 2).  

**Data flow**  
- **Input:** `part_id: int`, `part: str`, `model: Model`, `prev_info: str | None`, `language: str`.  
- **Output:** Cleaned documentation string for the chunk.  
- **Side‑effects:** Logging; no mutation of arguments.  

<a name="asynchronous-part-doc-generation"></a>  
## `async_write_docs_by_parts` – concurrent chunk documentation (async)  

**Responsibility**  
Same prompt logic as the sync variant but executed within an `asyncio.Semaphore` to limit parallel LLM calls.  

**Interactions**  
Accepts an `AsyncModel`, optional `update_progress` callback, and shares the same logger.  

**Technical details**  
- `async with semaphore` protects the call to `async_model.get_answer_without_history`.  
- After obtaining the answer, optionally invokes `update_progress` (used by the surrounding progress bar).  
- Performs identical back‑tick stripping and logging.  

**Data flow**  
- **Input:** `part: str`, `async_model: AsyncModel`, `global_info: str`, `semaphore`, `prev_info: str | None`, `language: str`, `update_progress: Callable | None`.  
- **Output:** Cleaned documentation string (coroutine result).  
- **Side‑effects:** Logging; may update external progress bar via callback.  

<a name="batch-doc-generation-sync"></a>  
## `gen_doc_parts` – orchestrated synchronous documentation pipeline  

**Responsibility**  
Splits the full code, sequentially processes each part with `write_docs_by_parts`, aggregates results, and maintains a sub‑task on a `BaseProgress` bar.  

**Interactions**  
Calls `split_data`, creates/updates/removes a sub‑task on `progress_bar`, and logs high‑level milestones.  

**Technical details**  
- After each part, retains the last 3000 characters of the generated text as `prev_info` to provide context for the next iteration.  
- Concatenates all part outputs separated by blank lines.  

**Data flow**  
- **Input:** `full_code_mix: str`, `max_symbols: int`, `model: Model`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Single string containing the complete documentation.  
- **Side‑effects:** Progress‑bar sub‑task lifecycle, extensive logging.  

<a name="batch-doc-generation-async"></a>  
## `async_gen_doc_parts` – orchestrated asynchronous documentation pipeline  

**Responsibility**  
Parallel version of `gen_doc_parts`; dispatches `async_write_docs_by_parts` for every split chunk, respecting a concurrency limit of four.  

**Interactions**  
Creates a semaphore, builds a list of coroutines, gathers them, and updates the same `BaseProgress` sub‑task via a lambda passed to each async worker.  

**Technical details**  
- Uses `asyncio.gather` to await all part‑level tasks.  
- Result aggregation mirrors the sync version (blank‑line separation).  

**Data flow**  
- **Input:** `full_code_mix: str`, `global_info: str`, `max_symbols: int`, `model: AsyncModel`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Full documentation string (awaited coroutine).  
- **Side‑effects:** Progress‑bar sub‑task updates, logging, semaphore enforcement.  
[2026-01-27 14:20:08.188644] [INFO] Documentation for part generated. total length: 4740
[2026-01-27 14:20:08.188669] [INFO] Part Documentation: <a name="data-splitting-algorithm"></a>  
## `split_data` – chunking source text to fit token limits  

**Responsibility**  
Breaks a monolithic code string into a list of substrings whose lengths stay below `max_symbols` while preserving line boundaries.  

**Interactions**  
Uses `BaseLogger` for progress messages; no external state is read or modified.  

**Technical details**  
1. Initial split on newline (`"\n"`).  
2. Repeatedly scans `splited_by_files`; any element exceeding `1.5 × max_symbols` is bisected at `max_symbols/2` and re‑inserted, forcing convergence.  
3. A second pass packs the trimmed fragments into `split_objects`, starting a new part whenever the accumulated length would exceed `1.25 × max_symbols`.  

**Data flow**  
- **Input:** `data: str`, `max_symbols: int`.  
- **Output:** `list[str]` – ordered chunks ready for LLM consumption.  
- **Side‑effects:** One `InfoLog` entry before and after splitting.  

<a name="synchronous-part-doc-generation"></a>  
## `write_docs_by_parts` – generate documentation for a single chunk (sync)  

**Responsibility**  
Builds a system‑user prompt, sends it to a synchronous `Model`, and returns the cleaned LLM answer.  

**Interactions**  
Relies on `BASE_PART_COMPLITE_TEXT` (static instruction), optional `prev_info` (context from the previous chunk), and `BaseLogger`.  

**Technical details**  
- Prompt composition: two mandatory system messages (language + part ID, base instruction) + optional previous‑part context + user message containing the code chunk.  
- Calls `model.get_answer_without_history`.  
- Strips surrounding Markdown fences (```), logs answer length and full content (debug level 2).  

**Data flow**  
- **Input:** `part_id: int`, `part: str`, `model: Model`, `prev_info: str | None`, `language: str`.  
- **Output:** Cleaned documentation string for the chunk.  
- **Side‑effects:** Logging; no mutation of arguments.  

<a name="asynchronous-part-doc-generation"></a>  
## `async_write_docs_by_parts` – concurrent chunk documentation (async)  

**Responsibility**  
Same prompt logic as the sync variant but executed within an `asyncio.Semaphore` to limit parallel LLM calls.  

**Interactions**  
Accepts an `AsyncModel`, optional `update_progress` callback, and shares the same logger.  

**Technical details**  
- `async with semaphore` protects the call to `async_model.get_answer_without_history`.  
- After obtaining the answer, optionally invokes `update_progress` (used by the surrounding progress bar).  
- Performs identical back‑tick stripping and logging.  

**Data flow**  
- **Input:** `part: str`, `async_model: AsyncModel`, `global_info: str`, `semaphore`, `prev_info: str | None`, `language: str`, `update_progress: Callable | None`.  
- **Output:** Cleaned documentation string (coroutine result).  
- **Side‑effects:** Logging; may update external progress bar via callback.  

<a name="batch-doc-generation-sync"></a>  
## `gen_doc_parts` – orchestrated synchronous documentation pipeline  

**Responsibility**  
Splits the full code, sequentially processes each part with `write_docs_by_parts`, aggregates results, and maintains a sub‑task on a `BaseProgress` bar.  

**Interactions**  
Calls `split_data`, creates/updates/removes a sub‑task on `progress_bar`, and logs high‑level milestones.  

**Technical details**  
- After each part, retains the last 3000 characters of the generated text as `prev_info` to provide context for the next iteration.  
- Concatenates all part outputs separated by blank lines.  

**Data flow**  
- **Input:** `full_code_mix: str`, `max_symbols: int`, `model: Model`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Single string containing the complete documentation.  
- **Side‑effects:** Progress‑bar sub‑task lifecycle, extensive logging.  

<a name="batch-doc-generation-async"></a>  
## `async_gen_doc_parts` – orchestrated asynchronous documentation pipeline  

**Responsibility**  
Parallel version of `gen_doc_parts`; dispatches `async_write_docs_by_parts` for every split chunk, respecting a concurrency limit of four.  

**Interactions**  
Creates a semaphore, builds a list of coroutines, gathers them, and updates the same `BaseProgress` sub‑task via a lambda passed to each async worker.  

**Technical details**  
- Uses `asyncio.gather` to await all part‑level tasks.  
- Result aggregation mirrors the sync version (blank‑line separation).  

**Data flow**  
- **Input:** `full_code_mix: str`, `global_info: str`, `max_symbols: int`, `model: AsyncModel`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Full documentation string (awaited coroutine).  
- **Side‑effects:** Progress‑bar sub‑task updates, logging, semaphore enforcement.  
[2026-01-27 14:20:08.188736] [INFO] Generating documentation for a part...
[2026-01-27 14:20:08.188764] [INFO] Generating answer...
[2026-01-27 14:20:29.502971] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:20:29.503030] [INFO] Answer: <a name="logging-structures"></a>
## `BaseLog` hierarchy – structured log messages  

**Responsibility**  
Defines lightweight log objects (`BaseLog`, `ErrorLog`, `WarningLog`, `InfoLog`) that prepend a timestamp and level tag to a user‑supplied message.  

**Interactions**  
Instances are passed to a `BaseLoggerTemplate` via its `log` method; the template decides whether to emit the message based on its configured `log_level`.  

**Technical details**  
- `BaseLog` stores `message` and numeric `level`; its `format` returns the raw text.  
- `_log_prefix` property builds `"[YYYY‑MM‑DD HH:MM:SS]"` using `datetime.fromtimestamp(time.time())`.  
- Sub‑classes override `format` to insert `[ERROR]`, `[WARNING]`, or `[INFO]` after the prefix.  

**Data flow**  
- **Input:** `message: str`, optional `level: int`.  
- **Output:** Formatted string via `format()`.  
- **Side‑effects:** None (pure data object).  

<a name="runtime-logger"></a>
## `BaseLogger` singleton – runtime logging façade  

**Responsibility**  
Provides a globally accessible logger that delegates to a concrete `BaseLoggerTemplate` (console, file, etc.).  

**Interactions**  
- `set_logger` injects a concrete template (`BaseLoggerTemplate`, `FileLoggerTemplate`, …).  
- Calls to `log` forward the `BaseLog` instance to the template’s `global_log`, which respects the template’s `log_level`.  

**Technical details**  
- Implements the classic singleton pattern in `__new__` to guarantee a single shared instance across the process.  
- `global_log` in `BaseLoggerTemplate` checks `log_level` (`-1` disables filtering).  

**Data flow**  
- **Input:** `BaseLog` object.  
- **Output:** Printed to stdout or appended to a file, depending on the active template.  
- **Side‑effects:** I/O (stdout or file write).  

<a name="progress-abstractions"></a>
## `BaseProgress` abstraction and concrete implementations  

**Responsibility**  
Offers a minimal API (`create_new_subtask`, `update_task`, `remove_subtask`) for tracking hierarchical progress in different environments.  

**Interactions**  
- `LibProgress` wraps Rich’s `Progress` object, exposing Rich‑based visual feedback.  
- `ConsoleGtiHubProgress` uses simple `print` statements via `ConsoleTask` for environments without Rich.  

**Technical details**  
- `LibProgress` creates a base task (`General progress`) and optional sub‑tasks; `update_task` advances either the sub‑task or base task.  
- `ConsoleTask` maintains a counter and prints a percentage on each `progress` call.  
- `ConsoleGtiHubProgress` delegates to a `ConsoleTask` for sub‑tasks, falling back to a `ConsoleTask` representing the overall progress.  

**Data flow**  
- **Input:** Sub‑task name and total length.  
- **Output:** Visual progress updates on the console or Rich UI.  
- **Side‑effects:** stdout writes (or Rich UI updates).  

These components together furnish a lightweight, interchangeable logging and progress‑tracking subsystem used throughout the autodocgenerator package.
[2026-01-27 14:20:29.503066] [INFO] Documentation for part generated. total length: 3001
[2026-01-27 14:20:29.503090] [INFO] Part Documentation: <a name="logging-structures"></a>
## `BaseLog` hierarchy – structured log messages  

**Responsibility**  
Defines lightweight log objects (`BaseLog`, `ErrorLog`, `WarningLog`, `InfoLog`) that prepend a timestamp and level tag to a user‑supplied message.  

**Interactions**  
Instances are passed to a `BaseLoggerTemplate` via its `log` method; the template decides whether to emit the message based on its configured `log_level`.  

**Technical details**  
- `BaseLog` stores `message` and numeric `level`; its `format` returns the raw text.  
- `_log_prefix` property builds `"[YYYY‑MM‑DD HH:MM:SS]"` using `datetime.fromtimestamp(time.time())`.  
- Sub‑classes override `format` to insert `[ERROR]`, `[WARNING]`, or `[INFO]` after the prefix.  

**Data flow**  
- **Input:** `message: str`, optional `level: int`.  
- **Output:** Formatted string via `format()`.  
- **Side‑effects:** None (pure data object).  

<a name="runtime-logger"></a>
## `BaseLogger` singleton – runtime logging façade  

**Responsibility**  
Provides a globally accessible logger that delegates to a concrete `BaseLoggerTemplate` (console, file, etc.).  

**Interactions**  
- `set_logger` injects a concrete template (`BaseLoggerTemplate`, `FileLoggerTemplate`, …).  
- Calls to `log` forward the `BaseLog` instance to the template’s `global_log`, which respects the template’s `log_level`.  

**Technical details**  
- Implements the classic singleton pattern in `__new__` to guarantee a single shared instance across the process.  
- `global_log` in `BaseLoggerTemplate` checks `log_level` (`-1` disables filtering).  

**Data flow**  
- **Input:** `BaseLog` object.  
- **Output:** Printed to stdout or appended to a file, depending on the active template.  
- **Side‑effects:** I/O (stdout or file write).  

<a name="progress-abstractions"></a>
## `BaseProgress` abstraction and concrete implementations  

**Responsibility**  
Offers a minimal API (`create_new_subtask`, `update_task`, `remove_subtask`) for tracking hierarchical progress in different environments.  

**Interactions**  
- `LibProgress` wraps Rich’s `Progress` object, exposing Rich‑based visual feedback.  
- `ConsoleGtiHubProgress` uses simple `print` statements via `ConsoleTask` for environments without Rich.  

**Technical details**  
- `LibProgress` creates a base task (`General progress`) and optional sub‑tasks; `update_task` advances either the sub‑task or base task.  
- `ConsoleTask` maintains a counter and prints a percentage on each `progress` call.  
- `ConsoleGtiHubProgress` delegates to a `ConsoleTask` for sub‑tasks, falling back to a `ConsoleTask` representing the overall progress.  

**Data flow**  
- **Input:** Sub‑task name and total length.  
- **Output:** Visual progress updates on the console or Rich UI.  
- **Side‑effects:** stdout writes (or Rich UI updates).  

These components together furnish a lightweight, interchangeable logging and progress‑tracking subsystem used throughout the autodocgenerator package.
[2026-01-27 14:20:29.503203] [INFO] Generating documentation for a part...
[2026-01-27 14:20:29.503241] [INFO] Generating answer...
[2026-01-27 14:20:47.037490] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:20:47.037563] [INFO] Answer: <a name="project-metadata"></a>
## `pyproject.toml` – package identity & version constraints  

**Responsibility**  
Encapsulates the canonical package descriptor for **autodocgenerator**: name, semantic version, human‑readable description, author contact, license, and the supported Python range.  

**Interactions**  
- Read by *Poetry* (or any PEP 517‑compatible builder) to resolve the project’s identity during `poetry install`, `poetry build`, and wheel creation.  
- Exposed to downstream tools (e.g., `pip`, `build`) via the generated `dist/*.whl` metadata.  

**Technical details**  
- `name = "autodocgenerator"` and `version = "0.8.9.9"` follow PEP 621 naming rules.  
- `description` and `readme` feed the **Core Metadata** fields of the resulting distribution.  
- `authors` is a list of `{"name": "...", "email": "..."}` objects, converted to the `Author` field in the wheel.  
- `license = {text = "MIT"}` supplies an SPDX‑compatible license string.  

**Data flow**  
- **Input:** Static TOML literals edited by maintainers.  
- **Output:** Serialized metadata consumed at build time; no runtime side‑effects.  

<a name="runtime-dependencies"></a>
## Dependency enumeration – third‑party runtime requirements  

**Responsibility**  
Lists exact version pins required for the library to operate, ensuring reproducible environments across Python 3.11–3.12.  

**Interactions**  
- Resolved by Poetry’s dependency resolver; each entry becomes a `Requires-Dist` clause in the wheel metadata.  
- At installation, `pip` (via the wheel) pulls the same versions, guaranteeing API stability.  

**Technical details**  
- All entries follow the `package==major.minor.patch` syntax, e.g., `rich==14.2.0`.  
- The list covers HTTP clients (`httpx`), authentication (`google-auth`), AI SDKs (`openai`, `groq`), data handling (`pyyaml`, `msgmsg`), and utilities (`rich_progress`, `tqdm`).  
- No optional groups are defined; every line is a mandatory runtime requirement.  

**Data flow**  
- **Input:** Version‑locked strings.  
- **Output:** Dependency graph constructed by the resolver; installed files appear in `site‑packages`.  

<a name="build-system-configuration"></a>
## Build‑system stanza – PEP 517 backend specification  

**Responsibility**  
Declares the toolchain needed to produce a distributable artifact from the source.  

**Interactions**  
- `requires = ["poetry-core>=2.0.0"]` tells the build frontend to fetch *poetry‑core* before invoking the backend.  
- `build-backend = "poetry.core.masonry.api"` points to the entry‑point that implements `build_wheel`, `build_sdist`, etc.  

**Technical details**  
- The stanza is minimal; no custom build steps are injected, relying on Poetry’s default isolation and caching mechanisms.  

**Data flow**  
- **Input:** The declared backend package.  
- **Output:** A built wheel or source distribution emitted to `dist/`; no side‑effects beyond temporary build directories.  
[2026-01-27 14:20:47.037620] [INFO] Documentation for part generated. total length: 2944
[2026-01-27 14:20:47.037650] [INFO] Part Documentation: <a name="project-metadata"></a>
## `pyproject.toml` – package identity & version constraints  

**Responsibility**  
Encapsulates the canonical package descriptor for **autodocgenerator**: name, semantic version, human‑readable description, author contact, license, and the supported Python range.  

**Interactions**  
- Read by *Poetry* (or any PEP 517‑compatible builder) to resolve the project’s identity during `poetry install`, `poetry build`, and wheel creation.  
- Exposed to downstream tools (e.g., `pip`, `build`) via the generated `dist/*.whl` metadata.  

**Technical details**  
- `name = "autodocgenerator"` and `version = "0.8.9.9"` follow PEP 621 naming rules.  
- `description` and `readme` feed the **Core Metadata** fields of the resulting distribution.  
- `authors` is a list of `{"name": "...", "email": "..."}` objects, converted to the `Author` field in the wheel.  
- `license = {text = "MIT"}` supplies an SPDX‑compatible license string.  

**Data flow**  
- **Input:** Static TOML literals edited by maintainers.  
- **Output:** Serialized metadata consumed at build time; no runtime side‑effects.  

<a name="runtime-dependencies"></a>
## Dependency enumeration – third‑party runtime requirements  

**Responsibility**  
Lists exact version pins required for the library to operate, ensuring reproducible environments across Python 3.11–3.12.  

**Interactions**  
- Resolved by Poetry’s dependency resolver; each entry becomes a `Requires-Dist` clause in the wheel metadata.  
- At installation, `pip` (via the wheel) pulls the same versions, guaranteeing API stability.  

**Technical details**  
- All entries follow the `package==major.minor.patch` syntax, e.g., `rich==14.2.0`.  
- The list covers HTTP clients (`httpx`), authentication (`google-auth`), AI SDKs (`openai`, `groq`), data handling (`pyyaml`, `msgmsg`), and utilities (`rich_progress`, `tqdm`).  
- No optional groups are defined; every line is a mandatory runtime requirement.  

**Data flow**  
- **Input:** Version‑locked strings.  
- **Output:** Dependency graph constructed by the resolver; installed files appear in `site‑packages`.  

<a name="build-system-configuration"></a>
## Build‑system stanza – PEP 517 backend specification  

**Responsibility**  
Declares the toolchain needed to produce a distributable artifact from the source.  

**Interactions**  
- `requires = ["poetry-core>=2.0.0"]` tells the build frontend to fetch *poetry‑core* before invoking the backend.  
- `build-backend = "poetry.core.masonry.api"` points to the entry‑point that implements `build_wheel`, `build_sdist`, etc.  

**Technical details**  
- The stanza is minimal; no custom build steps are injected, relying on Poetry’s default isolation and caching mechanisms.  

**Data flow**  
- **Input:** The declared backend package.  
- **Output:** A built wheel or source distribution emitted to `dist/`; no side‑effects beyond temporary build directories.  
[2026-01-27 14:20:47.037754] [INFO] Documentation generation by parts completed.

                       Total documentation length: 33338
[2026-01-27 14:20:47.037822] [INFO] Documentation: <a name="config-reader-responsibility"></a>
## Config‑Reader Responsibility
`read_config` loads a user‑supplied YAML string, transforms it into a strongly‑typed `Config` object and a list of custom documentation modules. It centralises all project‑wide settings (language, name, ignored files, additional info) and prepares module descriptors for the documentation engine.

<a name="config-reader-interaction"></a>
## Interaction with Factory Modules
The function imports `CustomModule` and `CustomModuleWithOutContext` from `autodocgenerator.factory.modules.general_modules`.  
* When a description line starts with “%” it is wrapped in `CustomModuleWithOutContext` (context‑free).  
* Otherwise the line is turned into a regular `CustomModule`.  
These objects are later consumed by the rendering pipeline to inject user‑defined sections into the generated markdown.

<a name="config-reader-technical-details"></a>
## Technical Details and Logic Flow
1. **YAML parsing** – `yaml.safe_load` converts the raw string to a Python dict.  
2. **Base `Config` creation** – a fresh `Config` instance is instantiated.  
3. **Core fields extraction** – `ignore_files`, `language`, `project_name`, and `project_additional_info` are read with sensible defaults.  
4. **Project settings handling** – a `ProjectConfigSettings` object (`pcs`) loads the `project_settings` mapping via `pcs.load_settings`.  
5. **Fluent configuration** – `config.set_language(...).set_project_name(...).set_pcs(pcs)` chains setters for readability.  
6. **Iterative population** – loops add ignore patterns and additional info via `config.add_ignore_file` and `config.add_project_additional_info`.  
7. **Custom description conversion** – each entry in `custom_descriptions` is examined; the leading character decides which module class to instantiate.  

<a name="config-reader-data-flow"></a>
## Input, Output, and Side Effects
| Aspect | Description |
|--------|-------------|
| **Input** | `file_data: str` – raw YAML content supplied by `autodocconfig.yml`. |
| **Output** | `Tuple[Config, List[CustomModule]]` – a ready‑to‑use configuration object and a collection of module descriptors. |
| **Side effects** | None; the function is pure aside from importing modules. It assumes the YAML schema contains the keys used above; missing optional keys fall back to defaults. |
| **Assumptions** | The YAML is well‑formed, custom description strings are non‑empty, and the imported module classes conform to the expected constructor signatures. |
| **Error handling** | Propagation of `yaml.YAMLError` or any constructor exception to the caller; the runner catches and logs them via the global logger. |

Together, `read_config` supplies the documentation engine with a complete, validated configuration snapshot and the custom module list required for the subsequent generation steps.

<a name="runfile-execution"></a>  
## Run‑file execution flow  

`gen_doc` orchestrates the full documentation pipeline. It instantiates a **synchronous** (`GPTModel`) and an **asynchronous** (`AsyncGPTModel`) LLM client using the global `API_KEY`. These models are injected into a `Manager` together with:  

* `project_path` – root of the source tree.  
* `config` – a `Config` instance (see *Configuration object construction*).  
* `ConsoleGtiHubProgress` – CLI progress visualiser.  

The manager then performs the ordered steps:  

1. `generate_code_file()` – parses the project and creates an internal code representation.  
2. `generete_doc_parts(max_symbols=5000)` – produces raw documentation fragments, capped at 5 k symbols each.  
3. `factory_generate_doc(DocFactory(*custom_modules))` – runs user‑supplied modules (e.g., custom extractors).  
4. `order_doc()` – re‑orders fragments according to logical sections.  
5. `factory_generate_doc(DocFactory(IntroLinks()))` – adds the autogenerated introductory link block.  
6. `clear_cache()` – removes temporary artefacts.  

Finally `read_file_by_file_key("output_doc")` returns the assembled markdown document.

<a name="configuration-object-construction"></a>  
## Configuration object construction  

`Config` aggregates global settings:  

* **ignore_files** – glob patterns excluded from scanning.  
* **language** – default `"en"`.  
* **project_name** & **project_additional_info** – descriptive metadata passed to `ProjectSettings`.  
* **pcs** – a `ProjectConfigSettings` container (log flags).  

Convenient setters (`set_language`, `set_project_name`, `add_ignore_file`, …) enable fluent building.  
`get_project_settings()` converts the stored metadata into a `ProjectSettings` instance used later by the `Manager`.

<a name="data-flow-and-side‑effects"></a>  
## Data flow and side‑effects  

* **Input** – `project_path` (string), a parsed `Config` object, and a list of instantiated custom module objects.  
* **Processing** – manager writes intermediate code files to the hidden `.auto_doc_cache` directory, updates the console progress bar, and may create temporary log files if `ProjectConfigSettings.save_logs` is true.  
* **Output** – a single string containing the complete documentation (`output_doc`).  
* **Assumptions** – a valid `API_KEY` is available, custom modules implement the `BaseModule` interface expected by `DocFactory`, and the project directory is readable.  

These steps provide the validated configuration snapshot and the custom module list required for the downstream generation engine.

<a name="parentmodel-shared-configuration"></a>  
## ParentModel – Shared Model Configuration  

`ParentModel` centralises authentication, history handling, and model‑selection logic. It receives an `api_key`, a `History` instance, and a `use_random` flag. The constructor shuffles the global `MODELS_NAME` list (when randomised) and stores the ordered copy in `self.regen_models_name`. Index tracking (`self.current_model_index`) enables round‑robin fallback when a model fails.

---

<a name="history-conversation-state"></a>  
## History – Conversation State Management  

`History` builds the message array sent to the LLM. It auto‑injects the system prompt (`BASE_SYSTEM_TEXT`) and provides `add_to_history(role, content)` to append user or assistant entries. The `self.history` list is consumed directly by the model classes.

---

<a name="gptmodel-synchronous-generation"></a>  
## GPTModel – Synchronous Answer Generation  

Inherits from `Model` (which extends `ParentModel`).  
* **Responsibility** – Wraps the Groq synchronous client (`Groq`) to produce a completion.  
* **Interaction** – Uses `self.client.chat.completions.create(messages, model)`; falls back through `self.regen_models_name` on exception, logging each attempt via `BaseLogger`.  
* **Data Flow** –  
  1. Chooses `messages` from `self.history.history` or a raw `prompt`.  
  2. Calls the Groq API, extracts `choices[0].message.content`.  
  3. Logs generation steps and returns the answer.

---

<a name="asyncgptmodel-asynchronous-generation"></a>  
## AsyncGPTModel – Asynchronous Answer Generation  

Mirrors `GPTModel` but operates with `AsyncGroq`.  
* **Responsibility** – Provides `async generate_answer` for non‑blocking calls.  
* **Interaction** – Awaits `self.client.chat.completions.create`. On failure, logs a warning and advances `self.current_model_index`.  
* **Data Flow** – Same as the synchronous variant, but returns a coroutine result and logs at level 2 for the final answer.

Both model classes rely on the shared fallback mechanism defined in `ParentModel`, ensuring resilience when a specific model endpoint is unavailable.

<a name="docfactory-orchestrating-documentation-generation"></a>  
## DocFactory – Orchestrating Documentation Generation  

* **Responsibility** – Accepts an ordered list of `BaseModule` instances and sequentially invokes each to produce a composite documentation string.  
* **Interaction** –  
  * Receives a `Model` (or `AsyncModel`) and a `BaseProgress` UI object from the caller.  
  * Calls `module.generate(info, model)` for every injected module.  
  * Emits `InfoLog` entries via the shared `BaseLogger`, forwarding the module name and its raw output (level 2).  
* **Technical Details** –  
  * Stores modules in `self.modules: list[BaseModule]`.  
  * Initializes a simple logger (`BaseLogger()`).  
  * Uses `progress.create_new_subtask` to allocate a sub‑task sized to the module count, `progress.update_task` after each module, and `progress.remove_subtask` on completion.  
* **Data Flow** –  
  1. Input: `info` dict, `model` instance, `progress` UI.  
  2. For each module: `module.generate` returns a string → concatenated to `output`.  
  3. Side‑effects: logging, progress bar updates.  
  4. Output: the final assembled documentation string.

<a name="custommodule-context-aware-intro-generation"></a>  
## CustomModule – Context‑Aware Intro Generation  

* **Responsibility** – Generates a custom introductory paragraph that incorporates a user‑provided description and a code‑snippet excerpt.  
* **Interaction** – Calls `split_data` to truncate `info["code_mix"]` to ≤ 5 000 symbols, then forwards the split text, the model, the description, and the target language to `generete_custom_discription`.  
* **Technical Details** –  
  * Inherits from `BaseModule`.  
  * Holds `self.discription` (typo retained from original).  
  * `generate` returns the processed string from the post‑processor.  

* **Data Flow** –  
  1. Input: `info` dict (keys `code_mix`, `language`), `model`.  
  2. Processing: `split_data → generete_custom_discription`.  
  3. Output: formatted intro paragraph.

<a name="custommodulewithoutcontext‑static-intro-generation"></a>  
## CustomModuleWithOutContext – Static Intro Generation  

* **Responsibility** – Produces a description‑only intro, omitting any code context.  
* **Interaction** – Directly invokes `generete_custom_discription_without` with the model, stored description, and language.  
* **Data Flow** – Input: `info["language"]`, `model`; Output: static intro string; No side‑effects beyond the returned text.

<a name="introlinks‑html‑link‑extraction"></a>  
## IntroLinks – HTML Link Extraction for Documentation  

* **Responsibility** – Extracts all HTML links from `info["full_data"]` and synthesizes a brief introductory block about those links.  
* **Interaction** – Uses `get_all_html_links` → `get_links_intro`, passing the model and language.  

<a name="introtext‑global‑context‑introduction"></a>  
## IntroText – Global Context Introduction  

* **Responsibility** – Generates a high‑level introduction based on `info["global_data"]`.  
* **Interaction** – Calls `get_introdaction` with the model and language, returning the result.  

<a name="manager-orchestration-pipeline"></a>
## Manager – Orchestration of Documentation Pipeline  

* **Responsibility** – Coordinates the end‑to‑end generation of documentation: builds the code‑mix cache, splits large sources, invokes the factory‑based module chain, and orders the final output.  

* **Interaction** –  
  * Accepts a project root, a `Config` object, optional synchronous (`Model`) and asynchronous (`AsyncModel`) language models, and a UI progress handler (`BaseProgress`).  
  * Utilises `BaseLogger` (file‑backed via `FileLoggerTemplate`) for all informational, warning and error logs.  
  * Calls external preprocessors (`split_data`, `gen_doc_parts`, `compress_to_one`), post‑processors (`get_introdaction`, `get_all_html_links`, …) and the injected `DocFactory` to produce the final markdown.  

* **Technical Details** –  
  * Constants `CACHE_FOLDER_NAME` and `FILE_NAMES` define persistent cache locations (`code_mix.txt`, `global_info.md`, `report.txt`, `output_doc.md`).  
  * `__init__` creates the cache folder, configures the logger, and stores references to models and progress UI.  
  * Core methods:  
    - `generate_code_file` → builds a `CodeMix` instance, writes the mixed source to cache, updates progress.  
    - `generete_doc_parts` → streams the cached mix through `gen_doc_parts`, writes the partial markdown, logs progress.  
    - `factory_generate_doc` → loads current output and code mix, assembles an `info` dict, logs the module list, invokes `doc_factory.generate_doc`, prepends the new fragment to the existing document.  
    - `order_doc` → splits the markdown by anchor markers, asks the synchronous model to reorder sections via `get_order`, and rewrites the file.  
    - `clear_cache` → optionally removes the log file based on `config.pcs.save_logs`.  

* **Data Flow** –  
  1. **Input**: project directory path, `Config`, optional models, UI progress object.  
  2. **Processing**:  
     - `CodeMix.build_repo_content` → writes raw source to `code_mix.txt`.  
     - `gen_doc_parts` → consumes the mix, returns a partial markdown string.  
     - `DocFactory.generate_doc` → receives `info` (`language`, `full_data`, `code_mix`), calls each module’s `generate`, aggregates results, updates progress.  
     - `split_text_by_anchors` / `get_order` → reorders sections.  
  3. **Side‑effects**: file system writes (cache files, final `output_doc.md`), logger entries, progress‑bar updates.  
  4. **Output**: a fully assembled `output_doc.md` containing ordered documentation ready for consumption.

**<a name="html-link-extraction"></a>HTML‑Link Extraction**  
The function `get_all_html_links` scans a markdown string for anchor tags of the form `<a name="…"></a>`. It logs the start/end of the operation, builds a list of fragment identifiers prefixed with “#”, and returns that list.  

*Inputs*: `data` – full documentation text.  
*Outputs*: `list[str]` – collected anchors (e.g., `["#section‑overview"]`).  
*Side‑effects*: Writes two `InfoLog` entries via `BaseLogger`.  

---

**<a name="intro-with-links-generation"></a>Intro‑With‑Links Generation**  
`get_links_intro` receives the anchor list, a `Model` implementation, and an optional language code. It composes a three‑message system‑user prompt (language hint, `BASE_INTRODACTION_CREATE_LINKS` template, and the raw link list) and calls `model.get_answer_without_history`.  

*Inputs*: `links` (list of anchors), `model` (concrete `Model`/`GPTModel`), `language`.  
*Outputs*: String containing a generated introductory paragraph that references the provided links.  
*Side‑effects*: Logs start/completion messages and the raw LLM response.  

---

**<a name="basic-introduction-generation"></a>Basic Introduction Generation**  
`get_introdaction` (typo intentional) builds a similar prompt using `BASE_INTRO_CREATE` and the full documentation (`global_data`). It returns the LLM‑generated introductory block.  

*Inputs*: `global_data` (full docs), `model`, `language`.  
*Outputs*: Introduction string.  

---

**<a name="custom-description-loop"></a>Custom Description Loop**  
`generete_custom_discription` iterates over a pre‑split collection of documentation fragments (`splited_data`). For each fragment it sends a detailed system prompt (technical analyst role, context snippet, `BASE_CUSTOM_DISCRIPTIONS`) together with the user‑requested `custom_description`. The loop breaks when the LLM returns a substantive answer (i.e., does not contain “!noinfo” or “No information found”). The final `result` is returned.  

*Inputs*: `splited_data` (iterable of strings), `model`, `custom_description`, `language`.  
*Outputs*: Custom description text for the first fragment that yields valid content.  

---

**<a name="standalone-custom-description"></a>Standalone Custom Description**  
`generete_custom_discription_without` builds a single‑shot prompt that forces the LLM to prepend a strict `<a name="CONTENT_DESCRIPTION"></a>` tag and obey naming constraints (no filenames, extensions, generic terms, or URLs). The result is returned directly.  

*Inputs*: `model`, `custom_description`, `language`.  
*Outputs*: Tagged description string.  

---

**Interaction Summary**  
All functions rely on the shared `BaseLogger` for traceability and on the abstract `Model` interface (concretely `GPTModel`) to invoke the LLM. They do **not** perform I/O beyond logging; the calling post‑processor aggregates their outputs into the final documentation file.

<a name="anchor-extraction-routine"></a>  
## Anchor Extraction Routine (sorting.py)

**Responsibility** – Detect HTML‑style anchors at the start of each markdown chunk and build a mapping `anchor → chunk`.  

**Interactions** – Consumes raw README text, feeds the resulting dictionary to `get_order`. Relies on `BaseLogger` for diagnostic output.  

**Technical Details** – `extract_links_from_start` uses a regex `^<a name=["']?(.*?)["']?</a>` to capture names longer than five characters, prefixing them with `#`. `split_text_by_anchors` splits the document on a look‑ahead pattern, trims empties, validates a one‑to‑one link‑chunk relationship, and returns `{anchor: chunk}` or `None`.  

**Data Flow** – Input: `text` (str). Output: `dict[str, str]` where keys are `#anchor`. Side‑effects: logging of chunk names and content; early exit if counts mismatch.  

<a name="semantic-ordering-workflow"></a>  
## Semantic Ordering Workflow (sorting.py)

**Responsibility** – Request an LLM to reorder section titles semantically and concatenate the associated chunks in that order.  

**Interactions** – Receives the anchor‑chunk map, sends a prompt to `Model.get_answer_without_history`, then assembles `order_output`.  

**Technical Details** – Constructs a user‑role prompt containing the list of titles, parses the comma‑separated LLM response, logs each step, and iterates over the ordered titles to build the final markdown string.  

**Data Flow** – Input: `Model` instance, `chanks` dict. Output: ordered markdown string. Side‑effects: extensive logging.  

<a name="repository-mix-builder"></a>  
## Repository Mix Builder (code_mix.py)

**Responsibility** – Walk a repository, emit a hierarchical file‑tree, and concatenate file contents into a single “repo mix” artifact while respecting ignore patterns.  

**Interactions** – Utilises `Path.rglob` for traversal, `should_ignore` for pattern checks, and `BaseLogger` for progress messages.  

**Technical Details** – `should_ignore` matches a path against user‑supplied glob patterns across the full relative path, basename, and individual parts. `build_repo_content` writes a tree view, inserts a delimiter, then writes each non‑ignored file wrapped in `<file path="...">` tags, handling read errors gracefully.  

**Data Flow** – Input: `root_dir` (Path), `ignore_patterns` (list). Output: `repomix-output.txt` (text file). Side‑effects: file creation, console logs, error annotations inside the output file.  

<a name="compress-function"></a>  
## `compress` – single‑pass text reduction  

**Responsibility**  
Accepts raw source `data`, builds a three‑message prompt (system settings, compression template, user payload) and returns the model’s answer, which is the compressed representation of the input.  

**Interactions**  
- Uses `ProjectSettings.prompt` for the contextual system message.  
- Calls `get_BASE_COMPRESS_TEXT` to obtain a size‑aware compression instruction.  
- Delegates the LLM request to any `Model` implementation (`GPTModel`, custom adapters).  

**Technical notes**  
- No async I/O; the model call is synchronous via `model.get_answer_without_history`.  
- Returns a plain string; callers are responsible for further aggregation.  

<a name="batch-compression"></a>  
## `compress_and_compare` – synchronous batch aggregator  

**Responsibility**  
Splits a list of file contents into groups of `compress_power` items, compresses each element with `compress`, concatenates the results per group, and tracks progress.  

**Data flow**  
- **Input:** `data` (list of strings), `model`, `project_settings`, optional `compress_power`, `progress_bar`.  
- **Output:** List of aggregated compressed chunks, length ≈ ⌈len(data)/compress_power⌉.  
- **Side‑effects:** Updates `progress_bar` (sub‑task creation, per‑item updates, removal).  

**Logic**  
1. Pre‑allocate a result list sized to the number of groups.  
2. For each element, compute its group index `i // compress_power`, append the compressed string plus newline.  
3. Progress bar reflects each processed element.  

<a name="async-batch-compression"></a>  
## `async_compress_and_compare` – parallel batch compressor  

**Responsibility**  
Performs the same grouping as `compress_and_compare` but launches up to four concurrent LLM calls using `asyncio.Semaphore`.  

**Key steps**  
- Creates a semaphore (limit = 4) and a task list.  
- Each element is wrapped in `async_compress`, which builds the identical prompt and awaits `model.get_answer_without_history`.  
- After `asyncio.gather`, results are re‑chunked according to `compress_power` and concatenated with newlines.  

**Data flow**  
- **Input/Output:** Same as the synchronous version, but returns a coroutine result.  
- **Side‑effects:** Progress bar updates inside `async_compress`; sub‑task lifecycle mirrors the sync variant.  

<a name="iterative-compression"></a>  
## `compress_to_one` – iterative reduction to a single payload  

**Responsibility**  
Repeatedly compresses the dataset until only one aggregated chunk remains, optionally using the async pipeline.  

**Algorithm**  
- While `len(data) > 1`, choose an effective `compress_power` (fallback = 2 for small lists).  
- Call either `async_compress_and_compare` (via `asyncio.run`) or `compress_and_compare`.  
- Increment iteration counter; final output is `data[0]`.  

<a name="description-generation"></a>  
## `generate_discribtions_for_code` – LLM‑driven documentation builder  

**Responsibility**  
Feeds each source file to the model with a strict instruction prompt, collects the model’s descriptive answer, and reports progress.  

**Data flow**  
- **Input:** List of code strings, `model`, `project_settings`, `progress_bar`.  
- **Output:** List of description strings (same order as input).  
- **Side‑effects:** Progress bar sub‑task updates.  

<a name="projectsettings-prompt"></a>  
## `ProjectSettings.prompt` – dynamic system context  

**Responsibility**  
Constructs a multi‑line system prompt by concatenating a base template (`BASE_SETTINGS_PROMPT`) with the project name and any key‑value pairs added via `add_info`.  

**Technical details**  
- `info` is a mutable dict; `add_info` inserts or overwrites entries.  
- Property accessor lazily builds the prompt each call, ensuring the latest `info` content is reflected.  

**Interactions**  
- Consumed by every compression or description function to provide project‑specific background to the LLM.  

<a name="data-splitting-algorithm"></a>  
## `split_data` – chunking source text to fit token limits  

**Responsibility**  
Breaks a monolithic code string into a list of substrings whose lengths stay below `max_symbols` while preserving line boundaries.  

**Interactions**  
Uses `BaseLogger` for progress messages; no external state is read or modified.  

**Technical details**  
1. Initial split on newline (`"\n"`).  
2. Repeatedly scans `splited_by_files`; any element exceeding `1.5 × max_symbols` is bisected at `max_symbols/2` and re‑inserted, forcing convergence.  
3. A second pass packs the trimmed fragments into `split_objects`, starting a new part whenever the accumulated length would exceed `1.25 × max_symbols`.  

**Data flow**  
- **Input:** `data: str`, `max_symbols: int`.  
- **Output:** `list[str]` – ordered chunks ready for LLM consumption.  
- **Side‑effects:** One `InfoLog` entry before and after splitting.  

<a name="synchronous-part-doc-generation"></a>  
## `write_docs_by_parts` – generate documentation for a single chunk (sync)  

**Responsibility**  
Builds a system‑user prompt, sends it to a synchronous `Model`, and returns the cleaned LLM answer.  

**Interactions**  
Relies on `BASE_PART_COMPLITE_TEXT` (static instruction), optional `prev_info` (context from the previous chunk), and `BaseLogger`.  

**Technical details**  
- Prompt composition: two mandatory system messages (language + part ID, base instruction) + optional previous‑part context + user message containing the code chunk.  
- Calls `model.get_answer_without_history`.  
- Strips surrounding Markdown fences (```), logs answer length and full content (debug level 2).  

**Data flow**  
- **Input:** `part_id: int`, `part: str`, `model: Model`, `prev_info: str | None`, `language: str`.  
- **Output:** Cleaned documentation string for the chunk.  
- **Side‑effects:** Logging; no mutation of arguments.  

<a name="asynchronous-part-doc-generation"></a>  
## `async_write_docs_by_parts` – concurrent chunk documentation (async)  

**Responsibility**  
Same prompt logic as the sync variant but executed within an `asyncio.Semaphore` to limit parallel LLM calls.  

**Interactions**  
Accepts an `AsyncModel`, optional `update_progress` callback, and shares the same logger.  

**Technical details**  
- `async with semaphore` protects the call to `async_model.get_answer_without_history`.  
- After obtaining the answer, optionally invokes `update_progress` (used by the surrounding progress bar).  
- Performs identical back‑tick stripping and logging.  

**Data flow**  
- **Input:** `part: str`, `async_model: AsyncModel`, `global_info: str`, `semaphore`, `prev_info: str | None`, `language: str`, `update_progress: Callable | None`.  
- **Output:** Cleaned documentation string (coroutine result).  
- **Side‑effects:** Logging; may update external progress bar via callback.  

<a name="batch-doc-generation-sync"></a>  
## `gen_doc_parts` – orchestrated synchronous documentation pipeline  

**Responsibility**  
Splits the full code, sequentially processes each part with `write_docs_by_parts`, aggregates results, and maintains a sub‑task on a `BaseProgress` bar.  

**Interactions**  
Calls `split_data`, creates/updates/removes a sub‑task on `progress_bar`, and logs high‑level milestones.  

**Technical details**  
- After each part, retains the last 3000 characters of the generated text as `prev_info` to provide context for the next iteration.  
- Concatenates all part outputs separated by blank lines.  

**Data flow**  
- **Input:** `full_code_mix: str`, `max_symbols: int`, `model: Model`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Single string containing the complete documentation.  
- **Side‑effects:** Progress‑bar sub‑task lifecycle, extensive logging.  

<a name="batch-doc-generation-async"></a>  
## `async_gen_doc_parts` – orchestrated asynchronous documentation pipeline  

**Responsibility**  
Parallel version of `gen_doc_parts`; dispatches `async_write_docs_by_parts` for every split chunk, respecting a concurrency limit of four.  

**Interactions**  
Creates a semaphore, builds a list of coroutines, gathers them, and updates the same `BaseProgress` sub‑task via a lambda passed to each async worker.  

**Technical details**  
- Uses `asyncio.gather` to await all part‑level tasks.  
- Result aggregation mirrors the sync version (blank‑line separation).  

**Data flow**  
- **Input:** `full_code_mix: str`, `global_info: str`, `max_symbols: int`, `model: AsyncModel`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Full documentation string (awaited coroutine).  
- **Side‑effects:** Progress‑bar sub‑task updates, logging, semaphore enforcement.  

<a name="logging-structures"></a>
## `BaseLog` hierarchy – structured log messages  

**Responsibility**  
Defines lightweight log objects (`BaseLog`, `ErrorLog`, `WarningLog`, `InfoLog`) that prepend a timestamp and level tag to a user‑supplied message.  

**Interactions**  
Instances are passed to a `BaseLoggerTemplate` via its `log` method; the template decides whether to emit the message based on its configured `log_level`.  

**Technical details**  
- `BaseLog` stores `message` and numeric `level`; its `format` returns the raw text.  
- `_log_prefix` property builds `"[YYYY‑MM‑DD HH:MM:SS]"` using `datetime.fromtimestamp(time.time())`.  
- Sub‑classes override `format` to insert `[ERROR]`, `[WARNING]`, or `[INFO]` after the prefix.  

**Data flow**  
- **Input:** `message: str`, optional `level: int`.  
- **Output:** Formatted string via `format()`.  
- **Side‑effects:** None (pure data object).  

<a name="runtime-logger"></a>
## `BaseLogger` singleton – runtime logging façade  

**Responsibility**  
Provides a globally accessible logger that delegates to a concrete `BaseLoggerTemplate` (console, file, etc.).  

**Interactions**  
- `set_logger` injects a concrete template (`BaseLoggerTemplate`, `FileLoggerTemplate`, …).  
- Calls to `log` forward the `BaseLog` instance to the template’s `global_log`, which respects the template’s `log_level`.  

**Technical details**  
- Implements the classic singleton pattern in `__new__` to guarantee a single shared instance across the process.  
- `global_log` in `BaseLoggerTemplate` checks `log_level` (`-1` disables filtering).  

**Data flow**  
- **Input:** `BaseLog` object.  
- **Output:** Printed to stdout or appended to a file, depending on the active template.  
- **Side‑effects:** I/O (stdout or file write).  

<a name="progress-abstractions"></a>
## `BaseProgress` abstraction and concrete implementations  

**Responsibility**  
Offers a minimal API (`create_new_subtask`, `update_task`, `remove_subtask`) for tracking hierarchical progress in different environments.  

**Interactions**  
- `LibProgress` wraps Rich’s `Progress` object, exposing Rich‑based visual feedback.  
- `ConsoleGtiHubProgress` uses simple `print` statements via `ConsoleTask` for environments without Rich.  

**Technical details**  
- `LibProgress` creates a base task (`General progress`) and optional sub‑tasks; `update_task` advances either the sub‑task or base task.  
- `ConsoleTask` maintains a counter and prints a percentage on each `progress` call.  
- `ConsoleGtiHubProgress` delegates to a `ConsoleTask` for sub‑tasks, falling back to a `ConsoleTask` representing the overall progress.  

**Data flow**  
- **Input:** Sub‑task name and total length.  
- **Output:** Visual progress updates on the console or Rich UI.  
- **Side‑effects:** stdout writes (or Rich UI updates).  

These components together furnish a lightweight, interchangeable logging and progress‑tracking subsystem used throughout the autodocgenerator package.

<a name="project-metadata"></a>
## `pyproject.toml` – package identity & version constraints  

**Responsibility**  
Encapsulates the canonical package descriptor for **autodocgenerator**: name, semantic version, human‑readable description, author contact, license, and the supported Python range.  

**Interactions**  
- Read by *Poetry* (or any PEP 517‑compatible builder) to resolve the project’s identity during `poetry install`, `poetry build`, and wheel creation.  
- Exposed to downstream tools (e.g., `pip`, `build`) via the generated `dist/*.whl` metadata.  

**Technical details**  
- `name = "autodocgenerator"` and `version = "0.8.9.9"` follow PEP 621 naming rules.  
- `description` and `readme` feed the **Core Metadata** fields of the resulting distribution.  
- `authors` is a list of `{"name": "...", "email": "..."}` objects, converted to the `Author` field in the wheel.  
- `license = {text = "MIT"}` supplies an SPDX‑compatible license string.  

**Data flow**  
- **Input:** Static TOML literals edited by maintainers.  
- **Output:** Serialized metadata consumed at build time; no runtime side‑effects.  

<a name="runtime-dependencies"></a>
## Dependency enumeration – third‑party runtime requirements  

**Responsibility**  
Lists exact version pins required for the library to operate, ensuring reproducible environments across Python 3.11–3.12.  

**Interactions**  
- Resolved by Poetry’s dependency resolver; each entry becomes a `Requires-Dist` clause in the wheel metadata.  
- At installation, `pip` (via the wheel) pulls the same versions, guaranteeing API stability.  

**Technical details**  
- All entries follow the `package==major.minor.patch` syntax, e.g., `rich==14.2.0`.  
- The list covers HTTP clients (`httpx`), authentication (`google-auth`), AI SDKs (`openai`, `groq`), data handling (`pyyaml`, `msgmsg`), and utilities (`rich_progress`, `tqdm`).  
- No optional groups are defined; every line is a mandatory runtime requirement.  

**Data flow**  
- **Input:** Version‑locked strings.  
- **Output:** Dependency graph constructed by the resolver; installed files appear in `site‑packages`.  

<a name="build-system-configuration"></a>
## Build‑system stanza – PEP 517 backend specification  

**Responsibility**  
Declares the toolchain needed to produce a distributable artifact from the source.  

**Interactions**  
- `requires = ["poetry-core>=2.0.0"]` tells the build frontend to fetch *poetry‑core* before invoking the backend.  
- `build-backend = "poetry.core.masonry.api"` points to the entry‑point that implements `build_wheel`, `build_sdist`, etc.  

**Technical details**  
- The stanza is minimal; no custom build steps are injected, relying on Poetry’s default isolation and caching mechanisms.  

**Data flow**  
- **Input:** The declared backend package.  
- **Output:** A built wheel or source distribution emitted to `dist/`; no side‑effects beyond temporary build directories.  


[2026-01-27 14:20:47.038045] [INFO] Documentation generation by parts completed.
[2026-01-27 14:20:47.039311] [INFO] Starting factory documentation generation 

                                CustomModuleWithOutContext CustomModule 

                                Input params: language: 2 chars full_data: 33338 chars code_mix: 278089 chars
                                
[2026-01-27 14:20:47.039382] [INFO] Generating answer...
[2026-01-27 14:20:52.156574] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:20:52.156636] [INFO] Answer: <a name="install-workflow-setup"></a>

**Installation workflow overview**

1. **PowerShell (Windows)**
   - Open a PowerShell terminal with elevated privileges.
   - Execute a one‑liner that downloads and runs the remote installer script:
     ```
     irm <raw‑url‑to‑powershell‑installer> | iex
     ```
   - The script will perform all required setup steps for the Windows environment.

2. **Shell (Linux/macOS)**
   - Open a terminal.
   - Run the following one‑liner to fetch and execute the installer for Unix‑like systems:
     ```
     curl -sSL <raw‑url‑to‑shell‑installer> | bash
     ```
   - The installer will handle dependency resolution and configuration automatically.

3. **GitHub Actions integration**
   - In the repository’s **Settings → Secrets and variables → Actions**, create a new secret named `GROCK_API_KEY`.
   - Populate this secret with the API key obtained from the Grock documentation.
   - The workflow file should reference this secret (e.g., `${{ secrets.GROCK_API_KEY }}`) so that the installer can authenticate with Grock services during the CI run.

4. **Verification**
   - After the script finishes, confirm that the application binaries are available and that any required services are running.
   - In CI, ensure the workflow passes the steps that rely on the `GROCK_API_KEY` secret.

**Key points**
- Use the one‑liner commands to avoid manual download steps.
- The secret must be set at the repository level; it is not exposed in logs.
- The installer scripts are designed to be idempotent, so re‑running them will not cause duplicate installations.
[2026-01-27 14:20:52.156686] [INFO] Module CustomModuleWithOutContext generated its part of the documentation.
[2026-01-27 14:20:52.156711] [INFO] Module Output: <a name="install-workflow-setup"></a>

**Installation workflow overview**

1. **PowerShell (Windows)**
   - Open a PowerShell terminal with elevated privileges.
   - Execute a one‑liner that downloads and runs the remote installer script:
     ```
     irm <raw‑url‑to‑powershell‑installer> | iex
     ```
   - The script will perform all required setup steps for the Windows environment.

2. **Shell (Linux/macOS)**
   - Open a terminal.
   - Run the following one‑liner to fetch and execute the installer for Unix‑like systems:
     ```
     curl -sSL <raw‑url‑to‑shell‑installer> | bash
     ```
   - The installer will handle dependency resolution and configuration automatically.

3. **GitHub Actions integration**
   - In the repository’s **Settings → Secrets and variables → Actions**, create a new secret named `GROCK_API_KEY`.
   - Populate this secret with the API key obtained from the Grock documentation.
   - The workflow file should reference this secret (e.g., `${{ secrets.GROCK_API_KEY }}`) so that the installer can authenticate with Grock services during the CI run.

4. **Verification**
   - After the script finishes, confirm that the application binaries are available and that any required services are running.
   - In CI, ensure the workflow passes the steps that rely on the `GROCK_API_KEY` secret.

**Key points**
- Use the one‑liner commands to avoid manual download steps.
- The secret must be set at the repository level; it is not exposed in logs.
- The installer scripts are designed to be idempotent, so re‑running them will not cause duplicate installations.
[2026-01-27 14:20:52.157142] [INFO] Starting data splitting...
[2026-01-27 14:20:52.157228] [INFO] Data split into 11 parts based on max symbols 5000.
[2026-01-27 14:20:52.157261] [INFO] Generating answer...
[2026-01-27 14:21:08.927567] [INFO] Generated answer with model openai/gpt-oss-120b.
[2026-01-27 14:21:08.927639] [INFO] Answer: <a name="autodocfile-options"></a>
Define the top‑level keys in the file as follows:

- **project_name** – a string that identifies the project.
- **language** – language code (default “en” if omitted).
- **project_settings** – a block containing:
  - **save_logs** – boolean, true to keep generation logs.
  - **log_level** – numeric level controlling output detail.
- **project_additional_info** – map of arbitrary key/value pairs for extra context (e.g., a global idea description).
- **ignore_files** – optional list of filename patterns that should be skipped.
- **custom_descriptions** – list of strings.  
  - If a string starts with “%”, the leading character is removed and the remainder is processed without additional context.  
  - Otherwise the string is taken as‑is and processed with full context.

Each entry should be written in standard YAML syntax, preserving indentation for nested blocks.
[2026-01-27 14:21:08.927697] [INFO] Module CustomModule generated its part of the documentation.
[2026-01-27 14:21:08.927726] [INFO] Module Output: <a name="autodocfile-options"></a>
Define the top‑level keys in the file as follows:

- **project_name** – a string that identifies the project.
- **language** – language code (default “en” if omitted).
- **project_settings** – a block containing:
  - **save_logs** – boolean, true to keep generation logs.
  - **log_level** – numeric level controlling output detail.
- **project_additional_info** – map of arbitrary key/value pairs for extra context (e.g., a global idea description).
- **ignore_files** – optional list of filename patterns that should be skipped.
- **custom_descriptions** – list of strings.  
  - If a string starts with “%”, the leading character is removed and the remainder is processed without additional context.  
  - Otherwise the string is taken as‑is and processed with full context.

Each entry should be written in standard YAML syntax, preserving indentation for nested blocks.
[2026-01-27 14:21:08.927809] [INFO] Factory documentation generation completed.
[2026-01-27 14:21:08.929289] [INFO] Start ordering
[2026-01-27 14:21:08.929353] [INFO] chanks name: ['#install-workflow-setup', '#autodocfile-options', '#config-reader-responsibility', '#config-reader-interaction', '#config-reader-technical-details', '#config-reader-data-flow', '#runfile-execution', '#configuration-object-construction', '#data-flow-and-side‑effects', '#parentmodel-shared-configuration', '#history-conversation-state', '#gptmodel-synchronous-generation', '#asyncgptmodel-asynchronous-generation', '#docfactory-orchestrating-documentation-generation', '#custommodule-context-aware-intro-generation', '#custommodulewithoutcontext‑static-intro-generation', '#introlinks‑html‑link‑extraction', '#introtext‑global‑context‑introduction', '#manager-orchestration-pipeline', '#html-link-extraction', '#intro-with-links-generation', '#basic-introduction-generation', '#custom-description-loop', '#standalone-custom-description', '#CONTENT_DESCRIPTION', '#anchor-extraction-routine', '#semantic-ordering-workflow', '#repository-mix-builder', '#compress-function', '#batch-compression', '#async-batch-compression', '#iterative-compression', '#description-generation', '#projectsettings-prompt', '#data-splitting-algorithm', '#synchronous-part-doc-generation', '#asynchronous-part-doc-generation', '#batch-doc-generation-sync', '#batch-doc-generation-async', '#logging-structures', '#runtime-logger', '#progress-abstractions', '#project-metadata', '#runtime-dependencies', '#build-system-configuration']
[2026-01-27 14:21:08.929584] [INFO] chanks: {'#install-workflow-setup': '<a name="install-workflow-setup"></a>\n\n**Installation workflow overview**\n\n1. **PowerShell (Windows)**\n   - Open a PowerShell terminal with elevated privileges.\n   - Execute a one‑liner that downloads and runs the remote installer script:\n     ```\n     irm <raw‑url‑to‑powershell‑installer> | iex\n     ```\n   - The script will perform all required setup steps for the Windows environment.\n\n2. **Shell (Linux/macOS)**\n   - Open a terminal.\n   - Run the following one‑liner to fetch and execute the installer for Unix‑like systems:\n     ```\n     curl -sSL <raw‑url‑to‑shell‑installer> | bash\n     ```\n   - The installer will handle dependency resolution and configuration automatically.\n\n3. **GitHub Actions integration**\n   - In the repository’s **Settings → Secrets and variables → Actions**, create a new secret named `GROCK_API_KEY`.\n   - Populate this secret with the API key obtained from the Grock documentation.\n   - The workflow file should reference this secret (e.g., `${{ secrets.GROCK_API_KEY }}`) so that the installer can authenticate with Grock services during the CI run.\n\n4. **Verification**\n   - After the script finishes, confirm that the application binaries are available and that any required services are running.\n   - In CI, ensure the workflow passes the steps that rely on the `GROCK_API_KEY` secret.\n\n**Key points**\n- Use the one‑liner commands to avoid manual download steps.\n- The secret must be set at the repository level; it is not exposed in logs.\n- The installer scripts are designed to be idempotent, so re‑running them will not cause duplicate installations.', '#autodocfile-options': '<a name="autodocfile-options"></a>\nDefine the top‑level keys in the file as follows:\n\n- **project_name** – a string that identifies the project.\n- **language** – language code (default “en” if omitted).\n- **project_settings** – a block containing:\n  - **save_logs** – boolean, true to keep generation logs.\n  - **log_level** – numeric level controlling output detail.\n- **project_additional_info** – map of arbitrary key/value pairs for extra context (e.g., a global idea description).\n- **ignore_files** – optional list of filename patterns that should be skipped.\n- **custom_descriptions** – list of strings.  \n  - If a string starts with “%”, the leading character is removed and the remainder is processed without additional context.  \n  - Otherwise the string is taken as‑is and processed with full context.\n\nEach entry should be written in standard YAML syntax, preserving indentation for nested blocks.', '#config-reader-responsibility': '<a name="config-reader-responsibility"></a>\n## Config‑Reader Responsibility\n`read_config` loads a user‑supplied YAML string, transforms it into a strongly‑typed `Config` object and a list of custom documentation modules. It centralises all project‑wide settings (language, name, ignored files, additional info) and prepares module descriptors for the documentation engine.', '#config-reader-interaction': '<a name="config-reader-interaction"></a>\n## Interaction with Factory Modules\nThe function imports `CustomModule` and `CustomModuleWithOutContext` from `autodocgenerator.factory.modules.general_modules`.  \n* When a description line starts with “%” it is wrapped in `CustomModuleWithOutContext` (context‑free).  \n* Otherwise the line is turned into a regular `CustomModule`.  \nThese objects are later consumed by the rendering pipeline to inject user‑defined sections into the generated markdown.', '#config-reader-technical-details': '<a name="config-reader-technical-details"></a>\n## Technical Details and Logic Flow\n1. **YAML parsing** – `yaml.safe_load` converts the raw string to a Python dict.  \n2. **Base `Config` creation** – a fresh `Config` instance is instantiated.  \n3. **Core fields extraction** – `ignore_files`, `language`, `project_name`, and `project_additional_info` are read with sensible defaults.  \n4. **Project settings handling** – a `ProjectConfigSettings` object (`pcs`) loads the `project_settings` mapping via `pcs.load_settings`.  \n5. **Fluent configuration** – `config.set_language(...).set_project_name(...).set_pcs(pcs)` chains setters for readability.  \n6. **Iterative population** – loops add ignore patterns and additional info via `config.add_ignore_file` and `config.add_project_additional_info`.  \n7. **Custom description conversion** – each entry in `custom_descriptions` is examined; the leading character decides which module class to instantiate.', '#config-reader-data-flow': '<a name="config-reader-data-flow"></a>\n## Input, Output, and Side Effects\n| Aspect | Description |\n|--------|-------------|\n| **Input** | `file_data: str` – raw YAML content supplied by `autodocconfig.yml`. |\n| **Output** | `Tuple[Config, List[CustomModule]]` – a ready‑to‑use configuration object and a collection of module descriptors. |\n| **Side effects** | None; the function is pure aside from importing modules. It assumes the YAML schema contains the keys used above; missing optional keys fall back to defaults. |\n| **Assumptions** | The YAML is well‑formed, custom description strings are non‑empty, and the imported module classes conform to the expected constructor signatures. |\n| **Error handling** | Propagation of `yaml.YAMLError` or any constructor exception to the caller; the runner catches and logs them via the global logger. |\n\nTogether, `read_config` supplies the documentation engine with a complete, validated configuration snapshot and the custom module list required for the subsequent generation steps.', '#runfile-execution': '<a name="runfile-execution"></a>  \n## Run‑file execution flow  \n\n`gen_doc` orchestrates the full documentation pipeline. It instantiates a **synchronous** (`GPTModel`) and an **asynchronous** (`AsyncGPTModel`) LLM client using the global `API_KEY`. These models are injected into a `Manager` together with:  \n\n* `project_path` – root of the source tree.  \n* `config` – a `Config` instance (see *Configuration object construction*).  \n* `ConsoleGtiHubProgress` – CLI progress visualiser.  \n\nThe manager then performs the ordered steps:  \n\n1. `generate_code_file()` – parses the project and creates an internal code representation.  \n2. `generete_doc_parts(max_symbols=5000)` – produces raw documentation fragments, capped at 5\u202fk symbols each.  \n3. `factory_generate_doc(DocFactory(*custom_modules))` – runs user‑supplied modules (e.g., custom extractors).  \n4. `order_doc()` – re‑orders fragments according to logical sections.  \n5. `factory_generate_doc(DocFactory(IntroLinks()))` – adds the autogenerated introductory link block.  \n6. `clear_cache()` – removes temporary artefacts.  \n\nFinally `read_file_by_file_key("output_doc")` returns the assembled markdown document.', '#configuration-object-construction': '<a name="configuration-object-construction"></a>  \n## Configuration object construction  \n\n`Config` aggregates global settings:  \n\n* **ignore_files** – glob patterns excluded from scanning.  \n* **language** – default `"en"`.  \n* **project_name** & **project_additional_info** – descriptive metadata passed to `ProjectSettings`.  \n* **pcs** – a `ProjectConfigSettings` container (log flags).  \n\nConvenient setters (`set_language`, `set_project_name`, `add_ignore_file`, …) enable fluent building.  \n`get_project_settings()` converts the stored metadata into a `ProjectSettings` instance used later by the `Manager`.', '#data-flow-and-side‑effects': '<a name="data-flow-and-side‑effects"></a>  \n## Data flow and side‑effects  \n\n* **Input** – `project_path` (string), a parsed `Config` object, and a list of instantiated custom module objects.  \n* **Processing** – manager writes intermediate code files to the hidden `.auto_doc_cache` directory, updates the console progress bar, and may create temporary log files if `ProjectConfigSettings.save_logs` is true.  \n* **Output** – a single string containing the complete documentation (`output_doc`).  \n* **Assumptions** – a valid `API_KEY` is available, custom modules implement the `BaseModule` interface expected by `DocFactory`, and the project directory is readable.  \n\nThese steps provide the validated configuration snapshot and the custom module list required for the downstream generation engine.', '#parentmodel-shared-configuration': '<a name="parentmodel-shared-configuration"></a>  \n## ParentModel – Shared Model Configuration  \n\n`ParentModel` centralises authentication, history handling, and model‑selection logic. It receives an `api_key`, a `History` instance, and a `use_random` flag. The constructor shuffles the global `MODELS_NAME` list (when randomised) and stores the ordered copy in `self.regen_models_name`. Index tracking (`self.current_model_index`) enables round‑robin fallback when a model fails.\n\n---', '#history-conversation-state': '<a name="history-conversation-state"></a>  \n## History – Conversation State Management  \n\n`History` builds the message array sent to the LLM. It auto‑injects the system prompt (`BASE_SYSTEM_TEXT`) and provides `add_to_history(role, content)` to append user or assistant entries. The `self.history` list is consumed directly by the model classes.\n\n---', '#gptmodel-synchronous-generation': '<a name="gptmodel-synchronous-generation"></a>  \n## GPTModel – Synchronous Answer Generation  \n\nInherits from `Model` (which extends `ParentModel`).  \n* **Responsibility** – Wraps the Groq synchronous client (`Groq`) to produce a completion.  \n* **Interaction** – Uses `self.client.chat.completions.create(messages, model)`; falls back through `self.regen_models_name` on exception, logging each attempt via `BaseLogger`.  \n* **Data Flow** –  \n  1. Chooses `messages` from `self.history.history` or a raw `prompt`.  \n  2. Calls the Groq API, extracts `choices[0].message.content`.  \n  3. Logs generation steps and returns the answer.\n\n---', '#asyncgptmodel-asynchronous-generation': '<a name="asyncgptmodel-asynchronous-generation"></a>  \n## AsyncGPTModel – Asynchronous Answer Generation  \n\nMirrors `GPTModel` but operates with `AsyncGroq`.  \n* **Responsibility** – Provides `async generate_answer` for non‑blocking calls.  \n* **Interaction** – Awaits `self.client.chat.completions.create`. On failure, logs a warning and advances `self.current_model_index`.  \n* **Data Flow** – Same as the synchronous variant, but returns a coroutine result and logs at level\u202f2 for the final answer.\n\nBoth model classes rely on the shared fallback mechanism defined in `ParentModel`, ensuring resilience when a specific model endpoint is unavailable.', '#docfactory-orchestrating-documentation-generation': '<a name="docfactory-orchestrating-documentation-generation"></a>  \n## DocFactory – Orchestrating Documentation Generation  \n\n* **Responsibility** – Accepts an ordered list of `BaseModule` instances and sequentially invokes each to produce a composite documentation string.  \n* **Interaction** –  \n  * Receives a `Model` (or `AsyncModel`) and a `BaseProgress` UI object from the caller.  \n  * Calls `module.generate(info, model)` for every injected module.  \n  * Emits `InfoLog` entries via the shared `BaseLogger`, forwarding the module name and its raw output (level\u202f2).  \n* **Technical Details** –  \n  * Stores modules in `self.modules: list[BaseModule]`.  \n  * Initializes a simple logger (`BaseLogger()`).  \n  * Uses `progress.create_new_subtask` to allocate a sub‑task sized to the module count, `progress.update_task` after each module, and `progress.remove_subtask` on completion.  \n* **Data Flow** –  \n  1. Input: `info` dict, `model` instance, `progress` UI.  \n  2. For each module: `module.generate` returns a string → concatenated to `output`.  \n  3. Side‑effects: logging, progress bar updates.  \n  4. Output: the final assembled documentation string.', '#custommodule-context-aware-intro-generation': '<a name="custommodule-context-aware-intro-generation"></a>  \n## CustomModule – Context‑Aware Intro Generation  \n\n* **Responsibility** – Generates a custom introductory paragraph that incorporates a user‑provided description and a code‑snippet excerpt.  \n* **Interaction** – Calls `split_data` to truncate `info["code_mix"]` to ≤\u202f5\u202f000 symbols, then forwards the split text, the model, the description, and the target language to `generete_custom_discription`.  \n* **Technical Details** –  \n  * Inherits from `BaseModule`.  \n  * Holds `self.discription` (typo retained from original).  \n  * `generate` returns the processed string from the post‑processor.  \n\n* **Data Flow** –  \n  1. Input: `info` dict (keys `code_mix`, `language`), `model`.  \n  2. Processing: `split_data → generete_custom_discription`.  \n  3. Output: formatted intro paragraph.', '#custommodulewithoutcontext‑static-intro-generation': '<a name="custommodulewithoutcontext‑static-intro-generation"></a>  \n## CustomModuleWithOutContext – Static Intro Generation  \n\n* **Responsibility** – Produces a description‑only intro, omitting any code context.  \n* **Interaction** – Directly invokes `generete_custom_discription_without` with the model, stored description, and language.  \n* **Data Flow** – Input: `info["language"]`, `model`; Output: static intro string; No side‑effects beyond the returned text.', '#introlinks‑html‑link‑extraction': '<a name="introlinks‑html‑link‑extraction"></a>  \n## IntroLinks – HTML Link Extraction for Documentation  \n\n* **Responsibility** – Extracts all HTML links from `info["full_data"]` and synthesizes a brief introductory block about those links.  \n* **Interaction** – Uses `get_all_html_links` → `get_links_intro`, passing the model and language.', '#introtext‑global‑context‑introduction': '<a name="introtext‑global‑context‑introduction"></a>  \n## IntroText – Global Context Introduction  \n\n* **Responsibility** – Generates a high‑level introduction based on `info["global_data"]`.  \n* **Interaction** – Calls `get_introdaction` with the model and language, returning the result.', '#manager-orchestration-pipeline': '<a name="manager-orchestration-pipeline"></a>\n## Manager – Orchestration of Documentation Pipeline  \n\n* **Responsibility** – Coordinates the end‑to‑end generation of documentation: builds the code‑mix cache, splits large sources, invokes the factory‑based module chain, and orders the final output.  \n\n* **Interaction** –  \n  * Accepts a project root, a `Config` object, optional synchronous (`Model`) and asynchronous (`AsyncModel`) language models, and a UI progress handler (`BaseProgress`).  \n  * Utilises `BaseLogger` (file‑backed via `FileLoggerTemplate`) for all informational, warning and error logs.  \n  * Calls external preprocessors (`split_data`, `gen_doc_parts`, `compress_to_one`), post‑processors (`get_introdaction`, `get_all_html_links`, …) and the injected `DocFactory` to produce the final markdown.  \n\n* **Technical Details** –  \n  * Constants `CACHE_FOLDER_NAME` and `FILE_NAMES` define persistent cache locations (`code_mix.txt`, `global_info.md`, `report.txt`, `output_doc.md`).  \n  * `__init__` creates the cache folder, configures the logger, and stores references to models and progress UI.  \n  * Core methods:  \n    - `generate_code_file` → builds a `CodeMix` instance, writes the mixed source to cache, updates progress.  \n    - `generete_doc_parts` → streams the cached mix through `gen_doc_parts`, writes the partial markdown, logs progress.  \n    - `factory_generate_doc` → loads current output and code mix, assembles an `info` dict, logs the module list, invokes `doc_factory.generate_doc`, prepends the new fragment to the existing document.  \n    - `order_doc` → splits the markdown by anchor markers, asks the synchronous model to reorder sections via `get_order`, and rewrites the file.  \n    - `clear_cache` → optionally removes the log file based on `config.pcs.save_logs`.  \n\n* **Data Flow** –  \n  1. **Input**: project directory path, `Config`, optional models, UI progress object.  \n  2. **Processing**:  \n     - `CodeMix.build_repo_content` → writes raw source to `code_mix.txt`.  \n     - `gen_doc_parts` → consumes the mix, returns a partial markdown string.  \n     - `DocFactory.generate_doc` → receives `info` (`language`, `full_data`, `code_mix`), calls each module’s `generate`, aggregates results, updates progress.  \n     - `split_text_by_anchors` / `get_order` → reorders sections.  \n  3. **Side‑effects**: file system writes (cache files, final `output_doc.md`), logger entries, progress‑bar updates.  \n  4. **Output**: a fully assembled `output_doc.md` containing ordered documentation ready for consumption.\n\n**', '#html-link-extraction': '<a name="html-link-extraction"></a>HTML‑Link Extraction**  \nThe function `get_all_html_links` scans a markdown string for anchor tags of the form `<a name="…"></a>`. It logs the start/end of the operation, builds a list of fragment identifiers prefixed with “#”, and returns that list.  \n\n*Inputs*: `data` – full documentation text.  \n*Outputs*: `list[str]` – collected anchors (e.g., `["#section‑overview"]`).  \n*Side‑effects*: Writes two `InfoLog` entries via `BaseLogger`.  \n\n---\n\n**', '#intro-with-links-generation': '<a name="intro-with-links-generation"></a>Intro‑With‑Links Generation**  \n`get_links_intro` receives the anchor list, a `Model` implementation, and an optional language code. It composes a three‑message system‑user prompt (language hint, `BASE_INTRODACTION_CREATE_LINKS` template, and the raw link list) and calls `model.get_answer_without_history`.  \n\n*Inputs*: `links` (list of anchors), `model` (concrete `Model`/`GPTModel`), `language`.  \n*Outputs*: String containing a generated introductory paragraph that references the provided links.  \n*Side‑effects*: Logs start/completion messages and the raw LLM response.  \n\n---\n\n**', '#basic-introduction-generation': '<a name="basic-introduction-generation"></a>Basic Introduction Generation**  \n`get_introdaction` (typo intentional) builds a similar prompt using `BASE_INTRO_CREATE` and the full documentation (`global_data`). It returns the LLM‑generated introductory block.  \n\n*Inputs*: `global_data` (full docs), `model`, `language`.  \n*Outputs*: Introduction string.  \n\n---\n\n**', '#custom-description-loop': '<a name="custom-description-loop"></a>Custom Description Loop**  \n`generete_custom_discription` iterates over a pre‑split collection of documentation fragments (`splited_data`). For each fragment it sends a detailed system prompt (technical analyst role, context snippet, `BASE_CUSTOM_DISCRIPTIONS`) together with the user‑requested `custom_description`. The loop breaks when the LLM returns a substantive answer (i.e., does not contain “!noinfo” or “No information found”). The final `result` is returned.  \n\n*Inputs*: `splited_data` (iterable of strings), `model`, `custom_description`, `language`.  \n*Outputs*: Custom description text for the first fragment that yields valid content.  \n\n---\n\n**', '#standalone-custom-description': '<a name="standalone-custom-description"></a>Standalone Custom Description**  \n`generete_custom_discription_without` builds a single‑shot prompt that forces the LLM to prepend a strict `', '#CONTENT_DESCRIPTION': '<a name="CONTENT_DESCRIPTION"></a>` tag and obey naming constraints (no filenames, extensions, generic terms, or URLs). The result is returned directly.  \n\n*Inputs*: `model`, `custom_description`, `language`.  \n*Outputs*: Tagged description string.  \n\n---\n\n**Interaction Summary**  \nAll functions rely on the shared `BaseLogger` for traceability and on the abstract `Model` interface (concretely `GPTModel`) to invoke the LLM. They do **not** perform I/O beyond logging; the calling post‑processor aggregates their outputs into the final documentation file.', '#anchor-extraction-routine': '<a name="anchor-extraction-routine"></a>  \n## Anchor Extraction Routine (sorting.py)\n\n**Responsibility** – Detect HTML‑style anchors at the start of each markdown chunk and build a mapping `anchor → chunk`.  \n\n**Interactions** – Consumes raw README text, feeds the resulting dictionary to `get_order`. Relies on `BaseLogger` for diagnostic output.  \n\n**Technical Details** – `extract_links_from_start` uses a regex `^<a name=["\']?(.*?)["\']?</a>` to capture names longer than five characters, prefixing them with `#`. `split_text_by_anchors` splits the document on a look‑ahead pattern, trims empties, validates a one‑to‑one link‑chunk relationship, and returns `{anchor: chunk}` or `None`.  \n\n**Data Flow** – Input: `text` (str). Output: `dict[str, str]` where keys are `#anchor`. Side‑effects: logging of chunk names and content; early exit if counts mismatch.', '#semantic-ordering-workflow': '<a name="semantic-ordering-workflow"></a>  \n## Semantic Ordering Workflow (sorting.py)\n\n**Responsibility** – Request an LLM to reorder section titles semantically and concatenate the associated chunks in that order.  \n\n**Interactions** – Receives the anchor‑chunk map, sends a prompt to `Model.get_answer_without_history`, then assembles `order_output`.  \n\n**Technical Details** – Constructs a user‑role prompt containing the list of titles, parses the comma‑separated LLM response, logs each step, and iterates over the ordered titles to build the final markdown string.  \n\n**Data Flow** – Input: `Model` instance, `chanks` dict. Output: ordered markdown string. Side‑effects: extensive logging.', '#repository-mix-builder': '<a name="repository-mix-builder"></a>  \n## Repository Mix Builder (code_mix.py)\n\n**Responsibility** – Walk a repository, emit a hierarchical file‑tree, and concatenate file contents into a single “repo mix” artifact while respecting ignore patterns.  \n\n**Interactions** – Utilises `Path.rglob` for traversal, `should_ignore` for pattern checks, and `BaseLogger` for progress messages.  \n\n**Technical Details** – `should_ignore` matches a path against user‑supplied glob patterns across the full relative path, basename, and individual parts. `build_repo_content` writes a tree view, inserts a delimiter, then writes each non‑ignored file wrapped in `<file path="...">` tags, handling read errors gracefully.  \n\n**Data Flow** – Input: `root_dir` (Path), `ignore_patterns` (list). Output: `repomix-output.txt` (text file). Side‑effects: file creation, console logs, error annotations inside the output file.', '#compress-function': '<a name="compress-function"></a>  \n## `compress` – single‑pass text reduction  \n\n**Responsibility**  \nAccepts raw source `data`, builds a three‑message prompt (system settings, compression template, user payload) and returns the model’s answer, which is the compressed representation of the input.  \n\n**Interactions**  \n- Uses `ProjectSettings.prompt` for the contextual system message.  \n- Calls `get_BASE_COMPRESS_TEXT` to obtain a size‑aware compression instruction.  \n- Delegates the LLM request to any `Model` implementation (`GPTModel`, custom adapters).  \n\n**Technical notes**  \n- No async I/O; the model call is synchronous via `model.get_answer_without_history`.  \n- Returns a plain string; callers are responsible for further aggregation.', '#batch-compression': '<a name="batch-compression"></a>  \n## `compress_and_compare` – synchronous batch aggregator  \n\n**Responsibility**  \nSplits a list of file contents into groups of `compress_power` items, compresses each element with `compress`, concatenates the results per group, and tracks progress.  \n\n**Data flow**  \n- **Input:** `data` (list of strings), `model`, `project_settings`, optional `compress_power`, `progress_bar`.  \n- **Output:** List of aggregated compressed chunks, length ≈\u202f⌈len(data)/compress_power⌉.  \n- **Side‑effects:** Updates `progress_bar` (sub‑task creation, per‑item updates, removal).  \n\n**Logic**  \n1. Pre‑allocate a result list sized to the number of groups.  \n2. For each element, compute its group index `i // compress_power`, append the compressed string plus newline.  \n3. Progress bar reflects each processed element.', '#async-batch-compression': '<a name="async-batch-compression"></a>  \n## `async_compress_and_compare` – parallel batch compressor  \n\n**Responsibility**  \nPerforms the same grouping as `compress_and_compare` but launches up to four concurrent LLM calls using `asyncio.Semaphore`.  \n\n**Key steps**  \n- Creates a semaphore (limit\u202f=\u202f4) and a task list.  \n- Each element is wrapped in `async_compress`, which builds the identical prompt and awaits `model.get_answer_without_history`.  \n- After `asyncio.gather`, results are re‑chunked according to `compress_power` and concatenated with newlines.  \n\n**Data flow**  \n- **Input/Output:** Same as the synchronous version, but returns a coroutine result.  \n- **Side‑effects:** Progress bar updates inside `async_compress`; sub‑task lifecycle mirrors the sync variant.', '#iterative-compression': '<a name="iterative-compression"></a>  \n## `compress_to_one` – iterative reduction to a single payload  \n\n**Responsibility**  \nRepeatedly compresses the dataset until only one aggregated chunk remains, optionally using the async pipeline.  \n\n**Algorithm**  \n- While `len(data) > 1`, choose an effective `compress_power` (fallback\u202f=\u202f2 for small lists).  \n- Call either `async_compress_and_compare` (via `asyncio.run`) or `compress_and_compare`.  \n- Increment iteration counter; final output is `data[0]`.', '#description-generation': '<a name="description-generation"></a>  \n## `generate_discribtions_for_code` – LLM‑driven documentation builder  \n\n**Responsibility**  \nFeeds each source file to the model with a strict instruction prompt, collects the model’s descriptive answer, and reports progress.  \n\n**Data flow**  \n- **Input:** List of code strings, `model`, `project_settings`, `progress_bar`.  \n- **Output:** List of description strings (same order as input).  \n- **Side‑effects:** Progress bar sub‑task updates.', '#projectsettings-prompt': '<a name="projectsettings-prompt"></a>  \n## `ProjectSettings.prompt` – dynamic system context  \n\n**Responsibility**  \nConstructs a multi‑line system prompt by concatenating a base template (`BASE_SETTINGS_PROMPT`) with the project name and any key‑value pairs added via `add_info`.  \n\n**Technical details**  \n- `info` is a mutable dict; `add_info` inserts or overwrites entries.  \n- Property accessor lazily builds the prompt each call, ensuring the latest `info` content is reflected.  \n\n**Interactions**  \n- Consumed by every compression or description function to provide project‑specific background to the LLM.', '#data-splitting-algorithm': '<a name="data-splitting-algorithm"></a>  \n## `split_data` – chunking source text to fit token limits  \n\n**Responsibility**  \nBreaks a monolithic code string into a list of substrings whose lengths stay below `max_symbols` while preserving line boundaries.  \n\n**Interactions**  \nUses `BaseLogger` for progress messages; no external state is read or modified.  \n\n**Technical details**  \n1. Initial split on newline (`"\\n"`).  \n2. Repeatedly scans `splited_by_files`; any element exceeding `1.5\u202f×\u202fmax_symbols` is bisected at `max_symbols/2` and re‑inserted, forcing convergence.  \n3. A second pass packs the trimmed fragments into `split_objects`, starting a new part whenever the accumulated length would exceed `1.25\u202f×\u202fmax_symbols`.  \n\n**Data flow**  \n- **Input:** `data: str`, `max_symbols: int`.  \n- **Output:** `list[str]` – ordered chunks ready for LLM consumption.  \n- **Side‑effects:** One `InfoLog` entry before and after splitting.', '#synchronous-part-doc-generation': '<a name="synchronous-part-doc-generation"></a>  \n## `write_docs_by_parts` – generate documentation for a single chunk (sync)  \n\n**Responsibility**  \nBuilds a system‑user prompt, sends it to a synchronous `Model`, and returns the cleaned LLM answer.  \n\n**Interactions**  \nRelies on `BASE_PART_COMPLITE_TEXT` (static instruction), optional `prev_info` (context from the previous chunk), and `BaseLogger`.  \n\n**Technical details**  \n- Prompt composition: two mandatory system messages (language + part ID, base instruction) + optional previous‑part context + user message containing the code chunk.  \n- Calls `model.get_answer_without_history`.  \n- Strips surrounding Markdown fences (```), logs answer length and full content (debug level\u202f2).  \n\n**Data flow**  \n- **Input:** `part_id: int`, `part: str`, `model: Model`, `prev_info: str | None`, `language: str`.  \n- **Output:** Cleaned documentation string for the chunk.  \n- **Side‑effects:** Logging; no mutation of arguments.', '#asynchronous-part-doc-generation': '<a name="asynchronous-part-doc-generation"></a>  \n## `async_write_docs_by_parts` – concurrent chunk documentation (async)  \n\n**Responsibility**  \nSame prompt logic as the sync variant but executed within an `asyncio.Semaphore` to limit parallel LLM calls.  \n\n**Interactions**  \nAccepts an `AsyncModel`, optional `update_progress` callback, and shares the same logger.  \n\n**Technical details**  \n- `async with semaphore` protects the call to `async_model.get_answer_without_history`.  \n- After obtaining the answer, optionally invokes `update_progress` (used by the surrounding progress bar).  \n- Performs identical back‑tick stripping and logging.  \n\n**Data flow**  \n- **Input:** `part: str`, `async_model: AsyncModel`, `global_info: str`, `semaphore`, `prev_info: str | None`, `language: str`, `update_progress: Callable | None`.  \n- **Output:** Cleaned documentation string (coroutine result).  \n- **Side‑effects:** Logging; may update external progress bar via callback.', '#batch-doc-generation-sync': '<a name="batch-doc-generation-sync"></a>  \n## `gen_doc_parts` – orchestrated synchronous documentation pipeline  \n\n**Responsibility**  \nSplits the full code, sequentially processes each part with `write_docs_by_parts`, aggregates results, and maintains a sub‑task on a `BaseProgress` bar.  \n\n**Interactions**  \nCalls `split_data`, creates/updates/removes a sub‑task on `progress_bar`, and logs high‑level milestones.  \n\n**Technical details**  \n- After each part, retains the last\u202f3000\u202fcharacters of the generated text as `prev_info` to provide context for the next iteration.  \n- Concatenates all part outputs separated by blank lines.  \n\n**Data flow**  \n- **Input:** `full_code_mix: str`, `max_symbols: int`, `model: Model`, `language: str`, `progress_bar: BaseProgress`.  \n- **Output:** Single string containing the complete documentation.  \n- **Side‑effects:** Progress‑bar sub‑task lifecycle, extensive logging.', '#batch-doc-generation-async': '<a name="batch-doc-generation-async"></a>  \n## `async_gen_doc_parts` – orchestrated asynchronous documentation pipeline  \n\n**Responsibility**  \nParallel version of `gen_doc_parts`; dispatches `async_write_docs_by_parts` for every split chunk, respecting a concurrency limit of four.  \n\n**Interactions**  \nCreates a semaphore, builds a list of coroutines, gathers them, and updates the same `BaseProgress` sub‑task via a lambda passed to each async worker.  \n\n**Technical details**  \n- Uses `asyncio.gather` to await all part‑level tasks.  \n- Result aggregation mirrors the sync version (blank‑line separation).  \n\n**Data flow**  \n- **Input:** `full_code_mix: str`, `global_info: str`, `max_symbols: int`, `model: AsyncModel`, `language: str`, `progress_bar: BaseProgress`.  \n- **Output:** Full documentation string (awaited coroutine).  \n- **Side‑effects:** Progress‑bar sub‑task updates, logging, semaphore enforcement.', '#logging-structures': '<a name="logging-structures"></a>\n## `BaseLog` hierarchy – structured log messages  \n\n**Responsibility**  \nDefines lightweight log objects (`BaseLog`, `ErrorLog`, `WarningLog`, `InfoLog`) that prepend a timestamp and level tag to a user‑supplied message.  \n\n**Interactions**  \nInstances are passed to a `BaseLoggerTemplate` via its `log` method; the template decides whether to emit the message based on its configured `log_level`.  \n\n**Technical details**  \n- `BaseLog` stores `message` and numeric `level`; its `format` returns the raw text.  \n- `_log_prefix` property builds `"[YYYY‑MM‑DD HH:MM:SS]"` using `datetime.fromtimestamp(time.time())`.  \n- Sub‑classes override `format` to insert `[ERROR]`, `[WARNING]`, or `[INFO]` after the prefix.  \n\n**Data flow**  \n- **Input:** `message: str`, optional `level: int`.  \n- **Output:** Formatted string via `format()`.  \n- **Side‑effects:** None (pure data object).', '#runtime-logger': '<a name="runtime-logger"></a>\n## `BaseLogger` singleton – runtime logging façade  \n\n**Responsibility**  \nProvides a globally accessible logger that delegates to a concrete `BaseLoggerTemplate` (console, file, etc.).  \n\n**Interactions**  \n- `set_logger` injects a concrete template (`BaseLoggerTemplate`, `FileLoggerTemplate`, …).  \n- Calls to `log` forward the `BaseLog` instance to the template’s `global_log`, which respects the template’s `log_level`.  \n\n**Technical details**  \n- Implements the classic singleton pattern in `__new__` to guarantee a single shared instance across the process.  \n- `global_log` in `BaseLoggerTemplate` checks `log_level` (`-1` disables filtering).  \n\n**Data flow**  \n- **Input:** `BaseLog` object.  \n- **Output:** Printed to stdout or appended to a file, depending on the active template.  \n- **Side‑effects:** I/O (stdout or file write).', '#progress-abstractions': '<a name="progress-abstractions"></a>\n## `BaseProgress` abstraction and concrete implementations  \n\n**Responsibility**  \nOffers a minimal API (`create_new_subtask`, `update_task`, `remove_subtask`) for tracking hierarchical progress in different environments.  \n\n**Interactions**  \n- `LibProgress` wraps Rich’s `Progress` object, exposing Rich‑based visual feedback.  \n- `ConsoleGtiHubProgress` uses simple `print` statements via `ConsoleTask` for environments without Rich.  \n\n**Technical details**  \n- `LibProgress` creates a base task (`General progress`) and optional sub‑tasks; `update_task` advances either the sub‑task or base task.  \n- `ConsoleTask` maintains a counter and prints a percentage on each `progress` call.  \n- `ConsoleGtiHubProgress` delegates to a `ConsoleTask` for sub‑tasks, falling back to a `ConsoleTask` representing the overall progress.  \n\n**Data flow**  \n- **Input:** Sub‑task name and total length.  \n- **Output:** Visual progress updates on the console or Rich UI.  \n- **Side‑effects:** stdout writes (or Rich UI updates).  \n\nThese components together furnish a lightweight, interchangeable logging and progress‑tracking subsystem used throughout the autodocgenerator package.', '#project-metadata': '<a name="project-metadata"></a>\n## `pyproject.toml` – package identity & version constraints  \n\n**Responsibility**  \nEncapsulates the canonical package descriptor for **autodocgenerator**: name, semantic version, human‑readable description, author contact, license, and the supported Python range.  \n\n**Interactions**  \n- Read by *Poetry* (or any PEP\u202f517‑compatible builder) to resolve the project’s identity during `poetry install`, `poetry build`, and wheel creation.  \n- Exposed to downstream tools (e.g., `pip`, `build`) via the generated `dist/*.whl` metadata.  \n\n**Technical details**  \n- `name = "autodocgenerator"` and `version = "0.8.9.9"` follow PEP\u202f621 naming rules.  \n- `description` and `readme` feed the **Core Metadata** fields of the resulting distribution.  \n- `authors` is a list of `{"name": "...", "email": "..."}` objects, converted to the `Author` field in the wheel.  \n- `license = {text = "MIT"}` supplies an SPDX‑compatible license string.  \n\n**Data flow**  \n- **Input:** Static TOML literals edited by maintainers.  \n- **Output:** Serialized metadata consumed at build time; no runtime side‑effects.', '#runtime-dependencies': '<a name="runtime-dependencies"></a>\n## Dependency enumeration – third‑party runtime requirements  \n\n**Responsibility**  \nLists exact version pins required for the library to operate, ensuring reproducible environments across Python\u202f3.11–3.12.  \n\n**Interactions**  \n- Resolved by Poetry’s dependency resolver; each entry becomes a `Requires-Dist` clause in the wheel metadata.  \n- At installation, `pip` (via the wheel) pulls the same versions, guaranteeing API stability.  \n\n**Technical details**  \n- All entries follow the `package==major.minor.patch` syntax, e.g., `rich==14.2.0`.  \n- The list covers HTTP clients (`httpx`), authentication (`google-auth`), AI SDKs (`openai`, `groq`), data handling (`pyyaml`, `msgmsg`), and utilities (`rich_progress`, `tqdm`).  \n- No optional groups are defined; every line is a mandatory runtime requirement.  \n\n**Data flow**  \n- **Input:** Version‑locked strings.  \n- **Output:** Dependency graph constructed by the resolver; installed files appear in `site‑packages`.', '#build-system-configuration': '<a name="build-system-configuration"></a>\n## Build‑system stanza – PEP\u202f517 backend specification  \n\n**Responsibility**  \nDeclares the toolchain needed to produce a distributable artifact from the source.  \n\n**Interactions**  \n- `requires = ["poetry-core>=2.0.0"]` tells the build frontend to fetch *poetry‑core* before invoking the backend.  \n- `build-backend = "poetry.core.masonry.api"` points to the entry‑point that implements `build_wheel`, `build_sdist`, etc.  \n\n**Technical details**  \n- The stanza is minimal; no custom build steps are injected, relying on Poetry’s default isolation and caching mechanisms.  \n\n**Data flow**  \n- **Input:** The declared backend package.  \n- **Output:** A built wheel or source distribution emitted to `dist/`; no side‑effects beyond temporary build directories.'}
[2026-01-27 14:21:08.929728] [INFO] Generating answer...
[2026-01-27 14:21:08.979965] [WARNING] Model openai/gpt-oss-120b failed with error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-120b` in organization `org_01kecdfh2cfyxr28aqdrxxzkhq` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 200000, Requested 436. Please try again in 3m8.352s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}. Trying next model...
[2026-01-27 14:21:09.731573] [INFO] Generated answer with model llama-3.3-70b-versatile.
[2026-01-27 14:21:09.731632] [INFO] Answer: #install-workflow-setup, #runfile-execution, #manager-orchestration-pipeline, #semantic-ordering-workflow, #config-reader-responsibility, #config-reader-interaction, #config-reader-technical-details, #config-reader-data-flow, #configuration-object-construction, #data-flow-and-side‑effects, #parentmodel-shared-configuration, #history-conversation-state, #gptmodel-synchronous-generation, #asyncgptmodel-asynchronous-generation, #docfactory-orchestrating-documentation-generation, #custommodule-context-aware-intro-generation, #custommodulewithoutcontext‑static-intro-generation, #introlinks‑html‑link‑extraction, #introtext‑global‑context‑introduction, #intro-with-links-generation, #basic-introduction-generation, #custom-description-loop, #standalone-custom-description, #CONTENT_DESCRIPTION, #anchor-extraction-routine, #description-generation, #html-link-extraction, #introlinks‑html‑link‑extraction, #autodocfile-options, #projectsettings-prompt, #data-splitting-algorithm, #synchronous-part-doc-generation, #asynchronous-part-doc-generation, #batch-doc-generation-sync, #batch-doc-generation-async, #repository-mix-builder, #compress-function, #batch-compression, #async-batch-compression, #iterative-compression, #logging-structures, #runtime-logger, #progress-abstractions, #project-metadata, #runtime-dependencies, #build-system-configuration
[2026-01-27 14:21:09.731696] [INFO] End ordering result list ['#install-workflow-setup', '#runfile-execution', '#manager-orchestration-pipeline', '#semantic-ordering-workflow', '#config-reader-responsibility', '#config-reader-interaction', '#config-reader-technical-details', '#config-reader-data-flow', '#configuration-object-construction', '#data-flow-and-side‑effects', '#parentmodel-shared-configuration', '#history-conversation-state', '#gptmodel-synchronous-generation', '#asyncgptmodel-asynchronous-generation', '#docfactory-orchestrating-documentation-generation', '#custommodule-context-aware-intro-generation', '#custommodulewithoutcontext‑static-intro-generation', '#introlinks‑html‑link‑extraction', '#introtext‑global‑context‑introduction', '#intro-with-links-generation', '#basic-introduction-generation', '#custom-description-loop', '#standalone-custom-description', '#CONTENT_DESCRIPTION', '#anchor-extraction-routine', '#description-generation', '#html-link-extraction', '#introlinks‑html‑link‑extraction', '#autodocfile-options', '#projectsettings-prompt', '#data-splitting-algorithm', '#synchronous-part-doc-generation', '#asynchronous-part-doc-generation', '#batch-doc-generation-sync', '#batch-doc-generation-async', '#repository-mix-builder', '#compress-function', '#batch-compression', '#async-batch-compression', '#iterative-compression', '#logging-structures', '#runtime-logger', '#progress-abstractions', '#project-metadata', '#runtime-dependencies', '#build-system-configuration']
[2026-01-27 14:21:09.731726] [INFO] Add to <a name="install-workflow-setup"></a>

**Installation workflow overview**

1. **PowerShell (Windows)**
   - Open a PowerShell terminal with elevated privileges.
   - Execute a one‑liner that downloads and runs the remote installer script:
     ```
     irm <raw‑url‑to‑powershell‑installer> | iex
     ```
   - The script will perform all required setup steps for the Windows environment.

2. **Shell (Linux/macOS)**
   - Open a terminal.
   - Run the following one‑liner to fetch and execute the installer for Unix‑like systems:
     ```
     curl -sSL <raw‑url‑to‑shell‑installer> | bash
     ```
   - The installer will handle dependency resolution and configuration automatically.

3. **GitHub Actions integration**
   - In the repository’s **Settings → Secrets and variables → Actions**, create a new secret named `GROCK_API_KEY`.
   - Populate this secret with the API key obtained from the Grock documentation.
   - The workflow file should reference this secret (e.g., `${{ secrets.GROCK_API_KEY }}`) so that the installer can authenticate with Grock services during the CI run.

4. **Verification**
   - After the script finishes, confirm that the application binaries are available and that any required services are running.
   - In CI, ensure the workflow passes the steps that rely on the `GROCK_API_KEY` secret.

**Key points**
- Use the one‑liner commands to avoid manual download steps.
- The secret must be set at the repository level; it is not exposed in logs.
- The installer scripts are designed to be idempotent, so re‑running them will not cause duplicate installations.
[2026-01-27 14:21:09.731764] [INFO] Add to <a name="runfile-execution"></a>  
## Run‑file execution flow  

`gen_doc` orchestrates the full documentation pipeline. It instantiates a **synchronous** (`GPTModel`) and an **asynchronous** (`AsyncGPTModel`) LLM client using the global `API_KEY`. These models are injected into a `Manager` together with:  

* `project_path` – root of the source tree.  
* `config` – a `Config` instance (see *Configuration object construction*).  
* `ConsoleGtiHubProgress` – CLI progress visualiser.  

The manager then performs the ordered steps:  

1. `generate_code_file()` – parses the project and creates an internal code representation.  
2. `generete_doc_parts(max_symbols=5000)` – produces raw documentation fragments, capped at 5 k symbols each.  
3. `factory_generate_doc(DocFactory(*custom_modules))` – runs user‑supplied modules (e.g., custom extractors).  
4. `order_doc()` – re‑orders fragments according to logical sections.  
5. `factory_generate_doc(DocFactory(IntroLinks()))` – adds the autogenerated introductory link block.  
6. `clear_cache()` – removes temporary artefacts.  

Finally `read_file_by_file_key("output_doc")` returns the assembled markdown document.
[2026-01-27 14:21:09.731789] [INFO] Add to <a name="manager-orchestration-pipeline"></a>
## Manager – Orchestration of Documentation Pipeline  

* **Responsibility** – Coordinates the end‑to‑end generation of documentation: builds the code‑mix cache, splits large sources, invokes the factory‑based module chain, and orders the final output.  

* **Interaction** –  
  * Accepts a project root, a `Config` object, optional synchronous (`Model`) and asynchronous (`AsyncModel`) language models, and a UI progress handler (`BaseProgress`).  
  * Utilises `BaseLogger` (file‑backed via `FileLoggerTemplate`) for all informational, warning and error logs.  
  * Calls external preprocessors (`split_data`, `gen_doc_parts`, `compress_to_one`), post‑processors (`get_introdaction`, `get_all_html_links`, …) and the injected `DocFactory` to produce the final markdown.  

* **Technical Details** –  
  * Constants `CACHE_FOLDER_NAME` and `FILE_NAMES` define persistent cache locations (`code_mix.txt`, `global_info.md`, `report.txt`, `output_doc.md`).  
  * `__init__` creates the cache folder, configures the logger, and stores references to models and progress UI.  
  * Core methods:  
    - `generate_code_file` → builds a `CodeMix` instance, writes the mixed source to cache, updates progress.  
    - `generete_doc_parts` → streams the cached mix through `gen_doc_parts`, writes the partial markdown, logs progress.  
    - `factory_generate_doc` → loads current output and code mix, assembles an `info` dict, logs the module list, invokes `doc_factory.generate_doc`, prepends the new fragment to the existing document.  
    - `order_doc` → splits the markdown by anchor markers, asks the synchronous model to reorder sections via `get_order`, and rewrites the file.  
    - `clear_cache` → optionally removes the log file based on `config.pcs.save_logs`.  

* **Data Flow** –  
  1. **Input**: project directory path, `Config`, optional models, UI progress object.  
  2. **Processing**:  
     - `CodeMix.build_repo_content` → writes raw source to `code_mix.txt`.  
     - `gen_doc_parts` → consumes the mix, returns a partial markdown string.  
     - `DocFactory.generate_doc` → receives `info` (`language`, `full_data`, `code_mix`), calls each module’s `generate`, aggregates results, updates progress.  
     - `split_text_by_anchors` / `get_order` → reorders sections.  
  3. **Side‑effects**: file system writes (cache files, final `output_doc.md`), logger entries, progress‑bar updates.  
  4. **Output**: a fully assembled `output_doc.md` containing ordered documentation ready for consumption.

**
[2026-01-27 14:21:09.731820] [INFO] Add to <a name="semantic-ordering-workflow"></a>  
## Semantic Ordering Workflow (sorting.py)

**Responsibility** – Request an LLM to reorder section titles semantically and concatenate the associated chunks in that order.  

**Interactions** – Receives the anchor‑chunk map, sends a prompt to `Model.get_answer_without_history`, then assembles `order_output`.  

**Technical Details** – Constructs a user‑role prompt containing the list of titles, parses the comma‑separated LLM response, logs each step, and iterates over the ordered titles to build the final markdown string.  

**Data Flow** – Input: `Model` instance, `chanks` dict. Output: ordered markdown string. Side‑effects: extensive logging.
[2026-01-27 14:21:09.731842] [INFO] Add to <a name="config-reader-responsibility"></a>
## Config‑Reader Responsibility
`read_config` loads a user‑supplied YAML string, transforms it into a strongly‑typed `Config` object and a list of custom documentation modules. It centralises all project‑wide settings (language, name, ignored files, additional info) and prepares module descriptors for the documentation engine.
[2026-01-27 14:21:09.731865] [INFO] Add to <a name="config-reader-interaction"></a>
## Interaction with Factory Modules
The function imports `CustomModule` and `CustomModuleWithOutContext` from `autodocgenerator.factory.modules.general_modules`.  
* When a description line starts with “%” it is wrapped in `CustomModuleWithOutContext` (context‑free).  
* Otherwise the line is turned into a regular `CustomModule`.  
These objects are later consumed by the rendering pipeline to inject user‑defined sections into the generated markdown.
[2026-01-27 14:21:09.731887] [INFO] Add to <a name="config-reader-technical-details"></a>
## Technical Details and Logic Flow
1. **YAML parsing** – `yaml.safe_load` converts the raw string to a Python dict.  
2. **Base `Config` creation** – a fresh `Config` instance is instantiated.  
3. **Core fields extraction** – `ignore_files`, `language`, `project_name`, and `project_additional_info` are read with sensible defaults.  
4. **Project settings handling** – a `ProjectConfigSettings` object (`pcs`) loads the `project_settings` mapping via `pcs.load_settings`.  
5. **Fluent configuration** – `config.set_language(...).set_project_name(...).set_pcs(pcs)` chains setters for readability.  
6. **Iterative population** – loops add ignore patterns and additional info via `config.add_ignore_file` and `config.add_project_additional_info`.  
7. **Custom description conversion** – each entry in `custom_descriptions` is examined; the leading character decides which module class to instantiate.
[2026-01-27 14:21:09.731910] [INFO] Add to <a name="config-reader-data-flow"></a>
## Input, Output, and Side Effects
| Aspect | Description |
|--------|-------------|
| **Input** | `file_data: str` – raw YAML content supplied by `autodocconfig.yml`. |
| **Output** | `Tuple[Config, List[CustomModule]]` – a ready‑to‑use configuration object and a collection of module descriptors. |
| **Side effects** | None; the function is pure aside from importing modules. It assumes the YAML schema contains the keys used above; missing optional keys fall back to defaults. |
| **Assumptions** | The YAML is well‑formed, custom description strings are non‑empty, and the imported module classes conform to the expected constructor signatures. |
| **Error handling** | Propagation of `yaml.YAMLError` or any constructor exception to the caller; the runner catches and logs them via the global logger. |

Together, `read_config` supplies the documentation engine with a complete, validated configuration snapshot and the custom module list required for the subsequent generation steps.
[2026-01-27 14:21:09.731938] [INFO] Add to <a name="configuration-object-construction"></a>  
## Configuration object construction  

`Config` aggregates global settings:  

* **ignore_files** – glob patterns excluded from scanning.  
* **language** – default `"en"`.  
* **project_name** & **project_additional_info** – descriptive metadata passed to `ProjectSettings`.  
* **pcs** – a `ProjectConfigSettings` container (log flags).  

Convenient setters (`set_language`, `set_project_name`, `add_ignore_file`, …) enable fluent building.  
`get_project_settings()` converts the stored metadata into a `ProjectSettings` instance used later by the `Manager`.
[2026-01-27 14:21:09.731960] [INFO] Add to <a name="data-flow-and-side‑effects"></a>  
## Data flow and side‑effects  

* **Input** – `project_path` (string), a parsed `Config` object, and a list of instantiated custom module objects.  
* **Processing** – manager writes intermediate code files to the hidden `.auto_doc_cache` directory, updates the console progress bar, and may create temporary log files if `ProjectConfigSettings.save_logs` is true.  
* **Output** – a single string containing the complete documentation (`output_doc`).  
* **Assumptions** – a valid `API_KEY` is available, custom modules implement the `BaseModule` interface expected by `DocFactory`, and the project directory is readable.  

These steps provide the validated configuration snapshot and the custom module list required for the downstream generation engine.
[2026-01-27 14:21:09.731983] [INFO] Add to <a name="parentmodel-shared-configuration"></a>  
## ParentModel – Shared Model Configuration  

`ParentModel` centralises authentication, history handling, and model‑selection logic. It receives an `api_key`, a `History` instance, and a `use_random` flag. The constructor shuffles the global `MODELS_NAME` list (when randomised) and stores the ordered copy in `self.regen_models_name`. Index tracking (`self.current_model_index`) enables round‑robin fallback when a model fails.

---
[2026-01-27 14:21:09.732004] [INFO] Add to <a name="history-conversation-state"></a>  
## History – Conversation State Management  

`History` builds the message array sent to the LLM. It auto‑injects the system prompt (`BASE_SYSTEM_TEXT`) and provides `add_to_history(role, content)` to append user or assistant entries. The `self.history` list is consumed directly by the model classes.

---
[2026-01-27 14:21:09.732024] [INFO] Add to <a name="gptmodel-synchronous-generation"></a>  
## GPTModel – Synchronous Answer Generation  

Inherits from `Model` (which extends `ParentModel`).  
* **Responsibility** – Wraps the Groq synchronous client (`Groq`) to produce a completion.  
* **Interaction** – Uses `self.client.chat.completions.create(messages, model)`; falls back through `self.regen_models_name` on exception, logging each attempt via `BaseLogger`.  
* **Data Flow** –  
  1. Chooses `messages` from `self.history.history` or a raw `prompt`.  
  2. Calls the Groq API, extracts `choices[0].message.content`.  
  3. Logs generation steps and returns the answer.

---
[2026-01-27 14:21:09.732044] [INFO] Add to <a name="asyncgptmodel-asynchronous-generation"></a>  
## AsyncGPTModel – Asynchronous Answer Generation  

Mirrors `GPTModel` but operates with `AsyncGroq`.  
* **Responsibility** – Provides `async generate_answer` for non‑blocking calls.  
* **Interaction** – Awaits `self.client.chat.completions.create`. On failure, logs a warning and advances `self.current_model_index`.  
* **Data Flow** – Same as the synchronous variant, but returns a coroutine result and logs at level 2 for the final answer.

Both model classes rely on the shared fallback mechanism defined in `ParentModel`, ensuring resilience when a specific model endpoint is unavailable.
[2026-01-27 14:21:09.732070] [INFO] Add to <a name="docfactory-orchestrating-documentation-generation"></a>  
## DocFactory – Orchestrating Documentation Generation  

* **Responsibility** – Accepts an ordered list of `BaseModule` instances and sequentially invokes each to produce a composite documentation string.  
* **Interaction** –  
  * Receives a `Model` (or `AsyncModel`) and a `BaseProgress` UI object from the caller.  
  * Calls `module.generate(info, model)` for every injected module.  
  * Emits `InfoLog` entries via the shared `BaseLogger`, forwarding the module name and its raw output (level 2).  
* **Technical Details** –  
  * Stores modules in `self.modules: list[BaseModule]`.  
  * Initializes a simple logger (`BaseLogger()`).  
  * Uses `progress.create_new_subtask` to allocate a sub‑task sized to the module count, `progress.update_task` after each module, and `progress.remove_subtask` on completion.  
* **Data Flow** –  
  1. Input: `info` dict, `model` instance, `progress` UI.  
  2. For each module: `module.generate` returns a string → concatenated to `output`.  
  3. Side‑effects: logging, progress bar updates.  
  4. Output: the final assembled documentation string.
[2026-01-27 14:21:09.732092] [INFO] Add to <a name="custommodule-context-aware-intro-generation"></a>  
## CustomModule – Context‑Aware Intro Generation  

* **Responsibility** – Generates a custom introductory paragraph that incorporates a user‑provided description and a code‑snippet excerpt.  
* **Interaction** – Calls `split_data` to truncate `info["code_mix"]` to ≤ 5 000 symbols, then forwards the split text, the model, the description, and the target language to `generete_custom_discription`.  
* **Technical Details** –  
  * Inherits from `BaseModule`.  
  * Holds `self.discription` (typo retained from original).  
  * `generate` returns the processed string from the post‑processor.  

* **Data Flow** –  
  1. Input: `info` dict (keys `code_mix`, `language`), `model`.  
  2. Processing: `split_data → generete_custom_discription`.  
  3. Output: formatted intro paragraph.
[2026-01-27 14:21:09.732131] [INFO] Add to <a name="custommodulewithoutcontext‑static-intro-generation"></a>  
## CustomModuleWithOutContext – Static Intro Generation  

* **Responsibility** – Produces a description‑only intro, omitting any code context.  
* **Interaction** – Directly invokes `generete_custom_discription_without` with the model, stored description, and language.  
* **Data Flow** – Input: `info["language"]`, `model`; Output: static intro string; No side‑effects beyond the returned text.
[2026-01-27 14:21:09.732162] [INFO] Add to <a name="introlinks‑html‑link‑extraction"></a>  
## IntroLinks – HTML Link Extraction for Documentation  

* **Responsibility** – Extracts all HTML links from `info["full_data"]` and synthesizes a brief introductory block about those links.  
* **Interaction** – Uses `get_all_html_links` → `get_links_intro`, passing the model and language.
[2026-01-27 14:21:09.732184] [INFO] Add to <a name="introtext‑global‑context‑introduction"></a>  
## IntroText – Global Context Introduction  

* **Responsibility** – Generates a high‑level introduction based on `info["global_data"]`.  
* **Interaction** – Calls `get_introdaction` with the model and language, returning the result.
[2026-01-27 14:21:09.732204] [INFO] Add to <a name="intro-with-links-generation"></a>Intro‑With‑Links Generation**  
`get_links_intro` receives the anchor list, a `Model` implementation, and an optional language code. It composes a three‑message system‑user prompt (language hint, `BASE_INTRODACTION_CREATE_LINKS` template, and the raw link list) and calls `model.get_answer_without_history`.  

*Inputs*: `links` (list of anchors), `model` (concrete `Model`/`GPTModel`), `language`.  
*Outputs*: String containing a generated introductory paragraph that references the provided links.  
*Side‑effects*: Logs start/completion messages and the raw LLM response.  

---

**
[2026-01-27 14:21:09.732230] [INFO] Add to <a name="basic-introduction-generation"></a>Basic Introduction Generation**  
`get_introdaction` (typo intentional) builds a similar prompt using `BASE_INTRO_CREATE` and the full documentation (`global_data`). It returns the LLM‑generated introductory block.  

*Inputs*: `global_data` (full docs), `model`, `language`.  
*Outputs*: Introduction string.  

---

**
[2026-01-27 14:21:09.732251] [INFO] Add to <a name="custom-description-loop"></a>Custom Description Loop**  
`generete_custom_discription` iterates over a pre‑split collection of documentation fragments (`splited_data`). For each fragment it sends a detailed system prompt (technical analyst role, context snippet, `BASE_CUSTOM_DISCRIPTIONS`) together with the user‑requested `custom_description`. The loop breaks when the LLM returns a substantive answer (i.e., does not contain “!noinfo” or “No information found”). The final `result` is returned.  

*Inputs*: `splited_data` (iterable of strings), `model`, `custom_description`, `language`.  
*Outputs*: Custom description text for the first fragment that yields valid content.  

---

**
[2026-01-27 14:21:09.732271] [INFO] Add to <a name="standalone-custom-description"></a>Standalone Custom Description**  
`generete_custom_discription_without` builds a single‑shot prompt that forces the LLM to prepend a strict `
[2026-01-27 14:21:09.732292] [INFO] Add to <a name="CONTENT_DESCRIPTION"></a>` tag and obey naming constraints (no filenames, extensions, generic terms, or URLs). The result is returned directly.  

*Inputs*: `model`, `custom_description`, `language`.  
*Outputs*: Tagged description string.  

---

**Interaction Summary**  
All functions rely on the shared `BaseLogger` for traceability and on the abstract `Model` interface (concretely `GPTModel`) to invoke the LLM. They do **not** perform I/O beyond logging; the calling post‑processor aggregates their outputs into the final documentation file.
[2026-01-27 14:21:09.732312] [INFO] Add to <a name="anchor-extraction-routine"></a>  
## Anchor Extraction Routine (sorting.py)

**Responsibility** – Detect HTML‑style anchors at the start of each markdown chunk and build a mapping `anchor → chunk`.  

**Interactions** – Consumes raw README text, feeds the resulting dictionary to `get_order`. Relies on `BaseLogger` for diagnostic output.  

**Technical Details** – `extract_links_from_start` uses a regex `^<a name=["']?(.*?)["']?</a>` to capture names longer than five characters, prefixing them with `#`. `split_text_by_anchors` splits the document on a look‑ahead pattern, trims empties, validates a one‑to‑one link‑chunk relationship, and returns `{anchor: chunk}` or `None`.  

**Data Flow** – Input: `text` (str). Output: `dict[str, str]` where keys are `#anchor`. Side‑effects: logging of chunk names and content; early exit if counts mismatch.
[2026-01-27 14:21:09.732333] [INFO] Add to <a name="description-generation"></a>  
## `generate_discribtions_for_code` – LLM‑driven documentation builder  

**Responsibility**  
Feeds each source file to the model with a strict instruction prompt, collects the model’s descriptive answer, and reports progress.  

**Data flow**  
- **Input:** List of code strings, `model`, `project_settings`, `progress_bar`.  
- **Output:** List of description strings (same order as input).  
- **Side‑effects:** Progress bar sub‑task updates.
[2026-01-27 14:21:09.732353] [INFO] Add to <a name="html-link-extraction"></a>HTML‑Link Extraction**  
The function `get_all_html_links` scans a markdown string for anchor tags of the form `<a name="…"></a>`. It logs the start/end of the operation, builds a list of fragment identifiers prefixed with “#”, and returns that list.  

*Inputs*: `data` – full documentation text.  
*Outputs*: `list[str]` – collected anchors (e.g., `["#section‑overview"]`).  
*Side‑effects*: Writes two `InfoLog` entries via `BaseLogger`.  

---

**
[2026-01-27 14:21:09.732378] [INFO] Add to <a name="introlinks‑html‑link‑extraction"></a>  
## IntroLinks – HTML Link Extraction for Documentation  

* **Responsibility** – Extracts all HTML links from `info["full_data"]` and synthesizes a brief introductory block about those links.  
* **Interaction** – Uses `get_all_html_links` → `get_links_intro`, passing the model and language.
[2026-01-27 14:21:09.732399] [INFO] Add to <a name="autodocfile-options"></a>
Define the top‑level keys in the file as follows:

- **project_name** – a string that identifies the project.
- **language** – language code (default “en” if omitted).
- **project_settings** – a block containing:
  - **save_logs** – boolean, true to keep generation logs.
  - **log_level** – numeric level controlling output detail.
- **project_additional_info** – map of arbitrary key/value pairs for extra context (e.g., a global idea description).
- **ignore_files** – optional list of filename patterns that should be skipped.
- **custom_descriptions** – list of strings.  
  - If a string starts with “%”, the leading character is removed and the remainder is processed without additional context.  
  - Otherwise the string is taken as‑is and processed with full context.

Each entry should be written in standard YAML syntax, preserving indentation for nested blocks.
[2026-01-27 14:21:09.732419] [INFO] Add to <a name="projectsettings-prompt"></a>  
## `ProjectSettings.prompt` – dynamic system context  

**Responsibility**  
Constructs a multi‑line system prompt by concatenating a base template (`BASE_SETTINGS_PROMPT`) with the project name and any key‑value pairs added via `add_info`.  

**Technical details**  
- `info` is a mutable dict; `add_info` inserts or overwrites entries.  
- Property accessor lazily builds the prompt each call, ensuring the latest `info` content is reflected.  

**Interactions**  
- Consumed by every compression or description function to provide project‑specific background to the LLM.
[2026-01-27 14:21:09.732439] [INFO] Add to <a name="data-splitting-algorithm"></a>  
## `split_data` – chunking source text to fit token limits  

**Responsibility**  
Breaks a monolithic code string into a list of substrings whose lengths stay below `max_symbols` while preserving line boundaries.  

**Interactions**  
Uses `BaseLogger` for progress messages; no external state is read or modified.  

**Technical details**  
1. Initial split on newline (`"\n"`).  
2. Repeatedly scans `splited_by_files`; any element exceeding `1.5 × max_symbols` is bisected at `max_symbols/2` and re‑inserted, forcing convergence.  
3. A second pass packs the trimmed fragments into `split_objects`, starting a new part whenever the accumulated length would exceed `1.25 × max_symbols`.  

**Data flow**  
- **Input:** `data: str`, `max_symbols: int`.  
- **Output:** `list[str]` – ordered chunks ready for LLM consumption.  
- **Side‑effects:** One `InfoLog` entry before and after splitting.
[2026-01-27 14:21:09.732460] [INFO] Add to <a name="synchronous-part-doc-generation"></a>  
## `write_docs_by_parts` – generate documentation for a single chunk (sync)  

**Responsibility**  
Builds a system‑user prompt, sends it to a synchronous `Model`, and returns the cleaned LLM answer.  

**Interactions**  
Relies on `BASE_PART_COMPLITE_TEXT` (static instruction), optional `prev_info` (context from the previous chunk), and `BaseLogger`.  

**Technical details**  
- Prompt composition: two mandatory system messages (language + part ID, base instruction) + optional previous‑part context + user message containing the code chunk.  
- Calls `model.get_answer_without_history`.  
- Strips surrounding Markdown fences (```), logs answer length and full content (debug level 2).  

**Data flow**  
- **Input:** `part_id: int`, `part: str`, `model: Model`, `prev_info: str | None`, `language: str`.  
- **Output:** Cleaned documentation string for the chunk.  
- **Side‑effects:** Logging; no mutation of arguments.
[2026-01-27 14:21:09.732486] [INFO] Add to <a name="asynchronous-part-doc-generation"></a>  
## `async_write_docs_by_parts` – concurrent chunk documentation (async)  

**Responsibility**  
Same prompt logic as the sync variant but executed within an `asyncio.Semaphore` to limit parallel LLM calls.  

**Interactions**  
Accepts an `AsyncModel`, optional `update_progress` callback, and shares the same logger.  

**Technical details**  
- `async with semaphore` protects the call to `async_model.get_answer_without_history`.  
- After obtaining the answer, optionally invokes `update_progress` (used by the surrounding progress bar).  
- Performs identical back‑tick stripping and logging.  

**Data flow**  
- **Input:** `part: str`, `async_model: AsyncModel`, `global_info: str`, `semaphore`, `prev_info: str | None`, `language: str`, `update_progress: Callable | None`.  
- **Output:** Cleaned documentation string (coroutine result).  
- **Side‑effects:** Logging; may update external progress bar via callback.
[2026-01-27 14:21:09.732511] [INFO] Add to <a name="batch-doc-generation-sync"></a>  
## `gen_doc_parts` – orchestrated synchronous documentation pipeline  

**Responsibility**  
Splits the full code, sequentially processes each part with `write_docs_by_parts`, aggregates results, and maintains a sub‑task on a `BaseProgress` bar.  

**Interactions**  
Calls `split_data`, creates/updates/removes a sub‑task on `progress_bar`, and logs high‑level milestones.  

**Technical details**  
- After each part, retains the last 3000 characters of the generated text as `prev_info` to provide context for the next iteration.  
- Concatenates all part outputs separated by blank lines.  

**Data flow**  
- **Input:** `full_code_mix: str`, `max_symbols: int`, `model: Model`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Single string containing the complete documentation.  
- **Side‑effects:** Progress‑bar sub‑task lifecycle, extensive logging.
[2026-01-27 14:21:09.732536] [INFO] Add to <a name="batch-doc-generation-async"></a>  
## `async_gen_doc_parts` – orchestrated asynchronous documentation pipeline  

**Responsibility**  
Parallel version of `gen_doc_parts`; dispatches `async_write_docs_by_parts` for every split chunk, respecting a concurrency limit of four.  

**Interactions**  
Creates a semaphore, builds a list of coroutines, gathers them, and updates the same `BaseProgress` sub‑task via a lambda passed to each async worker.  

**Technical details**  
- Uses `asyncio.gather` to await all part‑level tasks.  
- Result aggregation mirrors the sync version (blank‑line separation).  

**Data flow**  
- **Input:** `full_code_mix: str`, `global_info: str`, `max_symbols: int`, `model: AsyncModel`, `language: str`, `progress_bar: BaseProgress`.  
- **Output:** Full documentation string (awaited coroutine).  
- **Side‑effects:** Progress‑bar sub‑task updates, logging, semaphore enforcement.
[2026-01-27 14:21:09.732557] [INFO] Add to <a name="repository-mix-builder"></a>  
## Repository Mix Builder (code_mix.py)

**Responsibility** – Walk a repository, emit a hierarchical file‑tree, and concatenate file contents into a single “repo mix” artifact while respecting ignore patterns.  

**Interactions** – Utilises `Path.rglob` for traversal, `should_ignore` for pattern checks, and `BaseLogger` for progress messages.  

**Technical Details** – `should_ignore` matches a path against user‑supplied glob patterns across the full relative path, basename, and individual parts. `build_repo_content` writes a tree view, inserts a delimiter, then writes each non‑ignored file wrapped in `<file path="...">` tags, handling read errors gracefully.  

**Data Flow** – Input: `root_dir` (Path), `ignore_patterns` (list). Output: `repomix-output.txt` (text file). Side‑effects: file creation, console logs, error annotations inside the output file.
[2026-01-27 14:21:09.732578] [INFO] Add to <a name="compress-function"></a>  
## `compress` – single‑pass text reduction  

**Responsibility**  
Accepts raw source `data`, builds a three‑message prompt (system settings, compression template, user payload) and returns the model’s answer, which is the compressed representation of the input.  

**Interactions**  
- Uses `ProjectSettings.prompt` for the contextual system message.  
- Calls `get_BASE_COMPRESS_TEXT` to obtain a size‑aware compression instruction.  
- Delegates the LLM request to any `Model` implementation (`GPTModel`, custom adapters).  

**Technical notes**  
- No async I/O; the model call is synchronous via `model.get_answer_without_history`.  
- Returns a plain string; callers are responsible for further aggregation.
[2026-01-27 14:21:09.732604] [INFO] Add to <a name="batch-compression"></a>  
## `compress_and_compare` – synchronous batch aggregator  

**Responsibility**  
Splits a list of file contents into groups of `compress_power` items, compresses each element with `compress`, concatenates the results per group, and tracks progress.  

**Data flow**  
- **Input:** `data` (list of strings), `model`, `project_settings`, optional `compress_power`, `progress_bar`.  
- **Output:** List of aggregated compressed chunks, length ≈ ⌈len(data)/compress_power⌉.  
- **Side‑effects:** Updates `progress_bar` (sub‑task creation, per‑item updates, removal).  

**Logic**  
1. Pre‑allocate a result list sized to the number of groups.  
2. For each element, compute its group index `i // compress_power`, append the compressed string plus newline.  
3. Progress bar reflects each processed element.
[2026-01-27 14:21:09.732625] [INFO] Add to <a name="async-batch-compression"></a>  
## `async_compress_and_compare` – parallel batch compressor  

**Responsibility**  
Performs the same grouping as `compress_and_compare` but launches up to four concurrent LLM calls using `asyncio.Semaphore`.  

**Key steps**  
- Creates a semaphore (limit = 4) and a task list.  
- Each element is wrapped in `async_compress`, which builds the identical prompt and awaits `model.get_answer_without_history`.  
- After `asyncio.gather`, results are re‑chunked according to `compress_power` and concatenated with newlines.  

**Data flow**  
- **Input/Output:** Same as the synchronous version, but returns a coroutine result.  
- **Side‑effects:** Progress bar updates inside `async_compress`; sub‑task lifecycle mirrors the sync variant.
[2026-01-27 14:21:09.732645] [INFO] Add to <a name="iterative-compression"></a>  
## `compress_to_one` – iterative reduction to a single payload  

**Responsibility**  
Repeatedly compresses the dataset until only one aggregated chunk remains, optionally using the async pipeline.  

**Algorithm**  
- While `len(data) > 1`, choose an effective `compress_power` (fallback = 2 for small lists).  
- Call either `async_compress_and_compare` (via `asyncio.run`) or `compress_and_compare`.  
- Increment iteration counter; final output is `data[0]`.
[2026-01-27 14:21:09.732666] [INFO] Add to <a name="logging-structures"></a>
## `BaseLog` hierarchy – structured log messages  

**Responsibility**  
Defines lightweight log objects (`BaseLog`, `ErrorLog`, `WarningLog`, `InfoLog`) that prepend a timestamp and level tag to a user‑supplied message.  

**Interactions**  
Instances are passed to a `BaseLoggerTemplate` via its `log` method; the template decides whether to emit the message based on its configured `log_level`.  

**Technical details**  
- `BaseLog` stores `message` and numeric `level`; its `format` returns the raw text.  
- `_log_prefix` property builds `"[YYYY‑MM‑DD HH:MM:SS]"` using `datetime.fromtimestamp(time.time())`.  
- Sub‑classes override `format` to insert `[ERROR]`, `[WARNING]`, or `[INFO]` after the prefix.  

**Data flow**  
- **Input:** `message: str`, optional `level: int`.  
- **Output:** Formatted string via `format()`.  
- **Side‑effects:** None (pure data object).
[2026-01-27 14:21:09.732686] [INFO] Add to <a name="runtime-logger"></a>
## `BaseLogger` singleton – runtime logging façade  

**Responsibility**  
Provides a globally accessible logger that delegates to a concrete `BaseLoggerTemplate` (console, file, etc.).  

**Interactions**  
- `set_logger` injects a concrete template (`BaseLoggerTemplate`, `FileLoggerTemplate`, …).  
- Calls to `log` forward the `BaseLog` instance to the template’s `global_log`, which respects the template’s `log_level`.  

**Technical details**  
- Implements the classic singleton pattern in `__new__` to guarantee a single shared instance across the process.  
- `global_log` in `BaseLoggerTemplate` checks `log_level` (`-1` disables filtering).  

**Data flow**  
- **Input:** `BaseLog` object.  
- **Output:** Printed to stdout or appended to a file, depending on the active template.  
- **Side‑effects:** I/O (stdout or file write).
[2026-01-27 14:21:09.732716] [INFO] Add to <a name="progress-abstractions"></a>
## `BaseProgress` abstraction and concrete implementations  

**Responsibility**  
Offers a minimal API (`create_new_subtask`, `update_task`, `remove_subtask`) for tracking hierarchical progress in different environments.  

**Interactions**  
- `LibProgress` wraps Rich’s `Progress` object, exposing Rich‑based visual feedback.  
- `ConsoleGtiHubProgress` uses simple `print` statements via `ConsoleTask` for environments without Rich.  

**Technical details**  
- `LibProgress` creates a base task (`General progress`) and optional sub‑tasks; `update_task` advances either the sub‑task or base task.  
- `ConsoleTask` maintains a counter and prints a percentage on each `progress` call.  
- `ConsoleGtiHubProgress` delegates to a `ConsoleTask` for sub‑tasks, falling back to a `ConsoleTask` representing the overall progress.  

**Data flow**  
- **Input:** Sub‑task name and total length.  
- **Output:** Visual progress updates on the console or Rich UI.  
- **Side‑effects:** stdout writes (or Rich UI updates).  

These components together furnish a lightweight, interchangeable logging and progress‑tracking subsystem used throughout the autodocgenerator package.
[2026-01-27 14:21:09.732738] [INFO] Add to <a name="project-metadata"></a>
## `pyproject.toml` – package identity & version constraints  

**Responsibility**  
Encapsulates the canonical package descriptor for **autodocgenerator**: name, semantic version, human‑readable description, author contact, license, and the supported Python range.  

**Interactions**  
- Read by *Poetry* (or any PEP 517‑compatible builder) to resolve the project’s identity during `poetry install`, `poetry build`, and wheel creation.  
- Exposed to downstream tools (e.g., `pip`, `build`) via the generated `dist/*.whl` metadata.  

**Technical details**  
- `name = "autodocgenerator"` and `version = "0.8.9.9"` follow PEP 621 naming rules.  
- `description` and `readme` feed the **Core Metadata** fields of the resulting distribution.  
- `authors` is a list of `{"name": "...", "email": "..."}` objects, converted to the `Author` field in the wheel.  
- `license = {text = "MIT"}` supplies an SPDX‑compatible license string.  

**Data flow**  
- **Input:** Static TOML literals edited by maintainers.  
- **Output:** Serialized metadata consumed at build time; no runtime side‑effects.
[2026-01-27 14:21:09.732760] [INFO] Add to <a name="runtime-dependencies"></a>
## Dependency enumeration – third‑party runtime requirements  

**Responsibility**  
Lists exact version pins required for the library to operate, ensuring reproducible environments across Python 3.11–3.12.  

**Interactions**  
- Resolved by Poetry’s dependency resolver; each entry becomes a `Requires-Dist` clause in the wheel metadata.  
- At installation, `pip` (via the wheel) pulls the same versions, guaranteeing API stability.  

**Technical details**  
- All entries follow the `package==major.minor.patch` syntax, e.g., `rich==14.2.0`.  
- The list covers HTTP clients (`httpx`), authentication (`google-auth`), AI SDKs (`openai`, `groq`), data handling (`pyyaml`, `msgmsg`), and utilities (`rich_progress`, `tqdm`).  
- No optional groups are defined; every line is a mandatory runtime requirement.  

**Data flow**  
- **Input:** Version‑locked strings.  
- **Output:** Dependency graph constructed by the resolver; installed files appear in `site‑packages`.
[2026-01-27 14:21:09.732785] [INFO] Add to <a name="build-system-configuration"></a>
## Build‑system stanza – PEP 517 backend specification  

**Responsibility**  
Declares the toolchain needed to produce a distributable artifact from the source.  

**Interactions**  
- `requires = ["poetry-core>=2.0.0"]` tells the build frontend to fetch *poetry‑core* before invoking the backend.  
- `build-backend = "poetry.core.masonry.api"` points to the entry‑point that implements `build_wheel`, `build_sdist`, etc.  

**Technical details**  
- The stanza is minimal; no custom build steps are injected, relying on Poetry’s default isolation and caching mechanisms.  

**Data flow**  
- **Input:** The declared backend package.  
- **Output:** A built wheel or source distribution emitted to `dist/`; no side‑effects beyond temporary build directories.
[2026-01-27 14:21:09.733751] [INFO] Starting factory documentation generation 

                                IntroLinks 

                                Input params: language: 2 chars full_data: 36155 chars code_mix: 278089 chars
                                
[2026-01-27 14:21:09.733807] [INFO] Extracting HTML links from documentation...
[2026-01-27 14:21:09.734129] [INFO] Extracted 46 HTML links from documentation.
[2026-01-27 14:21:09.734177] [INFO] Links: ['#install-workflow-setup', '#runfile-execution', '#manager-orchestration-pipeline', '#semantic-ordering-workflow', '#config-reader-responsibility', '#config-reader-interaction', '#config-reader-technical-details', '#config-reader-data-flow', '#configuration-object-construction', '#data-flow-and-side‑effects', '#parentmodel-shared-configuration', '#history-conversation-state', '#gptmodel-synchronous-generation', '#asyncgptmodel-asynchronous-generation', '#docfactory-orchestrating-documentation-generation', '#custommodule-context-aware-intro-generation', '#custommodulewithoutcontext‑static-intro-generation', '#introlinks‑html‑link‑extraction', '#introtext‑global‑context‑introduction', '#intro-with-links-generation', '#basic-introduction-generation', '#custom-description-loop', '#standalone-custom-description', '#CONTENT_DESCRIPTION', '#anchor-extraction-routine', '#description-generation', '#html-link-extraction', '#introlinks‑html‑link‑extraction', '#autodocfile-options', '#projectsettings-prompt', '#data-splitting-algorithm', '#synchronous-part-doc-generation', '#asynchronous-part-doc-generation', '#batch-doc-generation-sync', '#batch-doc-generation-async', '#repository-mix-builder', '#compress-function', '#batch-compression', '#async-batch-compression', '#iterative-compression', '#logging-structures', '#runtime-logger', '#progress-abstractions', '#project-metadata', '#runtime-dependencies', '#build-system-configuration']
[2026-01-27 14:21:09.734217] [INFO] Generating introduction with links...
[2026-01-27 14:21:09.734239] [INFO] Generating answer...
[2026-01-27 14:21:11.140628] [INFO] Generated answer with model llama-3.3-70b-versatile.
[2026-01-27 14:21:11.140685] [INFO] Answer: ## Executive Navigation Tree
* 📂 **Setup & Configuration**
  * [Install Workflow Setup](#install-workflow-setup)
  * [Config Reader Responsibility](#config-reader-responsibility)
  * [Config Reader Interaction](#config-reader-interaction)
  * [Config Reader Technical Details](#config-reader-technical-details)
  * [Config Reader Data Flow](#config-reader-data-flow)
  * [Configuration Object Construction](#configuration-object-construction)
  * [Project Settings Prompt](#projectsettings-prompt)
  * [Data Splitting Algorithm](#data-splitting-algorithm)
* ⚙️ **Core Engine**
  * [Runfile Execution](#runfile-execution)
  * [Manager Orchestration Pipeline](#manager-orchestration-pipeline)
  * [Semantic Ordering Workflow](#semantic-ordering-workflow)
  * [Data Flow and Side Effects](#data-flow-and-side‑effects)
  * [Parent Model Shared Configuration](#parentmodel-shared-configuration)
  * [History Conversation State](#history-conversation-state)
* 📄 **Documentation Generation**
  * [GPT Model Synchronous Generation](#gptmodel-synchronous-generation)
  * [Async GPT Model Asynchronous Generation](#asyncgptmodel-asynchronous-generation)
  * [Doc Factory Orchestrating Documentation Generation](#docfactory-orchestrating-documentation-generation)
  * [Custom Module Context Aware Intro Generation](#custommodule-context-aware-intro-generation)
  * [Custom Module Without Context Static Intro Generation](#custommodulewithoutcontext‑static-intro-generation)
  * [Intro Links HTML Link Extraction](#introlinks‑html‑link‑extraction)
  * [Intro Text Global Context Introduction](#introtext‑global‑context‑introduction)
  * [Intro With Links Generation](#intro-with-links-generation)
  * [Basic Introduction Generation](#basic-introduction-generation)
  * [Custom Description Loop](#custom-description-loop)
  * [Standalone Custom Description](#standalone-custom-description)
  * [Content Description](#CONTENT_DESCRIPTION)
  * [Anchor Extraction Routine](#anchor-extraction-routine)
  * [Description Generation](#description-generation)
  * [HTML Link Extraction](#html-link-extraction)
* 🗂️ **Build & Deployment**
  * [Synchronous Part Doc Generation](#synchronous-part-doc-generation)
  * [Asynchronous Part Doc Generation](#asynchronous-part-doc-generation)
  * [Batch Doc Generation Sync](#batch-doc-generation-sync)
  * [Batch Doc Generation Async](#batch-doc-generation-async)
  * [Repository Mix Builder](#repository-mix-builder)
  * [Compress Function](#compress-function)
  * [Batch Compression](#batch-compression)
  * [Async Batch Compression](#async-batch-compression)
  * [Iterative Compression](#iterative-compression)
* 📊 **Logging & Monitoring**
  * [Logging Structures](#logging-structures)
  * [Runtime Logger](#runtime-logger)
  * [Progress Abstractions](#progress-abstractions)
  * [Project Metadata](#project-metadata)
  * [Runtime Dependencies](#runtime-dependencies)
  * [Build System Configuration](#build-system-configuration)
[2026-01-27 14:21:11.140735] [INFO] Introduction with links generated.
[2026-01-27 14:21:11.140761] [INFO] Introduction Links: ## Executive Navigation Tree
* 📂 **Setup & Configuration**
  * [Install Workflow Setup](#install-workflow-setup)
  * [Config Reader Responsibility](#config-reader-responsibility)
  * [Config Reader Interaction](#config-reader-interaction)
  * [Config Reader Technical Details](#config-reader-technical-details)
  * [Config Reader Data Flow](#config-reader-data-flow)
  * [Configuration Object Construction](#configuration-object-construction)
  * [Project Settings Prompt](#projectsettings-prompt)
  * [Data Splitting Algorithm](#data-splitting-algorithm)
* ⚙️ **Core Engine**
  * [Runfile Execution](#runfile-execution)
  * [Manager Orchestration Pipeline](#manager-orchestration-pipeline)
  * [Semantic Ordering Workflow](#semantic-ordering-workflow)
  * [Data Flow and Side Effects](#data-flow-and-side‑effects)
  * [Parent Model Shared Configuration](#parentmodel-shared-configuration)
  * [History Conversation State](#history-conversation-state)
* 📄 **Documentation Generation**
  * [GPT Model Synchronous Generation](#gptmodel-synchronous-generation)
  * [Async GPT Model Asynchronous Generation](#asyncgptmodel-asynchronous-generation)
  * [Doc Factory Orchestrating Documentation Generation](#docfactory-orchestrating-documentation-generation)
  * [Custom Module Context Aware Intro Generation](#custommodule-context-aware-intro-generation)
  * [Custom Module Without Context Static Intro Generation](#custommodulewithoutcontext‑static-intro-generation)
  * [Intro Links HTML Link Extraction](#introlinks‑html‑link‑extraction)
  * [Intro Text Global Context Introduction](#introtext‑global‑context‑introduction)
  * [Intro With Links Generation](#intro-with-links-generation)
  * [Basic Introduction Generation](#basic-introduction-generation)
  * [Custom Description Loop](#custom-description-loop)
  * [Standalone Custom Description](#standalone-custom-description)
  * [Content Description](#CONTENT_DESCRIPTION)
  * [Anchor Extraction Routine](#anchor-extraction-routine)
  * [Description Generation](#description-generation)
  * [HTML Link Extraction](#html-link-extraction)
* 🗂️ **Build & Deployment**
  * [Synchronous Part Doc Generation](#synchronous-part-doc-generation)
  * [Asynchronous Part Doc Generation](#asynchronous-part-doc-generation)
  * [Batch Doc Generation Sync](#batch-doc-generation-sync)
  * [Batch Doc Generation Async](#batch-doc-generation-async)
  * [Repository Mix Builder](#repository-mix-builder)
  * [Compress Function](#compress-function)
  * [Batch Compression](#batch-compression)
  * [Async Batch Compression](#async-batch-compression)
  * [Iterative Compression](#iterative-compression)
* 📊 **Logging & Monitoring**
  * [Logging Structures](#logging-structures)
  * [Runtime Logger](#runtime-logger)
  * [Progress Abstractions](#progress-abstractions)
  * [Project Metadata](#project-metadata)
  * [Runtime Dependencies](#runtime-dependencies)
  * [Build System Configuration](#build-system-configuration)
[2026-01-27 14:21:11.140801] [INFO] Module IntroLinks generated its part of the documentation.
[2026-01-27 14:21:11.140823] [INFO] Module Output: ## Executive Navigation Tree
* 📂 **Setup & Configuration**
  * [Install Workflow Setup](#install-workflow-setup)
  * [Config Reader Responsibility](#config-reader-responsibility)
  * [Config Reader Interaction](#config-reader-interaction)
  * [Config Reader Technical Details](#config-reader-technical-details)
  * [Config Reader Data Flow](#config-reader-data-flow)
  * [Configuration Object Construction](#configuration-object-construction)
  * [Project Settings Prompt](#projectsettings-prompt)
  * [Data Splitting Algorithm](#data-splitting-algorithm)
* ⚙️ **Core Engine**
  * [Runfile Execution](#runfile-execution)
  * [Manager Orchestration Pipeline](#manager-orchestration-pipeline)
  * [Semantic Ordering Workflow](#semantic-ordering-workflow)
  * [Data Flow and Side Effects](#data-flow-and-side‑effects)
  * [Parent Model Shared Configuration](#parentmodel-shared-configuration)
  * [History Conversation State](#history-conversation-state)
* 📄 **Documentation Generation**
  * [GPT Model Synchronous Generation](#gptmodel-synchronous-generation)
  * [Async GPT Model Asynchronous Generation](#asyncgptmodel-asynchronous-generation)
  * [Doc Factory Orchestrating Documentation Generation](#docfactory-orchestrating-documentation-generation)
  * [Custom Module Context Aware Intro Generation](#custommodule-context-aware-intro-generation)
  * [Custom Module Without Context Static Intro Generation](#custommodulewithoutcontext‑static-intro-generation)
  * [Intro Links HTML Link Extraction](#introlinks‑html‑link‑extraction)
  * [Intro Text Global Context Introduction](#introtext‑global‑context‑introduction)
  * [Intro With Links Generation](#intro-with-links-generation)
  * [Basic Introduction Generation](#basic-introduction-generation)
  * [Custom Description Loop](#custom-description-loop)
  * [Standalone Custom Description](#standalone-custom-description)
  * [Content Description](#CONTENT_DESCRIPTION)
  * [Anchor Extraction Routine](#anchor-extraction-routine)
  * [Description Generation](#description-generation)
  * [HTML Link Extraction](#html-link-extraction)
* 🗂️ **Build & Deployment**
  * [Synchronous Part Doc Generation](#synchronous-part-doc-generation)
  * [Asynchronous Part Doc Generation](#asynchronous-part-doc-generation)
  * [Batch Doc Generation Sync](#batch-doc-generation-sync)
  * [Batch Doc Generation Async](#batch-doc-generation-async)
  * [Repository Mix Builder](#repository-mix-builder)
  * [Compress Function](#compress-function)
  * [Batch Compression](#batch-compression)
  * [Async Batch Compression](#async-batch-compression)
  * [Iterative Compression](#iterative-compression)
* 📊 **Logging & Monitoring**
  * [Logging Structures](#logging-structures)
  * [Runtime Logger](#runtime-logger)
  * [Progress Abstractions](#progress-abstractions)
  * [Project Metadata](#project-metadata)
  * [Runtime Dependencies](#runtime-dependencies)
  * [Build System Configuration](#build-system-configuration)
[2026-01-27 14:21:11.140881] [INFO] Factory documentation generation completed.
